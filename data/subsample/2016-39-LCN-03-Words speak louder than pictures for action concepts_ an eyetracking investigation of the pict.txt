Language, Cognition and Neuroscience

ISSN: 2327-3798 (Print) 2327-3801 (Online) Journal homepage: www.tandfonline.com/journals/plcp21

Words speak louder than pictures for action concepts: an
eyetracking investigation of the picture superiority effect
in semantic categorisation
Jinyi Hung, Lisa A. Edmonds & Jamie Reilly
To cite this article: Jinyi Hung, Lisa A. Edmonds & Jamie Reilly (2016) Words speak louder than
pictures for action concepts: an eyetracking investigation of the picture superiority effect
in semantic categorisation, Language, Cognition and Neuroscience, 31:9, 1150-1166, DOI:
10.1080/23273798.2016.1198818
To link to this article: https://doi.org/10.1080/23273798.2016.1198818

Published online: 27 Jun 2016.

Submit your article to this journal

Article views: 1097

View related articles

View Crossmark data

Citing articles: 1 View citing articles

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=plcp21

LANGUAGE, COGNITION AND NEUROSCIENCE, 2016
VOL. 31, NO. 9, 1150-1166
http://dx.doi.org/10.1080/23273798.2016.1198818

REGULAR ARTICLE

Words speak louder than pictures for action concepts: an eyetracking
investigation of the picture superiority effect in semantic categorisation
Jinyi Hunga,b, Lisa A. Edmondsc and Jamie Reillya,b
a

Eleanor M. Saffran Center for Cognitive Neuroscience, Philadelphia, PA, USA; bDepartment of Communication Sciences and Disorders, Temple
University, Philadelphia, PA, USA; cDepartment of Biobehavioral Sciences, Program of Communication Sciences and Disorders, Teachers College,
Columbia University, New York, NY, USA
ABSTRACT

ARTICLE HISTORY

An evolving consensus holds that actions and objects are supported by at least partially distinct
neurocognitive substrates. In addition, representational modality (e.g. pictures vs. words)
moderates the speed and accuracy of semantic access for both word classes. The picture
superiority effect refers to the advantage that pictures manifest relative to words across a range
of object recognition and memory recall tasks. While the effect has been investigated exclusively
in the context of static objects, it is unclear whether this modality advantage extends to the
processing of actions and/or verbs. We report two eyetracking experiments examining the
dynamics of modality-specific access to actions via associative semantic judgement tasks for
actions depicted as pictures or words. A range of eyetracking and behavioural measures
revealed that participants consistently showed a reversal of the picture superiority effect for
action semantic categorisation. These results challenge the privileged access hypothesis and
demonstrate further distinctiveness of the action-object processing dichotomy. We discuss
implications for action semantic processing.

Received 16 December 2015
Accepted 1 June 2016

Introduction
A dominant perspective of semantic memory holds that
word and object meaning ultimately converge within a
central semantic store. Numerous pathways can access
that store. For example, we routinely access meaning
through exposure to written and spoken words,
gesture, environmental sounds, or visual images. Language offers an arbitrarily symbolic mode of accessing
meaning in that the phonological forms of words are
untethered to their referents (Reilly, Westbury, Kean, &
Peelle, 2012; Saussure, 1916). In contrast, it has been
argued that pictures offer a veridical route to the
semantic system and that this unfiltered access
confers a significant processing advantage, a phenomenon known as the picture superiority effect (Nelson,
Reed, & McEvoy, 1977; Paivio, 1991; Potter & Faulconer,
1975). Decades of research addressing this modality
advantage has elucidated the time course of semantic
access as a function of representational format in the
domain of static objects (Hantsch, Jescheniak, & Madebach, 2012). However, less is known about the
dynamics of modality-specific access to actions
(Bedny, Caramazza, Pascual-Leone, & Saxe, 2012;
Kable, Lease-Spellmeyer, & Chatterjee, 2002). Yet,
there are compelling reasons to investigate the
CONTACT Jinyi Hung

tuf78049@temple.edu

(c) 2016 Informa UK Limited, trading as Taylor & Francis Group

KEYWORDS

Semantic memory; verb
processing; eyetracking;
semantic access; action
concept

effects of modality-specific access to actions, including
unique trajectories of age-of-acquisition for actions and
objects throughout early childhood (Gentner, 1982;
Juhasz, 2005; McDonough, Song, Hirsh-Pasek, Golinkoff,
& Lannon, 2011; Ramey, Chrysikou, & Reilly, 2012), performance discrepancies in reading ability and word
recognition among neurotypical adults (Crepaldi, Berlingeri, Paulesu, & Luzzatti, 2011), and neuropsychological dissociations among patients with prefrontal vs.
posterior cortical pathologies (Cappa et al., 1998;
Hillis, Sangjin, & Ken, 2004; Hillis, Tuffiash, & Caramazza,
2002; Matzig, Druks, Masterson, & Vigliocco, 2009).

Modality-specific effects in word-picture access
Pictures and words differ both in their respective modes
of engaging the semantic system and in the cognitive
architectures that support such access (Huttenlocher &
Lui, 1979). People are typically faster and more accurate
when reading single words (i.e. oral reading latencies)
relative to naming corresponding pictures of the same
targets (Theios & Amrhein, 1989; Valente, Pinet, Alario,
& Laganaro, 2016). Interactive activation and cascaded
models of reading and repetition predict that input
and output phonological systems show connectivity,
thus affording a means of bypassing the semantic

LANGUAGE, COGNITION AND NEUROSCIENCE

system (Coltheart, 2006; Coltheart, Rastle, Perry, &
Langdon, 2001; Dell & O'Seaghdha, 1992). For example,
we can read aloud and repeat nonwords or words in
foreign languages without explicitly accessing their
respective meanings. Indeed, reading aloud without a
clear appreciation of meaning is one of the hallmark features of surface dyslexia (Woollams, Lambon Ralph, Plaut,
& Patterson, 2007). In contrast, most psycholinguistic
models of visual confrontation naming hold that pictures
must first engage the semantic system prior to executing
output form encoding (Potter & Faulconer, 1975; but see
Roth, Nadeau, Hollingsworth, Cimino-Knight, & Heilman,
2006).
When task demands require deep semantic processing, the word superiority advantage typically reverses.
This advantage for pictures over words has been
observed across numerous recall and recognition paradigms (Craik & Tulving, 1975; Nelson, Reed, & Walling,
1976; Waddill & McDaniel, 1992), and the phenomenon
has generally been interpreted in favour of a levels-ofprocessing approach to human memory (Pellegrino,
Rosinski, Chiesi, & Siegel, 1977). That is, pictures may
promote greater depth of perceptual and semantic processing, whereas words engage a more shallow linguistic
memory encoding strategy. In addition to traditional
recall and recognition experimental paradigms,
however, the picture superiority effect is also evident in
semantic categorisation tasks. For example, when cued
to determine whether a given item belongs to a
certain semantic category (e.g. animal), individuals are
typically faster with pictures than words (e.g. Potter &
Faulconer, 1975; Smith & Magee, 1980).
The etiology of the picture superiority effect remains
contested. One historically dominant account is derived
from Paivio's Dual Coding Theory (DCT) (Paivio, 1991).
DCT has been characterised within the broad class of
"multiple semantics" theories premised upon distinct
semantic systems for specific domains of knowledge.
Under DCT, orthogonal yet highly interactive systems
are dedicated to the processing and representation of
verbal vs. perceptual-based (imaginistic) codes. According to this view, pictures enjoy access to both perceptual
and verbal codes, whereas words are effectively shunted
towards the verbal system. Thus, pictures tend to evoke
more of a dually coded representation, and this additional semantic richness confers processing advantages
over words (Paivio, 1991, 2007).
Others have accounted for the picture superiority
effect within the context of unitary semantic theories
that do not hold the existence of distinct modalityspecific semantic stores. Caramazza's (1996) Organised
Unitary Content Hypothesis framework is one such
theory premised on the claim that all input modalities

1151

(e.g. vision, spoken, written language) converge upon
an amodal semantic system (Mahon & Caramazza,
2009).1 According to this view, pictures enjoy privileged
access to semantic knowledge relative to words whose
meaning must first be distilled through symbolic processes (Reilly, Peelle, Garcia, & Crutch, in 2016). Glaser
(1992) proposed an alternate model in which pictures
also have privileged access to the unitary semantic
system, but words require translational processing to
their mental lexicon before accessing the semantic
system (also see Glaser & Glaser, 1989; Seifert, 1997).
To our knowledge, the picture superiority effect has
exclusively been investigated within the context of
static object concepts (i.e. concrete nouns). Yet, there
exists a compelling rationale for investigating this phenomenon within the domain of action concepts. Neuropsychological case studies have demonstrated double
dissociations both in the comprehension and in the
expression of verbs and actions (Papeo & Hochmann,
2012; Papeo, Negri, Zadini, & Ida Rumiati, 2010). For
example, patients who sustain parietal lobe injuries
with subsequent ideomotor apraxia have been reported
to demonstrate a selective impairment of recognising
and executing motor gestures while showing relatively
spared verb knowledge in language processing tasks
(Rapcsak, Ochipa, Anderson, & Poizner, 1995). Conversely, the reverse dissociation has been reported among
patients with prefrontal lesions (e.g. nonfluent aphasia)
who demonstrate modality-specific impairment in
reading and understanding verbs while showing relative
preservation of the actions such words represent (Hillis
et al., 2002).2 Thus, the picture superiority effect is sometimes amplified, yet other times radically reversed in the
context of action/verb processing. Much remains unclear
about the nature of these modality reversals.
One intuitive empirical approach to deconstructing
action picture/word dissociations involves extrapolating
from a much more extensive knowledge base regarding
static object nouns. Szekely et al. (2005) cautioned
against such methodological assumption, arguing that
actions and objects fundamentally differ across numerous dimensions including, grammatical complexity,
argument and event structure, and morphophonological
complexity. Syntactically, verbs, the referents of action
concepts in language, are rich in grammatical marking
and argument structure (e.g. den Ouden, Fix, Parrish, &
Thompson, 2009). In addition, the semantic representation of actions has been considered to be shallower
and matrix-like; that is, members in the same action category may share fewer semantic features and be less hierarchically organised than objects (Huttenlocher & Lui,
1979; Miller & Fellbaum, 1991). Finally, direct comparisons of modality advantages for verbs and nouns are

1152

J. HUNG ET AL.

complicated by differences in the formal structures of
these word classes, including factors such as word
length, affixation, and unique prosodic marking (Monaghan, Christiansen, & Chater, 2007; Piantadosi, Tily, &
Gibson, 2011; Reilly, Hung, & Westbury, 2016; Reilly,
Martin, & Grossman, 2005). For these reasons, it can be
argued that actions and objects are ill suited to serve
as directly comparable "control" conditions for each
other.
The complex nature of action event concepts introduces potential variability when accessing action semantic knowledge from different input modalities. Action
event concepts, by definition, convey information
regarding interrelations (e.g. who does what to whom)
and/or spatial and temporal dynamic change (Chatterjee,
Southwood, & Basilico, 1999; Wu, Waller, & Chatterjee,
2007). Thus, action events include more sensorimotor
experience and can have more abstract information
than object concepts. For example, the action "falling"
not only implies a downward movement but also introduces a level of abstractness given its reference to
spatio-temporal relations. Some have argued that a
static action picture may not truly capture the dynamic
or abstract nature of the concept itself (e.g. d'Honincthun
& Pillon, 2008). A static action picture may only partially
represent one fractional moment of the entire process;
therefore, it is less natural than presenting the motion
of the entire action concept. While this proposition has
not yet been systematically investigated in the healthy
population, a number of clinical studies have shown facilitation of action naming and comprehension with videos
depicting actions as compared to static action pictures in
persons with frontotemporal dementia (d'Honincthun &
Pillon, 2008; den Ouden et al., 2009). Yet, no differences
have been observed between static pictures and videos
in terms of facilitation of action naming in persons with
aphasia (e.g. Berndt, Mitchum, Haendiges, & Sandson,
1997).
An action word can generate multiple expectations
regarding an action event, since it can be associated
with different doers, receivers, or other types of information such as instrument or location (McRae, Ferretti, &
Amyote, 1997). For example, the word "serving" can be
mentally imaged as a waiter serving a dish or a tennis
player serving a ball. Therefore, it is likely that single
action word may activate multiple perceptually dissimilar
events around a central theme. These expectations can
be further regulated by the argument structure of the
action words as different action words may be associated
with a specific number and optionality of arguments
(Fiez, 1997).
The question of whether the picture superiority
effect applies to actions/verbs remains open. Here, we

addressed this issue using eyetracking to examine the
dynamics of action processing during semantic categorisation. Eyetracking is a potentially powerful tool for
investigating fine-grained processing, especially with
regard to the moderating effects of modality on evaluating semantic associations between words vs. pictures
(e.g. walk-hike) (e.g. Kukona, Fang, Aicher, Chen, &
Magnuson, 2011; Mirman & Magnuson, 2009; Odekar,
Hallowell, Kruse, Moates, & Lee, 2009; Yee, Overton, &
Thompson-Schill, 2009). We specifically investigated
advantages conferred by representational format
(e.g. word vs. picture) in processing semantic associative relations. We hypothesise that action concepts
are imbued with features such as motor simulation
and thematic information (Chatterjee et al., 1999;
McRae et al., 1997; Wu, Morganti, & Chatterjee, 2008),
and their associated representations are inherently
relative rather than absolute (e.g. kick evokes a
dynamic spatio-temporal schema that may apply to a
human foot, a goat's hoof, etc.) (Bird, Howard, & Franklin, 2003; Gentner, 1982). The static visual depiction of
actions is context-specific, and the action components
often must be inferred through indirect affordance
data (e.g. falling is inferred by the biomechanics of
an impossible angle for standing). These properties
will, in turn, produce a reverse picture superiority
effect such that action words will engage the semantic
system faster and more accurately than action images.
These effects will be evident in gaze metrics that
assess semantic relations between action concepts.
Hence, we expected that participants' eye movements
would indicate a greater demand in processing action
pictures (e.g. longer duration and/or more fixation
counts, slower to look to the correct answer) than
action words.

Methods
We report two eyetracking experiments designed to
examine the dynamics of associative semantic processing for actions as moderated by representational modality (e.g. picture vs. word). In Experiment 1, we
examined gaze dynamics for one common neuropsychological measure of associative semantic processing
among triads of actions, the Kissing and Dancing Test
(KDT) (Bak & Hodges, 2003). An advantage of using an
off-the-self neuropsychological measure such as the
KDT is that it affords inspection of representational modality (picture-word) within the context of a well-worn
test with wide neuropsychological precedence. A significant disadvantage of the KDT is that it was not optimised
for eyetracking. In Experiment 2, participants made categorical judgements of association for pairs of actions

LANGUAGE, COGNITION AND NEUROSCIENCE

better matched on a variety of perceptual and psycholinguistic factors that impact gaze (e.g. visual complexity,
associative strength).
Experiment 1 was conducted at University of Florida.
Experiment 2 was conducted at Temple University. All
participants provided informed consent in accord with
the local institutional review board. Inclusion criteria
were as follows: (1) age range of 18-30 years, (2) righthanded, (3) native English speaker, (4) normal or corrected to normal vision, (5) normal hearing, and (6) no
history of self-reported neurological or psychiatric disorders, (7) global cognition within age-based norms (i.e.
>26 score) on the Montreal Cognitive Assessment
(MoCA; Nasreddine et al., 2005), (8) No glare resistant
eyeglasses or hard contact lenses. Participants removed
mascara or other eye makeup due prior to eyetracking
in order to avoid light deflections (e.g. misinterpretation
of black eyeliner as the pupil).

1153

the original paper and pencil versions of the KDT by
scanning each stimulus card and cropping individual
images as bitmap files of a standardised size (800 x
600 pixels) to be displayed on the PC.

We examined gaze dynamics in conjunction with behavioural measures of accuracy and reaction time while
participants completed a semantic associative task for
triads of actions presented in either word or picture
format. We used a well-established neuropsychological
measure of action processing, in which participants
judge semantic relatedness from a triad of pictures or
words arrayed in a triangle. The word or action at the
top of the triangle (e.g. Writing) serves as a probe.
From the two choices at the base of the triangle participants must select the most semantically associated
target item (e.g. typing), thereby rejecting a foil (e.g. stirring). Targets and foils appear with equal likelihood at
either the left or right base of the triangle. The KDT is
composed of 52 discrete trials with analogous picture
and word versions for the same set of concepts (e.g. dripping water in picture form vs. the word, DRIPPING) (Bak &
Hodges, 2003).

Pair-relation survey
We conducted offline normative surveys to evaluate the
strength of associative relations and visual complexity
differences between items of the KDT. Two groups of
participants, none of whom participated in the experimental task, rated visual similarity, semantic similarity,
and semantic association strength for all KDT trials via
a 5-point Likert scale. The first group (N = 11, mean
age = 19.4 yrs, SD = 1.03) judged the pair relations in
the form of pictures, whereas the second group (N =
10, mean age = 23.2 yrs, SD = 1.87) judged pair relations
in the form of words. A probe positioned on the upper
portion of the screen guided the selection of two possible choices positioned below (target and foil). We
assessed relations between all pairwise combinations
for each trial (i.e. probe-target, probe-foil, foil-target)
with the goal of ensuring that the probe-target relationships were not explicitly biased in terms of variables that
could potentially contaminate the gaze metrics. For the
first scale (visual similarity), participants subjectively
judged visual similarity for all pairs of items (e.g.
biting-barking). For the second scale (semantic similarity), participants judged similarity of meaning between
pairs of items (e.g. walking-hiking). In the final scale
(association strength), participants judged the strength
of association between pairs of stimuli (e.g. running-
falling). Table 1 depicts trial-level norms on a 5-point
Likert scale for all item pairs for the KDT.
The norming surveys revealed that the probe-target
relationship was more associated relative to the other
pairwise possibilities within each triad (i.e. probe-foil or
target-foil) (p < .05) and that these differences were
comparable across the picture and word versions of
the test.

Participants

Apparatus

Participants (N = 42) included undergraduates from the
University of Florida. Participant demographics were as
follows: mean age (years) = 19.79 (SD = 1.37), mean education (years) = 14.31 (SD = 1.14), mean MoCA cognitive
= 27.98 (of 30 possible) (SD = 1.51).

We presented the stimuli to participants and monitored
various aspects of eye movements using an infrared,
table-mounted eyetracking system (SMI iView X RED eyetracker) (SensoMotoric Instruments Inc., Boston, MA).
This eyetracker samples binocular eye movements at a
rate of 120 Hz with spatial resolution < 0.03.
The eyetracker setup at the University of Florida
included a hydraulic chair fixed at a horizontal distance
of 47 inches (120 cm) from a 46-inch LED television
screen. The infrared eye detector was positioned on a
hinged platform that swung down over the participant's

Experiment 1: the KDT

Materials
We adapted the KDT test words to standard American
English by replacing the word "hoovering" with
"vacuuming" and "posting" with "mailing". We modified

1154

J. HUNG ET AL.

Table 1. Mean and standard deviation of the 5-point Likert scale for the pair-relation survey for KDT.
Picture

Word

Rating

Probe/target

Probe/foil

Target/foil

Probe/target

Probe/foil

Target/foil

Visual similarity
Semantic similarity
Semantic association

1.83 (0.57)
2.43 (0.72)
3.21 (0.97)

1.38 (0.42)
1.24 (0.35)
1.42 (0.46)

1.74 (0.57)
1.39 (0.39)
1.80 (0.75)

1.84 (0.63)
2.1 (0.90)
3.35 (0.92)

1.12 (0.25)
1.13 (0.36)
1.32 (0.45)

1.29 (0.43)
1.21 (0.31)
1.56 (0.70)

Notes: 1: not at all similar/associated; 5: very similar/associated.

legs. Participants were seated at the eyetracker and
positioned for optimal recording. We then conducted
a 5-point calibration procedure, accepting a minimal
tolerance of 0.5. After calibrating the eyetracker, participants initiated a brief familiarisation sequence and
then began the experiment.

Behavioural
For accuracy and reaction time, we converted raw scores
(X of 104 triads) to per cent accuracy and contrasted
reaction time differences exclusively for correct
responses. We conducted a repeated ANOVA with
input modality (i.e. picture or word) as the fixed factor
for the subject and item analyses.

Procedure
Participants completed a total of 104 trials (2 blocks)
corresponding to the 52 triads of the KDT (i.e. KDTpics,
KDTwords). Stimuli were presented in random order
within each block, and the order of two blocks was
counterbalanced across participants. During testing
we included a washout period between blocks to
disrupt memorisation of the previous trials. That is,
there was a 10-min break during which participants
completed two paper and pencil tasks, the Shipley
Vocabulary Test (Zachary, 1986) and the MoCA (Nasreddine et al., 2005). We recalibrated the eyetracker
using the same 5-point calibration method prior to
initiating each block.
Each discrete trial was composed of a triad of pictures
or words arrayed in triangular format. We used a gaze
contingent fixation procedure to start each distinct trial.
That is, participants were instructed to concentrate on a
central fixation point. Immediately upon 1000 ms of
cumulative gaze time to a small bounding box surrounding the fixation, the next trial automatically advanced.
This process ensured that participants initiated their
visual search from the same spatial location on every trial.
On each individual trial, participants selected the strongest associated picture/word from the bottom two choices
matched to the top probe stimulus via button press. Three
practice trials preceded the actual experiment. Participants
judged which one of the two pictures/words was most
strongly associated with the probe image by keying "z"
for the left-positioned item and "m" for the right-positioned item. After completion of these practice items, the
experimental items were presented.

Data recording and design
We conducted subject (F1 or t1) and item (F2 or t2) analyses using repeated analysis of variance (ANOVAs) for
the accuracy, reaction time, and gaze data.

Eyetracking
For the gaze measures, eye movements were recorded
from the offset of the fixation cross until the response
for each triad. We focused on the dynamics of gaze
fixation within three areas of interest (hereafter AOIs)
corresponding to the probe (AOIprobe), its semantic
associated target (AOItarget), and the foil (AOIfoil)
within each trial. We operationally defined fixations as
dwell times within a circumscribed region, exceeding
80 ms in duration. Other events, including saccades
and blinks, were filtered from the analyses during post
processing.
The crucial question is whether there are differences
in the latency and duration of target fixations as a
result of input modality (picture/word). Therefore, we
conducted a target fixation analysis, including proportion of cumulative gaze time within the AOItarget as quantified as the fixation proportion over time and the latency
to first target fixation to evaluate the amount and time
course of visual attention to the correct answer (i.e.
AOItarget).
We first analysed fixation proportion over time to
evaluate visual attention allocated to the target over
a 3000 ms time period. We binned the eyetracking
data in 50 ms increments subsequent to trial onset
(Altmann & Kamide, 1999). We then contrasted
these bins as a series of discrete time windows out
to 3000 ms after initiation of each trial. Thus, each
trial was composed of 61 aggregated time bins. Fixation proportion over time summed the amount of
fixation duration to AOItarget in each time bin during
3000 ms time period. Second, the latency to first
target fixation was analysed using the timing of fixation first entering the AOItarget . Similar to the behavioural analyses, we entered the eyetracking output
into repeated ANOVAs (i.e. by subject and by item)
with the same predictor (i.e. modality).

LANGUAGE, COGNITION AND NEUROSCIENCE

Table 2. Summary of the descriptive and F statistics for accuracy
and reaction time between modality (picture, word) using
subjects (F1) and items (F2) analyses for KDT.
Modality
Accuracy (%)
Reaction time (ms)

Picture

Word

F1(1,39)

F2(1,49)

0.94 (0.06)
2533.4 (574.8)

0.96 (0.04)
2102.6 (468.2)

11.68***
59.44***

5.89*
25.27***

*Significant main effect where p  .05.
*** Significant main effect where p  .001.

Results
Data trimming
Within participants, we eliminated reaction time outliers that exceeded three standard deviations of the
average response time. We also excluded two participants who performed with exceptionally poor accuracy (z < -3). At the item level, we excluded two
items from KDT that elicited near chance accuracy
across participants (planting-sweeping-sowing and
ripping-erasing-sewing). The implementation of
these trimming procedures resulted in retention of
>90% of the original data.

Accuracy
The ANOVA for accuracy revealed a main effect of modality (word or picture). For the KDT, word triads elicited a
small but significantly higher accuracy than pictures
[t1(39) = 3.63, p = .001; t2(49) = 2.42, p = .02].

Reaction time
The ANOVA for reaction time revealed a significant
main effect of modality. Participants took longer to
process action pictures than action words [t1(39) =
7.71, p < .001; t2(49) = 5.03, p < .001]. Figure 1 shows
the accuracy (panel a) and reaction time (panel b)
results, and Table 2 summarises the descriptive
statistics and ANOVA results of the behavioural
measures.
Fixation proportion over time. Action pictures elicited
more fixation duration over 3000 ms than action words,
but the difference did not reach statistical significance
(p > .05).
Latency to first target fixation
Main effect of modality showed a more rapid latency to
the target fixation for words relative to pictures [t1(39) =
7.63, p < .001; t2(49) = 7.03, p < .001]. The descriptive statistics and ANOVA results of the target fixation analyses
are summarised in Table 3.

1155

Interim discussion: experiment 1
Results were consistent across items and subjects,
lending confidence to the stability of the observed
effects. Neurologically healthy participants typically
perform at or near ceiling on the KDT (Klein & Buchanan,
2009). This is an artefact of the test's design that often
hinders parametric comparisons of the data (i.e. controls
have minimal variance). Within actions, there was a small
modality advantage in accuracy for words > pictures (2%
difference), but these differences are not directly attributable to confounds in association strength, visual, or
semantic similarity. We also analysed response latency
(via button press) and found that participants were
faster to judge triads of action words relative to action
pictures.
These behavioural findings revealed a reversed
picture processing advantage for actions. Participants
showed prolonged reaction time difference among
action pictures and words. This interaction was consistent with a reversal of the picture superiority effect in
that participants took longer to complete semantic associations for pictures than words (400 ms difference)
exclusively in the action domain. These behavioural findings received complementary support from the more
sensitive eyetracking measures that follow.
In the latency-to-first-target-fixation analysis we
examined the speed with which participants attained
their first visual fixation to the AOItarget. This eyetracking measure is commonly used to evaluate the early
time course of semantic associative processing
(Holmqvist et al., 2011). We hypothesised that if the
surface form of pictures provides rather direct access
to semantic knowledge, a shorter latency to target
should be reported in the picture condition. On the
contrary, pictures elicited slower latencies to the
target than words for actions. Thus, the latency-tofirst-target-fixation analysis reveals another reversal
of the picture superiority effect in that words facilitated recognition.
In the fixation proportion-over-time analyses, we
examined the amount of cumulative fixation duration
over the 3000 ms time window beginning with the
onset of each trial. A higher magnitude of fixation duration is associated with a deeper and more effortful cognitive processing (Holmqvist et al., 2011). We only
observed a trend of greater fixation proportion over
3000 ms when viewing action pictures compared to
action words, indicating a tendency of processing
demand in action pictures.
We analysed two components of visual attention (i.e.
latency to first target fixation, and fixation proportion
over time) in an effort to evaluate the dynamics of

1156

J. HUNG ET AL.

Figure 1. Behavioural results of accuracy and reaction measures between modality (picture, word).

action and object processing. Each of these measures
informs a different processing component. However, a
general trend across analyses indicated a processing
advantage of words over pictures for actions. Specifically,
this trend was largely driven by the fact that participants
tended to dedicate more attentional resources to action
pictures relative to words.

Experiment 2: semantic associative judgement
task
The KDT represents one of the most widely utilised
assessments of associative semantic processing of
actions. We employed this measure to examine gross
discrepancies in processing as a function of input modality, demonstrating a reversal of the picture superiority
effect for actions. There are, however, inherent limitations to the inferences that can be drawn from the
test. The assessment was not optimised for eyetracking,
and potential confounding effects from other psycholinguistic variables (e.g. word associative strength) are
also problematic. For this reason, we conducted a
second, better controlled semantic categorisation task
examining pairs of actions, by manipulating input
modality for a range of actions varied both in terms of
self-initiated action (e.g. walking vs. melting) and perceptual similarity. In this experiment, participants
viewed pairs of actions and semantically judged relatedness (yes/no) as we tracked eye movements. This
semantic judgement task employed pictorial and orthographic presentation of the same items, allowing modality performance contrasts.

Participants
Participants in the main experiment included healthy
young adults (N = 35, 9 male) recruited from Temple

University. Participant demographics were as follows:
Mean age (years) = 21.5 (SD = 2.38); mean education
(years) = 14.75 (SD = 1.40), mean MoCA cognitive =
27.84 (of 30 possible) (SD = 1.46).

Materials
We obtained line drawings of actions from a variety of
open-source databases (Akinina et al., 2015; Druks &
Masterson, 2000; Schwitter, Boyer, Meot, Bonin, & Laganaro, 2004; Szekely et al., 2004). We also rendered
picture stimuli as line drawings using Adobe Photoshop CS6's filter feature. All stimulus slides were standardised to 500 x 500 pixels. The lexical stimuli were
presented in lowercase black letters (Arial, 50 point)
on a white background with the present progressive
marker -ing added.

Lexical and visual norming
A total of 62 new participants, independent from the
main experiment, were recruited for stimulus
norming. A series of norming studies were conducted
to match the experimental stimuli on a range of psycholinguistic and perceptual variables. All action
words (N = 142) used in the experimental task were
entered into orthogonal norming surveys to evaluate
their lexical and visual characteristics. The single word
norming tests included an action category rating (i.e.
can you perform with your body parts), visual motion
rating (i.e. how much visual movement you can image
given a written word) (Bedny, Caramazza, Grossman,
Pascual-Leone, & Saxe, 2008), subjective familiarity
rating, subjective concreteness rating, lexical frequency
(Brysbaert & New, 2009), word length (i.e. number of
letters), naming agreement, and objective visual complexity (Szekely & Bates, 2000). Thirty-six young adults
(5 males, mean age = 20.1 yrs, SD = 1.31) from the

LANGUAGE, COGNITION AND NEUROSCIENCE

1157

Table 3. Summary of the descriptive and F statistics for target fixation measures (ms) for 3000-ms time window using subjects (F1) and
items (F2) analyses.
Modality
Picture

Word

F1(1,60)

F2(1,60)

Fixation proportion

17.98 (11.53)

16.76 (12.98)

Time to first fixation

1173.86 (221.96)

928.53 (235.02)

1.17
F1(1,39)
58.19***

1.00
F2(1,49)
49.37***

#

Fixation proportion reflects the fixation duration (in percentage) to AOItarget relative to the other two AOIs aggregated over 61 trials (every 50 ms) over the
observation window (3000 ms).
*** Significant main effect where p  .001.

#

University of Florida were randomly assigned to several
word norming questionnaires depending on how many
questionnaires they could finish within one hour. Questionnaires included action category, visual movement,
familiarity, and concreteness, all based on a 7-point
Likert scale. Twelve additional participants were
recruited for the naming agreement rating at Temple
University. Participants were seated at a Dell laptop
with E-prime software, which displayed an action
picture for 3 s preceded by a fixation cross for 400 ms
followed by a scrambled image for 500 ms. Participants
were asked to generate a name for the action image
verbally as quickly and accurately as possible during
the 3 s of projection. Items were counted as correct if
norming participants gave the same label as the
picture was originally designed. If most participants (>
80%) gave a different label with consistency (e.g.
name a picture of "skidding" as "swerving"), the original
label was replaced for the picture. Furthermore, synonyms were coded as incorrect (e.g. "taking off" for
"launching"). After single word norming was completed, another normative study of action pair relations
was also conducted to determine semantic similarity,
pair association, and visual similarity. For this study,
14 additional subjects were recruited from the online
crowd sourcing programme Amazon Mechanical Turk
(www.MTurk.com) (see Buhrmester, Kwang, & Gosling,
2011 for a description and validation of the approach).
Table 4 depicts the psycholinguistic characteristics of
the action stimuli and Table 5 shows the pair-relation
results. Norming results indicated that action stimuli
used in Experiment 2 have high familiarity, concreteness,
and naming agreement.

Forty pairs of physical actions (e.g. kneading-squeezing) and 40 pairs of non-physical actions (e.g. leaking-
dripping) served as experimental stimuli. Pairs were
related either based on the similarity of the manner of
action (e.g. jumping and skipping are performed similarly) or related associatively (e.g. pouring and straining
co-occur within kitchens). Items with mean Likert scale
ratings higher than 4 on a 7-point Likert scale were considered to be related/associated. Eighty unrelated pairs,
half physical half non-physical actions, were also quasirandomly created by taking a probe used in a related
pair and pairing it with another probe. The same selection rules applied to the target. In this manner, all
items appeared in both positions (i.e. probe, target).
The unrelated pairs were examined for association or
similarity. The master experiment stimulus list contained
80 related and 80 unrelated stimulus pairs. Two sub-lists
(i.e. A or B) were created and counterbalanced in terms of
pair-relation ratings, such as manner similarity, association strength, and visual similarity (p > .05 all). Both
sub-lists contained 40 related and 40 unrelated stimulus
pairs. Participants were presented with both lists but half
of the participants received list A first, whereas the other
received list B first. Trial order was randomised for each
participant.

Apparatus
We tracked eye movements using an SMI RED-M
120 Hz eyetracking system housed in a windowless
testing room at Temple University. Participants were
positioned on an optical chin-rest 20 inches (50 cm)
away from an infrared eyebar positioned at the base

Table 4. Mean characteristics of action stimuli.
Action type

%NA

SVC

REM

VM

Fam

CNC

Freq

WL

Physical
Non-physical

.86 (0.16)
.84 (0.15)

29.60 (8.30)
28.09 (10.59)

6.10 (0.59)
3.68 (1.33)

5.05 (1.03)
4.49 (0.99)

6.54 (0.41)
6.37 (0.58)

6.20 (0.48)
5.74 (0.55)

57.2 (89.5)
43.7 (71.8)

4.5 (1.02)
4.7 (1.18)

Notes: CNC: subjective concreteness rating on a 7-point Likert scale (1 = not at all concrete); Fam: subjective familiarity rating on a 7-point Likert scale (1 = not at
all familiar); Freq: SUBTL frequency in third-person singular form; WL: length in third-person singular form; %NA: percentage of name agreement; REM: relative
embodied experience with regard to the extent participants can perform with their body parts on a 7-point Likert scale (1 = Not at all possible/non-physical),
SVC: subjective visual complexity (KB).

1158

J. HUNG ET AL.

Table 5. Mean and standard deviation of the 7-point Likert scale
for the pair-relation survey.
Semantic
similarity

Association
strength

Visual
similarity

Similarity
Association
Similarity

4.93 (0.61)
3.28 (0.51)
4.99 (0.75)

5.01 (0.93)
5.12 (0.71)
5.07 (0.89)

3.76 (1.16)
3.88 (1.13)
3.75 (1.26)

Association

3.52 (0.80)

5.06 (0.72)

3.85 (1.46)

Pair relation
Physical
Nonphysical

Notes: 7-point Likert scale ratings (1 = low similarity/association, 7 = high
similarity/association) were obtained from the Amazon Mechanical Turk
(Buhrmester et al., 2011) (N = 14; 8 male, mean age = 35.5 yrs, SD = 10.3).
Raters were asked to judged for (1) the similarity of manner of performance,
(2) the associative strength, and (3) the perceptual similarity between pairs.
Physical actions refer to actions that people usually perform with their body
parts (e.g. clapping), whereas non-physical actions are those we observe
(e.g. dripping).

of a 17-inch monitor. The same 5-point calibration procedure as Experiment 1 was implemented before the
main experiment.

Procedure
Participants were asked to judge whether pairs of concepts that appeared on the screen are related (i.e. do
two actions go together?). They were to respond to
each pair by verbally responding "Yes" or "No" as
quickly and accurately as possible. Participants were
instructed to think about all aspects of a concept. For
example, pairs that "go together" could be similar in
the way they were performed such as "wiping" and
"rubbing," or associated in meaning such as "reading"
and "writing."
Trials consisted of two forms of stimulus type - one
with pictures and an analogous version with words. Participants first viewed a fixation cross and heard a simultaneous pure tone for 600 ms to begin each
presentation. A probe and a scrambled image of the
same size positioned horizontally to either the right or
left (randomised) appeared on the screen for 500 ms followed by a brief presentation of a white screen (250 ms).
Participants then viewed the probe-target pair (perceived as the stimulus appearing where there was
before a scrambled image). The probe-target pair
timed out after 3000 ms if there was no response, in
which case the trial was counted as an error. The intertrial structure contained another white screen
(1000 ms) followed by a mask (i.e. #######) (600 ms).
Figure 2 depicts the trial procedure.
The experiment consisted of 320 action trials (half in
pictorial form and half in word form). Picture and word
targets were presented in separated blocks, within
each two balanced lists (i.e. A or B). Each list contained
80 stimulus pairs (half consistent, half inconsistent). The
orders of the presentation of blocks, stimulus

presentation within the list, and the location of probe/
target (i.e. left or right) were randomised four times (2
input modality x 2 loci on the screen). Before the main
experiment, a practice section with feedback on correctness was implemented.

Data analyses
Behavioural
Participants' verbal responses were recorded via
TASCAM digital recorder for offline accuracy and reaction
time coding. Reaction time was first labelled and measured from the onset of fixation tone to the onset of
their verbal response by marking latencies within the
waveform using the Audacity waveform editor. We
then analysed response accuracies and latencies by
subject (F1, t1) and by item (F2, t2) analysis using repeated
and mixed ANOVA. Fixed factors included modality
(picture, word) and category (physical, non-physical).
Eyetracking
Eye movements were recorded from the onset of the
fixation cross to the neutral mask (i.e. #######) for all
experimental trials. A target fixation analysis was conducted to examine a series of gaze measures inside the
target area of interest (AOItarget). The variables of interest
were the number of fixations inside an AOI and the total
fixation duration in an AOI to previously visited AOI. We
focused on the above gaze measures within the AOItarget
in the target slide (3000 ms).
The number of fixations within an AOI reflects the raw
count of discrete fixations in the AOI during the presentation time of the slide (i.e. 3000 ms). Total number of
fixations is indicative of search efficiency and/or task difficulty. More fixations, revisits, and greater saccade
amplitude are associated with increased processing
demands (Holmqvist et al., 2011). We entered the eyetracking measures into mixed-model ANOVAs with the
same predictors as the behavioural analyses (i.e. modality, category) for the AOItarget by subject and item analysis, respectively.
We eliminated erroneous and lengthy responses
from statistical analyses. Fourteen per cent of the
total numbers of experimental trials were classified
as errors, including response errors (i.e. no answer),
and incorrect responses. We conducted data trimming both by subject and by items. One participant
was excluded due to exceptionally poor accuracy (z
< -3) in one or more of her experimental conditions.
At the item level, five item pairs were excluded due to
eliciting poor accuracy across subjects (z < -3). The
final analyses reflect retention of 82% of the original
data.

LANGUAGE, COGNITION AND NEUROSCIENCE

1159

Figure 2. Experimental trial procedure.

Results
Behavioural
Both subject and item analyses showed significant
main effects of modality and category for response
accuracy. Participants more accurately identified
relatedness of action words relative to action pictures
[0.82 vs. 0.74, t 1(34) = 4.9, p < .001; t 2 (73) = 3.5, p
< .001]. Non-physical action relations were more
accurately identified than physical actions [0.81 vs.
0.75, t 1(34) = 5.4, p < .001; t2 (73) = 2.0, p = .051]. A significant main effect of modality for reaction time
showed that participants took significantly longer to
identify action pictures relative to action words
[1034 vs. 949 ms; t1 (34) = 4.5, p < .001, t2 (73) = 5.4, p
< .001] (see Tables 6 and 7).

Eyetracking
Both subject and item analyses showed a significant
main effect of modality for all eyetracking measures. Participants fixated more [4.2 vs. 3.3; t1(34) = 14.5, p < .001,
t2(73) = 24.1, p < .001] and for a longer duration [1132
vs. 1049 ms; t1(34) = 5.6, p < .001, t2(73) = 16.8, p < .001]
on action pictures relative to action words (see Tables
8 and 9). However, there were no significant main
effect of category and no interaction.

Secondary analysis
A secondary analysis was conducted to evaluate the
effects of pairwise relations (i.e. semantic similarity, pair
association). The same item and subject analyses were
conducted with an additional fixed factor, relation.

Accuracy
A main effect of modality showed that participants were
more accurate with action words than action pictures
[0.81 vs. 0.75, t1(34) = 3.53, p = .001; t2(71) = 2.52, p
= .01]. A significant 3-way interaction effect indicated
that participants had higher accuracy with words than
picture for both action categories when pairs shared
semantic similarity. There was a trend towards a
picture advantage for physical actions and a word advantage for the non-physical actions when pairs were associatively related.
Reaction time
Participants took longer to judge action pictures relative to action words [1062 vs. 976 ms, t1(34) = 4.37, p
< .001; t2(71) = 5.11, p < .001]. The significant two-way
modality * relation interaction reflected a greater
effect of longer reaction times in pictures than words
when pairs were semantically similar [1085 vs.
957 ms, t1(34) = 6.16, p < .001; t2(71) = 6.56, p < .001].
Moreover, a two-way relation * category interaction
demonstrated faster reaction times to judge physical
actions relative to non-physical actions when
word-picture pairs were associatively related [992 vs.
1042 ms, t1(34) = -3.44, p = .002; t2(71) = -1.28,
p = .206]. In contrast, word/action pairs that were
related by semantic similarity elicited the reverse
pattern, characterised by faster reaction times to
non-physical relative to physical actions [1001 vs.
1041 ms, t1(34) = -3.57, p = .001; t2(71) = -1.40,
p = .167].
Table 7. Summary of F statistics for the group ANOVA results
examining accuracy and reaction time for modality (picture,
word) and category (physical, non-physical) using subjects (F1)
and items (F2) analyses.
Accuracy

Table 6. Mean and standard deviation of the accuracy and
reaction time (ms) for association decisions.
Accuracy
Reaction time

Picture
Word
Picture
Word

Physical

Non-physical

0.71 (0.15)
0.79 (0.12)
1035 (258)
958 (214)

0.77 (0.12)
0.84 (0.13)
1034 (256)
939 (224)

Modality
Category
Modality x category

Reaction time

Subject
F1(1,34)

Item
F2(1,73)

Subject
F1(1,34)

Item
F2(1,73)

23.84***
28.44***
0.64

12.16***
3.93*
0.05

20.10***
0.99
1.74

28.70***
0.36
0.27

*Significant main effect where p  .05.
***Significant main effect where p  .001.

1160

J. HUNG ET AL.

Table 8. Mean and standard deviation of the gaze measures in
AOItarget for association decisions.
Fixation count
Fixation duration (ms)

Picture
Word
Picture
Word

Physical

Non-physical

4.2 (0.6)
3.3 (0.6)
1130 (176)
1049 (189)

4.2 (0.5)
3.3 (0.6)
1133 (162)
1049 (196)

Eyetracking
Both the subject and item analyses showed significant
main effects of modality in the two eyetracking measures. Participants demonstrated more fixation counts
[4.2 vs. 3.3, t1(34) = 15.1, p < .001; t2(71) = 22.6, p < .001]
and longer fixation duration [1133 vs. 1049 ms, t1(34) =
5.64, p < .001; t2(71) = 15.8, p < .001] on action pictures
than action words. Tables 10-12 reflect descriptive statistics and ANOVA results for both the behavioural and eyetracking measures.

Interim discussion: experiment 2
Participants demonstrated a strong processing advantage for orthographic relative to pictorial representations of action concepts. These effects were evident
both in behavioural response latencies and in gaze
metrics. Participants spent longer and were more
error prone when making action semantic judgements
for pictures relative to words. In addition, participants
showed more visual fixations and also fixated a longer
duration on action pictures relative to words. These
converging behavioural and eyetracking measures are
consistent with the results of Experiment 1. Namely,
participants demonstrate a reversal of the picture
superiority effect when making judgements of semantic
similarity for actions.
There are, however, a number of caveats to consider
regarding this overarching finding.
Although pictures elicited an advantage over words,
these main effects could have potentially masked a
complex interaction between action relation and
action type. Participants judged a range of actions that
could be performed either by oneself (i.e. endogenous
Table 9. Summary of F statistics for the group ANOVA results
examining gaze measures in AOItarget for association decisions
for modality (picture, word) and category (physical, nonphysical) using subjects (F1) and items (F2) analyses.
Fixation count
Subject
F1(1,34)

Item
F2(1,73)

Modality
209.56***
589.49***
Category
.70
.02
Modality x category
2.27
1.09
***Significant main effect where p  .001.

Fixation duration (ms)
Subject
F1(1,34)

Item
F2(1,73)

31.36***
.06
.08

282.39***
.16
.04

physical actions) or actions that denote changes of
state within the external environment (i.e. exogenous
non-physical actions). Additionally, within each of
these domains, the actions could be linked either
through associative or similarity relations. For example,
"laughing" and "clapping" are perceptually dissimilar
endogenous actions that tend to co-occur in the same
scenario. In contrast, "waving" and "slapping" are contextually unassociated actions that share high perceptual similarity. When we considered relation (similarity/
association) as an independent factor, we discovered
an unexpected relation * domain interaction. This interaction was such that for associatively related actions,
participants were more accurate to identify pictures of
endogenous physical actions, whereas words were
faster and more accurately identified in the exogenous
non-physical action condition. In contrast, actions that
were perceptually similar consistently demonstrated a
word > picture advantage (i.e. a reverse picture superiority effect) regardless of whether the actions were endogenous or exogenous.
One might speculate that pictures of self-executed
actions confer an "embodiment" advantage over words
in the context of limited support from associated event
schemas (i.e. actions are related but do not occur within
the same context) (Dove, 2009; Reilly et al., 2014; Reilly,
Rodriguez, Peelle, & Grossman, 2011). However, this
hypothesis is tenuous, given that we employed an unbalanced design in which many more stimuli were associatively rather than perceptually related. Nevertheless,
there may be considerable merit in unpacking this interaction within future work that examines whether reversal
of the picture superiority effect is indeed a blanket property of actions or alternatively whether the effect only
applies to specific classes of actions (e.g. actions a
person can execute with his/her body).

General discussion
An extensive body of past research within the domain of
static objects has shown that semantic access differs in a
variety of ways for word vs. picture input. There is also evidence to suggest that differences in representational
modality impact (or at least colour) higher-level semantic
processes. Saffran, Coslett, and Keener (2003) compellingly demonstrated these downstream effects of modality in a word generation experiment wherein healthy
young adults generated more action verbs when cued
with an object picture relative to its analague in orthographic form. Here, we investigated the temporal
dynamics of associative semantic processing for actions
as a function of input modality (Picture or Word).
Actions depicted as words were faster and more

LANGUAGE, COGNITION AND NEUROSCIENCE

1161

Table 10. Summary of descriptive statistics for behavioural and gaze measures in AOItarget for modality (picture, word), category
(physical, non-physical), and relation (similarity, association).
Modality
Picture

Relation

Category

Association

Phy
Non-Phy
Similarity
Phy
Non-Phy
Word
Association
Phy
Non-Phy
Similarity
Phy
Non-Phy
Notes: Non-Phy: non-physical action; Phy: physical action.

Accuracy

Reaction time (ms)

Fixation count

Fixation duration (ms)

0.83 (0.16)
0.73 (0.15)
0.66 (0.16)
0.79 (0.12)
0.76 (0.16)
0.79 (0.15)
0.80 (0.12)
0.87 (0.12)

1007 (265)
1071 (270)
1100 (277)
1070 (282)
977 (263)
1014 (265)
982 (219)
932 (218)

4.2 (0.6)
4.2 (0.5)
4.2 (0.6)
4.2 (0.5)
3.3 (0.6)
3.3 (0.6)
3.4 (0.6)
3.3 (0.6)

1127 (190)
1142 (162)
1133 (172)
1128 (163)
1042 (199)
1052 (203)
1053 (188)
1048 (196)

accurately classified than the same concepts presented in
picture form. These results suggest that actions elicit a
reversal of the canonical picture superiority effect that
occurs among static objects. Although our results
cannot support any claims that adjudicate any models
for the organisation of semantic systems (i.e. our results
cannot conclude that the reversed pictorial superiority
effect for action concepts favours the unitary or multiple
semantic system), the presence of a true reversal of the
picture superiority effect has significant implications for
our understanding of action and object processing.
Past work within the domain of static objects has fostered the assumption that images inherently trump
words during semantic access. Our results, however,
suggest a more nuanced interpretation of this finding.
Namely, actions and objects may be subject to a different
set of processing constraints, characterised by a linguistic
bias for accessing action representations relative to a
perceptual bias for objects. The etiology of this phenomenon remains unclear. In the section to follow, we outline
a range of possible explanations.

Why do action words speak louder than action
pictures?
One possible reason why action words manifest an
advantage over action pictures is that pictures can
Table 11. Summary of F statistics for the group ANOVA results
examining accuracy and reaction time for modality (picture,
word), category (physical, non-physical) and relation (similarity,
association) using subjects (F1) and items (F2) analyses.
Accuracy

Reaction time

Subject
F1(1,34)

Item
F2(1,71)

Subject
F1(1,34)

Item
F2(1,71)

Modality
12.35***
Relation
0.06
Category
10.54**
Modality x relation
38.67***
Modality x category
4.28*
Relation x category
31.83***
Modality x relation x category 43.65***
*Significant main effect where p  .05.
**Significant main effect where p  .01.
***Significant main effect where p  .001.

6.34***
0.012
0.24
7.78**
0.46
4.52*
5.75*

19.07***
0.14
0.31
14.62***
3.52
22.06***
0.03

26.07***
0.02
0.06
6.09*
0.49
3.42
0.003

only capture a static snapshot of a complex, spatiotemporally unfolding event (e.g. falling). It is difficult if not
impossible to convey the multitude of spatio-temporal
and thematic information associated with actions
within the context of static photographs. The representation of falling, for example, must be inferred through
unnatural/impossible object orientation when viewing a
photograph of a falling vase, whereas perception of the
actual event yields additional motion detail such as
path/manner properties as well as an eventual
outcome (e.g. a vase shattering on the floor) (Pruden,
Hirsh-Pasek, Maguire, & Meyer, 2004). Thus, action pictures may represent a highly abstracted form of
action concepts, relative to pictures of static objects
whose meaning is not informed by motion or causal
relations (but see Buxbaum & Saffran, 2002; Reilly
et al., 2014; Satori, Negri, Mariani, & Prioni, 2004). As
previously discussed in the Introduction, object pictures
are considered advantageous as a result of either dual
coding or transparent semantic feature access compared to object words. However, the lack of properties
fully represented in an action image could limit such
privileged access and potentially increase the processing demand. Future work will benefit from

Table 12. Summary of F statistics for the group ANOVA results
examining gaze measures in AOItarget for modality (picture,
word), category (physical, non-physical), and relation (similarity,
association) using subjects (F1) and items (F2) analyses.
Fixation count
Subject
F1(1,34)

Item
F2(1,71)

Modality
225.9*** 501.97***
Relation
0.12
0.01
Category
0.61
0.005
Modality x relation
0.37
0.46
Modality x category
1.81
1.03
Relation x category
0.06
0.02
Modality x relation x
0.18
0.003
category
*Significant main effect where p  .05.
***Significant main effect where p  .001.

Fixation duration
(ms)
Subject
F1(1,34)

Item
F2(1,71)

31.79***
0.001
0.53
0.33
0.05
4.21*
0.04

249.64***
0.75
0.85
0.24
0.05
2.37
0.05

1162

J. HUNG ET AL.

comparisons of words with dynamic video representations of corresponding actions.
Another possible account of a lexical advantage in
action processing regards the inherent relational dimension of actions. For example, an action such as "kick"
might be executed by a wide range of agents (e.g. children, horses, robots) and effectors (e.g. feet, hooves,
paws, etc.). This relational property introduces intersubject variability (i.e. your prototype of "running" may
differ substantially from mine in terms canonical
agents/patients). Lexical access freely allows readers to
access their own idiosyncratic prototypes of actions. In
contrast, pictures force the viewer to process a specific
exemplar within a fixed event schema. These modality
differences exert powerful influences on semantic associative processing in our experiment here. Consider, for
example, an action pair drawn from the current study
("biting-fighting"). The picture stimuli depicted a girl
biting an apple vs. two women engaged in fisticuffs. Participants differed in their relatedness judgements when
"biting-fighting" appeared in picture vs. word format.
Participants strongly agreed (81%) that biting and fighting were related concepts when these appeared as a
word pair, but the same participants strongly disagreed
that these action concepts were associated when presented in picture form (10% agreement). In this case,
action word pairs could also benefit from the compatibility of context activated in the idiosyncratic semantic
network, whereas the thematic examples in the picture
pairs demonstrated incompatible context and thus
reduced the agreement of pair association. Previous
studies have demonstrated effects of contextual modulation such as distinctive N400 amplitude (Amoruso et al.,
2013; Amoruso & Urgesi, 2016) or differential cortical
excitability (Wurm & Schubotz, 2012) in response to compatible vs. incompatible semantic association. It is likely
that the prediction is also modality-specific for action
concepts such that greater contextual convergence is
induced by the word modality, whereas greater contextual variability may be found in the picture modality.
Yet, this hypothesis requires further justification and
testing.
Alternatively, the observed reversal of the picture
superiority effect may be either paradigm or stimulus
specific. We observed the effect within the context of a
semantic categorisation paradigm, whereas most work
to date has reported picture superiority effects within
the context of recall and recognition tasks (Nelson
et al., 1976; Stenberg, Radeborg, & Hedman, 1995).
Thus, it is not entirely clear whether the reversal of the
picture superiority effect is specific to actions or alternatively whether high-level semantic categorisation tasks
always elicit reverse picture superiority effects regardless

of stimulus class. Smith and Magee (1980) and Kiefer
(2001) both previously reported that pictures were
more rapidly categorised than words for static objects
(i.e. a classic picture superiority effect). Based upon this
prior work, it would appear that the task itself (i.e. semantic categorisation) does not universally elicit a reversal of
the picture superiority effect.

Concluding remarks
The present study extends the comparison between pictures and words to action concepts. Our findings suggest
that the picture superiority effect may not be as ubiquitous as previously assumed, especially within the domain
of actions. It is clear that much remains to be learned
about the phenomenon (e.g. the prospect of a perceptual vs. linguistic bias for object-action representational
dichotomy). Yet, there may also be great value in investigating the effect further in terms of both theory and
practical applications to language interventions.

Notes
1. The Hub-and-Spoke model of semantic cognition is
another unitary model premised upon amodal semantic
convergence within regions of anterior temporal cortex
(Patterson, Nestor, & Rogers, 2007). Patients with progressive atrophy of these regions have been reported
to experience semantic dementia, now more commonly
known as semantic variant primary progressive aphasia
(svPPA) (Gorno-Tempini et al., 2011; Hodges, Patterson,
Oxbury, & Funnell, 1992). Patients with svPPA typically
show homogeneous impairment for actions and
objects, and such impairments tend to transcend representational modality (Corbett, Jefferies, Ehsan, & Lambon
Ralph, 2009; Lambon Ralph, Sage, Jones, & Mayberry,
2010; Pulvermuller et al., 2010). The relative stability of
this impairment across categories and modalities lends
support to the presence of a single amodal semantic
store (Binney, Embleton, Jefferies, Parker, & Lambon
Ralph, 2010).
2. It should be noted that the neural substrates of action
concepts are represented not only in the motor regions
but also in the lateral temporal region (see Kemmerer,
2015 for discussion). The underlying mechanism of
these representations is still debatable. While a strong
embodied approach emphasizes the automaticity and
necessity of motor regions during action processing
(Hauk, Johnsrude, & Pulvermuller, 2004; Pulvermuller,
Harle, & Hummel, 2001), opponents argue that the
lateral temporal region is also engaged, and thus activation in the motor regions alone is not sufficient for the
conceptual representation of actions (Bedny et al.,
2008; Kemmerer, Rudrauf, Manzel, & Tranel, 2012;
Papeo et al., 2015; Peelen, Romagno, & Caramazza,
2012; Watson, Cardillo, Ianni, & Chatterjee, 2013). Convergent evidence would be essential to determine how and
to what extent actions are encoded in the brain.

LANGUAGE, COGNITION AND NEUROSCIENCE

Disclosure statement
No potential conflict of interest was reported by the authors.

Funding
This work was supported by US Public Health Service grant
DC013063.

References
Akinina, Y., Malyutina, S., Ivanova, M., Iskra, E., Mannova, E., &
Dragoy, O. (2015). Russian normative data for 375 action pictures and verbs. Behavior Research Methods, 47(3), 691-707.
doi:10.3758/s13428-014-0492-9
Altmann, G. T., & Kamide, Y. (1999). Incremental interpretation
at verbs: Restricting the domain of subsequent reference.
Cognition, 73(3), 247-264. doi:10.1016/S0010-0277(99)
00059-1
Amoruso, L., Gelormini, C., Aboitiz, F., Alvarez Gonzalez, M.,
Manes, F., Cardona, J. F., & Ibanez, A. (2013). N400 ERPs for
actions: Building meaning in context. Frontiers in Human
Neuroscience, 7, 57. doi:10.3389/fnhum.2013.00057
Amoruso, L., & Urgesi, C. (2016). Contextual modulation of
motor resonance during the observation of everyday
actions. Neuroimage, 134, 74-84. doi:10.1016/j.neuroimage.
2016.03.060
Bak, T. H., & Hodges, J. R. (2003). Kissing and dancing - a test to
distinguish the lexical and conceptual contributions to noun/
verb and action/object dissociation. Preliminary results in
patients with frontotemporal dementia. Journal of
Neurolinguistics, 16(2-3), 169-181. doi:10.1016/S0911-6044
(02)00011-8
Bedny, M., Caramazza, A., Grossman, E., Pascual-Leone, A., &
Saxe, R. (2008). Concepts are more than percepts: The case
of action verbs. Journal of Neuroscience, 28(44), 11347-
11353. doi:10.1523/JNEUROSCI.3039-08.2008
Bedny, M., Caramazza, A., Pascual-Leone, A., & Saxe, R. (2012).
Typical neural representations of action verbs develop
without vision. Cerebral Cortex, 22(2), 286-293. doi:10.1093/
cercor/bhr081
Berndt, R. S., Mitchum, C. C., Haendiges, A. N., & Sandson, J.
(1997). Verb retrieval in aphasia. 1. Characterizing single
word impairments. Brain and Language, 56(1), 68-106.
doi:10.1006/brln.1997.1727
Binney, R. J., Embleton, K. V., Jefferies, E., Parker, G. J. M., &
Lambon Ralph, M. A. (2010). The ventral and inferolateral
aspects of the anterior temporal Lobe are crucial in
semantic memory: Evidence from a novel direct comparison of distortion-corrected fMRI, rTMS, and semantic
dementia. Cerebral Cortex, 20(11), 2728-2738. doi:10.
1093/cercor/bhq019
Bird, H., Howard, D., & Franklin, S. (2003). Verbs and nouns: The
importance of being imageable. Journal of Neurolinguistics,
16(2-3), 113-149. doi:10.1016/S0911-6044(02)00016-7
Brysbaert, M., & New, B. (2009). Moving beyond Kuera and
Francis: A critical evaluation of current word frequency
norms and the introduction of a new and improved word frequency measure for American English. Behavior Research
Methods, 41(4), 977-990. doi:10.3758/BRM.41.4.977

1163

Buhrmester, M., Kwang, T., & Gosling, S. D. (2011). Amazon's
mechanical Turk: A new source of inexpensive, yet highquality, data? Perspectives on Psychological Science, 6(1), 3-
5. doi:10.1177/1745691610393980
Buxbaum, L. J., & Saffran, E. M. (2002). Knowledge of object
manipulation and object function: Dissociations in apraxic
and nonapraxic subjects. Brain and Language, 82(2), 179-
199. doi:10.1016/S0093-934X(02)00014-7
Cappa, S. F., Binetti, G., Pezzini, A., Padovani, A., Rozzini, L., &
Trabucchi, M. (1998). Object and action naming in
Alzheimer's disease and frontotemporal dementia.
Neurology, 50(2), 351-355.
Caramazza, A. (1996). Neuropsychology: Pictures, words and
the brain. Nature, 380, 485-486. doi:10.1038/383216a0
Chatterjee, A., Southwood, M. H., & Basilico, D. (1999). Verbs,
events and spatial representations. Neuropsychologia, 37(4),
395-402. doi:10.1016/S0028-3932(98)00108-0
Coltheart, M. (2006). Acquired dyslexias and the computational
modelling of reading. Cognitive Neuropsychology, 23(1), 96-
109. doi:10.1080/02643290500202649
Coltheart, M., Rastle, K., Perry, C., & Langdon, R. (2001). DRC: A
dual route cascaded model of visual word recognition and
reading aloud. Psychological Review, 108(1), 204-256.
doi:10.1037/0033-295X.108.1.204
Corbett, F., Jefferies, E., Ehsan, S., & Lambon Ralph, M. A. (2009).
Different impairments of semantic cognition in semantic
dementia and semantic aphasia: Evidence from the nonverbal domain. Brain, 132(9), 2593-2608.
Craik, F. I., & Tulving, E. (1975). Depth of processing and the
retention of words in episodic memory. Journal of
Experimental Psychology: General, 104(3), 268. doi:10.1037/
0096-3445.104.3.268
Crepaldi, D., Berlingeri, M., Paulesu, E., & Luzzatti, C. (2011). A
place for nouns and a place for verbs? A critical review of
neurocognitive data on grammatical-class effects. Brain
and Language, 116(1), 33-49. doi:10.1016/j.bandl.2010.09.
005
d'Honincthun, P., & Pillon, A. (2008). Verb comprehension and
naming in frontotemporal degeneration: The role of the
static depiction of actions. Cortex, 44(7), 834-847. doi:10.
1016/j.cortex.2007.04.003
Dell, G. S., & O'Seaghdha, P. G. (1992). Stages of lexical access in
language production. Cognition, 42(1-3), 287-314. doi:10.
1016/0010-0277(92)90046-K
Dove, G. (2009). Beyond perceptual symbols: A call for representational pluralism. Cognition, 110(3), 412-431. doi:10.1016/j.
cognition.2008.11.016
Druks, J., & Masterson, J. (2000). An object and action naming
battery. London: Psychology Press.
Fiez, J. A. (1997). Phonology, semantics, and the role of the left
inferior prefrontal cortex. Human Brain Mapping, 5(2), 79-
83. doi:10.1002/(SICI)1097-0193(1997)5:2<79::AID-HBM1>3.
0.CO;2-J
Gentner, D. (1982). Why nouns are learned before verbs:
Linguistic relativity versus natural partitioning. In S. Kuozaj
(Ed.), Language development: Vol. 2. Language, thought, and
culture (pp. 301-334). Hillsdale, NJ: Lawrence Erlbaum.
Glaser, W. R. (1992). Picture naming. Cognition, 42(1-3), 61-105.
Glaser, W. R., & Glaser, M. O. (1989). Context effects in strooplike word and picture processing. Journal of Experimental
Psychology: General, 118(1), 13-42. doi:10.1037/0096-3445.
118.1.13

1164

J. HUNG ET AL.

Gorno-Tempini, M. L., Hillis, A. E., Weintraub, S., Kertesz, A.,
Mendez, M., Cappa, S. F., ... Grossman, M. (2011).
Classification of primary progressive aphasia and its variants.
Neurology, 76(11), 1006-1014. doi:10.1212/WNL.0b013e3182
1103e6
Hantsch, A., Jescheniak, J., & Madebach, A. (2012). Naming and
categorizing objects: Task differences modulate the polarity
of semantic effects in the picture-word interference paradigm. Memory & Cognition, 40(5), 760-768. doi:10.3758/
s13421-012-0184-6
Hauk, O., Johnsrude, I., & Pulvermuller, F. (2004). Somatotopic
representation of action words in human motor and premotor cortex. Neuron, 41(2), 301-307. doi:10.1016/S0896-6273
(03)00838-9
Hillis, A. E., Sangjin, O., & Ken, L. (2004). Deterioration of naming
nouns versus verbs in primary progressive Aphasia. Annals of
Neurology, 55, 268-275. doi:10.1002/ana.10812
Hillis, A. E., Tuffiash, E., & Caramazza, A. (2002). Modality-specific
deterioration in naming verbs in nonfluent primary progressive aphasia. Journal of Cognitive Neuroscience, 14(7), 1099-
1108. doi:10.1162/089892902320474544
Hodges, J. R., Patterson, K., Oxbury, S., & Funnell, E. (1992).
Semantic dementia; progressive fluent Aphasia with temporal lobe Atrophy. Brain, 115, 1783-1806. doi:10.1093/
brain/115.6.1783
Holmqvist, K., Nystrom, M., Andersson, R., Dewhurst, R.,
Jarodzka, H., & van de Weijer, J. (2011). Eye tracking: A comprehensive guide to methods and measures. Oxford: Oxford
University Press.
Huttenlocher, J., & Lui, F. (1979). The semantic organization of
some simple nouns and verbs. Journal of Verbal Learning
and Verbal Behavior, 18(2), 141-162.
Juhasz, B. J. (2005). Age-of-acquisition effects in word and
picture identification. Psychological Bulletin, 131(5), 684-
712. doi:10.1037/0033-2909.131.5.684
Kable, J. W., Lease-Spellmeyer, J., & Chatterjee, A. (2002). Neural
substrates of action event knowledge. Journal of Cognitive
Neuroscience, 14(5), 795-805. doi:10.1162/08989290260138
681
Kemmerer, D. (2015). Are the motor features of verb meanings
represented in the precentral motor cortices? Yes, but within
the context of a flexible, multilevel architecture for conceptual knowledge. Psychonomic Bulletin & Review, 22(4), 1068-
1075. doi:10.3758/s13423-014-0784-1
Kemmerer, D., Rudrauf, D., Manzel, K., & Tranel, D. (2012).
Behavioral patterns and lesion sites associated with impaired
processing of lexical and conceptual knowledge of actions.
Cortex, 48(7), 826-848. doi:10.1016/j.cortex.2010.11.001
Kiefer, M. (2001). Perceptual and semantic sources of categoryspecific effects: Event-related potentials during picture and
word categorization. Memory & Cognition, 29(1), 100-116.
doi:10.3758/BF03195745
Klein, L. A., & Buchanan, J. A. (2009). Psychometric properties of
the pyramids and palm trees test. Journal of Clinical and
Experimental Neuropsychology, 31(7), 803-808. doi:10.1080/
13803390802508926
Kukona, A., Fang, S. Y., Aicher, K. A., Chen, H., & Magnuson, J. S.
(2011). The time course of anticipatory constraint integration. Cognition, 119(1), 23-42. doi:10.1016/j.cognition.2010.
12.002
Lambon Ralph, M. A., Sage, K., Jones, R. W., & Mayberry, E. J.
(2010). Coherent concepts are computed in the anterior

temporal lobes. Proceedings of the National Academy of
Sciences, 107(6), 2717-2722. doi:10.1073/pnas.0907307107
Mahon, B. Z., & Caramazza, A. (2009). Concepts and categories: A cognitive neuropsychological perspective.
Annual Review of Psychology, 60, 27-51. doi:10.1146/
annurev.psych.60
Matzig, S., Druks, J., Masterson, J., & Vigliocco, G. (2009). Noun
and verb differences in picture naming: Past studies and
new evidence. Cortex, 45(6), 738-758. doi:10.1016/j.cortex.
2008.10.003
McDonough, C., Song, L., Hirsh-Pasek, K., Golinkoff, R. M., &
Lannon, R. (2011). An image is worth a thousand words:
Why nouns tend to dominate verbs in early word learning.
Developmental Science, 14(2), 181-189. doi:10.1111/j.14677687.2010.00968.x
McRae, K., Ferretti, T. R., & Amyote, L. (1997). Thematic roles as
verb-specific concepts. Language and Cognitive Processes, 12
(2/3), 137-176. doi:10.1080/016909697386835
Miller, G. A., & Fellbaum, C. (1991). Semantic networks of
English. Cognition, 41, 197-229.
Mirman, D., & Magnuson, J. S. (2009). Dynamics of activation of
semantically similar concepts during spoken word recognition. Memory & Cognition, 37(7), 1026-1039. doi:10.3758/
MC.37.7.1026
Monaghan, P., Christiansen, M. H., & Chater, N. (2007). The phonological-distributional coherence hypothesis: Cross-linguistic evidence in language acquisition. Cognitive Psychology, 55
(4), 259-305. doi:10.1016/j.cogpsych.2006.12.001
Nasreddine, Z. S., Phillips, N. A., Bedirian, V., Charbonneau, S.,
Whitehead, V., Collin, I., ... Chertkow, H. (2005). The
Montreal cognitive assessment, MoCA: A brief screening
tool for mild cognitive impairment. Journal of the American
Geriatrics Society, 53(4), 695-699. doi:10.1111/j.1532-5415.
2005.53221.x
Nelson, D. L., Reed, V. S., & McEvoy, C. L. (1977). Learning to
order pictures and words: A model of sensory and semantic
encoding. Journal of Experimental Psychology: Human
Learning and Memory, 3(5), 485-497.
Nelson, D. L., Reed, V. S., & Walling, J. R. (1976). Pictorial superiority effect. Journal of Experimental Psychology: Human
Learning and Memory, 2(5), 523-528.
Odekar, A., Hallowell, B., Kruse, H., Moates, D., & Lee, C.-Y. (2009).
Validity of eye movement methods and indices for capturing
semantic (associative) priming effects. Journal of Speech,
Language, and Hearing Research, 52(1), 31-48. doi:10.1044/
1092-4388(2008/07-0100)
den Ouden, D. B., Fix, S., Parrish, T. B., & Thompson, C. K. (2009).
Argument structure effects in action verb naming in static
and dynamic conditions. Journal of Neurolinguistics, 22(2),
196-215. doi:10.1016/j.jneuroling.2008.10.004
Paivio, A. (1991). Dual coding theory: Retrospect and current
status. Canadian Journal of Psychology, 45(3), 255-287.
Paivio, A. (2007). Mind and its evolution: A dual coding theoretical
approach. New York: Psychology Press.
Papeo, L., & Hochmann, J. R. (2012). A cross-talk between braindamage patients and infants on action and language.
Neuropsychologia,
50(7),
1222-1234.
doi:10.1016/j.
neuropsychologia.2012.03.025
Papeo, L., Lingnau, A., Agosta, S., Pascual-Leone, A., Battelli, L., &
Caramazza, A. (2015). The origin of word-related motor activity. Cerebral Cortex, 25(6), 1668-1675. doi:10.1093/cercor/
bht423

LANGUAGE, COGNITION AND NEUROSCIENCE

Papeo, L., Negri, G. A., Zadini, A., & Ida Rumiati, R. (2010). Action
performance and action-word understanding: Evidence of
double dissociations in left-damaged patients. Cognitive
Neuropsychology, 27(5), 428-461. doi:10.1080/02643294.
2011.570326
Patterson, K., Nestor, P. J., & Rogers, T. T. (2007). Where do you
know what you know? The representation of semantic
knowledge in the human brain. Nature Reviews
Neuroscience, 8(12), 976-987. doi:10.1038/nrn2277
Peelen, M. V., Romagno, D., & Caramazza, A. (2012).
Independent representations of verbs and actions in left
lateral temporal cortex. Journal of Cognitive Neuroscience,
24(10), 2096-2107. doi:10.1162/jocn_a_00257
Pellegrino, J., Rosinski, R., Chiesi, H., & Siegel, A. (1977). Pictureword differences in decision latency: An analysis of single
and dual memory models. Memory & Cognition, 5(4), 383-
396. doi:10.3758/BF03197377
Piantadosi, S. T., Tily, H., & Gibson, E. (2011). Word lengths are
optimized for efficient communication. Proceedings of the
National Academy of Sciences, 108(9), 3526-3529. doi:10.
1073/pnas.1012551108
Potter, M. C., & Faulconer, B. A. (1975). Time to understand pictures and words. Nature, 253, 437-438.
Pruden, S. M., Hirsh-Pasek, K., Maguire, M. J., & Meyer, M. A.
(2004). Foundations of verb learning: Infants from categories
of path and manner in motion events. In A. Brugos,
L. Micciulla, & C. E. Smith (Eds.), Proceedings of the 28th
Annual Boston University conference on language development (pp. 461-472). Somerville, MA: Cascadilla Press.
Pulvermuller, F., Cooper-Pye, E., Dine, C., Hauk, O., Nestor, P. J., &
Patterson, K. (2010). The word processing deficit in semantic
dementia: All categories are equal, but some categories are
more equal than others. Journal of Cognitive Neuroscience,
22(9), 2027-2041. doi:10.1162/jocn.2009.21339
Pulvermuller, F., Harle, M., & Hummel, F. (2001). Walking or
talking? Behavioral and neurophysiological correlates of
action verb processing. Brain and Language, 78(2), 143-
168. doi:10.1006/brln.2000.2390
Ramey, C. H., Chrysikou, E. G., & Reilly, J. (2012). Snapshots of
children's changing biases during language development:
Differential weighting of perceptual and linguistic factors
predicts noun age of acquisition. Journal of Cognition and
Development, 14(4), 573-592. doi:10.1080/15248372.2012.
689386
Rapcsak, S. Z., Ochipa, C., Anderson, K. C., & Poizner, H. (1995).
Progressive ideomotor apraxia: Evidence for a selective
impairment of the action production system. Brain and
Cognition, 27(2), 213-236. doi:10.1006/brcg.1995.1018
Reilly, J., Harnish, S., Garcia, A., Hung, J., Rodriguez, A. D., &
Crosson, B. (2014). Lesion symptom mapping of manipulable
object naming in nonfluent aphasia: Can a brain be both
embodied and disembodied? Cognitive Neuropsychology,
31(4), 287-312. doi:10.1080/02643294.2014.914022
Reilly, J., Hung, J., & Westbury, C. (2016). Non-Arbitrariness in
mapping word form to meaning: Cross-linguistic formal
markers of word concreteness. Cognitive Science. doi:10.
1111/cogs.12361
Reilly, J., Martin, N., & Grossman, M. (2005). Verbal learning in
semantic dementia: Is repetition priming a useful strategy?
Aphasiology, 19, 329-339. doi:10.1080/02687030444000787
Reilly, J., Peelle, J. E., Garcia, A., & Crutch, S. J. (2016). Linking
somatic and symbolic representation in semantic memory:

1165

The dynamic multilevel reactivation framework. Psychonomic
Bulletin & Review. doi:10.3758/s13423-015-0824-5
Reilly, J., Rodriguez, A. D., Peelle, J. E., & Grossman, M. (2011).
Frontal lobe damage impairs process and content in semantic memory: Evidence from category-specific effects in progressive non-fluent aphasia. Cortex, 47(6), 645-658. doi:10.
1016/j.cortex.2010.05.005
Reilly, J., Westbury, C., Kean, J., & Peelle, J. E. (2012). Arbitrary
symbolism in natural language revisited: When word forms
carry meaning. PLoS One, 7(8). doi:10.1371/journal.pone.
0042286
Roth, H. L., Nadeau, S. E., Hollingsworth, A. L., Cimino-Knight, A.
M., & Heilman, K. M. (2006). Naming concepts: Evidence of
two routes. Neurocase, 12(1), 61-70. doi:10.1080/
13554790500502892
Saffran, E. M., Coslett, H. B., & Keener, M. T. (2003). Differences in
word associations to pictures and words. Neuropsychologia,
41(11), 1541-1546. doi:10.1016/S0028-3932(03)00080-0
Satori, G., Negri, G., Mariani, I., & Prioni, S. (2004). Relevance of
semantic features and category specificity. Cortex, 40(1),
191-193.
Saussure, F. D. (1916). Cours de linguistique generale (1907).
New York: Philosophical Library.
Schwitter, V., Boyer, B., Meot, A., Bonin, P., & Laganaro, M. (2004).
French normative data and naming times for action pictures.
Behavior Research Methods, Instruments, & Computers, 36(3),
564-576.
Seifert, L. S. (1997). Activating representations in permanent
memory: Different benefits for pictures and words. Journal of
Experimental Psychology: Learning, Memory, and Cognition, 23
(5), 1106-1121. doi:10.1037/0278-7393.23.5.1106
Smith, M. C., & Magee, L. E. (1980). Tracing the time course of
picture-word processing.
Journal of Experimental
Psychology: General, 109(4), 373-392. doi:10.1037/00963445.109.4.373
Stenberg, G., Radeborg, K., & Hedman, L. (1995). The picture
superiority effect in a cross-modality recognition task.
Memory & Cognition, 23(4), 425-441. doi:10.3758/
BF03197244
Szekely, A., & Bates, E. (2000). Objective visual complexity as a
variable in studies of picture naming. Center for Research in
Language Newsletter, 12(2), 1-33.
Szekely, A., D'Amico, S., Devescovi, A., Federmeier, K., Herron, D.,
Iyer, G., ... Bates, E. (2005). Timed action and object naming.
Cortex, 41(1), 7-25.
Szekely, A., Jacobsen, T., D'Amico, S., Devescovi, A., Andonova,
E., Herron, D., ... Bates, E. (2004). A new on-line resource for
psycholinguistic studies. Journal of Memory and Language,
51(2), 247-250.
Theios, J., & Amrhein, P. C. (1989). Theoretical analysis of the cognitive processing of lexical and pictorial stimuli: Reading,
naming, and visual and conceptual comparisons. Psychological
Review, 96(1), 5-24. doi:10.1037/0033-295X.96.1.5
Valente, A., Pinet, S., Alario, F. X., & Laganaro, M. (2016). "When"
does picture naming take longer than word reading?
Frontiers in Psychology, 7, 31. doi:10.3389/fpsyg.2016.00031
Waddill, P. J., & McDaniel, M. A. (1992). Pictorial enhancement of
text memory: Limitations imposed by picture type and comprehension skill. Memory & Cognition, 20(5), 472-482. doi:10.
3758/BF03199580
Watson, C. E., Cardillo, E. R., Ianni, G. R., & Chatterjee, A. (2013).
Action concepts in the brain: An activation likelihood

1166

J. HUNG ET AL.

estimation meta-analysis. Journal of Cognitive Neuroscience,
25(8), 1191-1205. doi:10.1162/jocn_a_00401
Woollams, A. M., Lambon Ralph, M. A., Plaut, D. C., & Patterson,
K. (2007). SD-squared: On the association between semantic
dementia and surface dyslexia. Psychological Review, 114(2),
316-339. doi:10.1037/0033-295X.114.2.316
Wu, D. H., Morganti, A., & Chatterjee, A. (2008). Neural substrates
of processing path and manner information of a moving
event. Neuropsychologia, 46(2), 704-713. doi:10.1016/j.
neuropsychologia.2007.09.016
Wu, D. H., Waller, S., & Chatterjee, A. (2007). The functional neuroanatomy of thematic role and locative relational

knowledge. Journal of Cognitive Neuroscience, 19(9), 1542-
1555. doi:10.1162/jocn.2007.19.9.1542
Wurm, M. F., & Schubotz, R. I. (2012). Squeezing lemons in the
bathroom: Contextual information modulates action recognition. Neuroimage, 59(2), 1551-1559. doi:10.1016/j.
neuroimage.2011.08.038
Yee, E., Overton, E., & Thompson-Schill, S. L. (2009). Looking for
meaning: Eye movements are sensitive to overlapping
semantic features, not association. Psychonomic Bulletin &
Review, 16(5), 869-874. doi:10.3758/PBR.16.5.869
Zachary, R. (1986). Shipley institute of living scale. Revised
manual. Los Angeles: Western Psychological Services.

