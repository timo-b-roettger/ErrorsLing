Available online at www.sciencedirect.com

Journal of Memory and Language 58 (2008) 815-836

Journal of
Memory and
Language
www.elsevier.com/locate/jml

Precategorical acoustic storage and the perception of speech
Clive Frankish

*

Department of Experimental Psychology, University of Bristol, 12a Priory Road, Bristol BS8 1TU, UK
Received 21 November 2006; revision received 31 May 2007
Available online 7 August 2007

Abstract
Theoretical accounts of both speech perception and of short term memory must consider the extent to which perceptual representations of speech sounds might survive in relatively unprocessed form. This paper describes a novel version of the serial recall task that can be used to explore this area of shared interest. In immediate recall of digit
sequences, a recall advantage for the final item in spoken rather than written lists has been interpreted as evidence
for a precategorical acoustic store (PAS). Three experiments are reported in which participants recalled lists of digits
presented with synchronised speech and visual displays, with spoken items degraded by narrow band-pass filtering
or by signal processing techniques that eliminate formant cues. Although simultaneous provision of top-down cues
from the visual display creates an illusion of speech clarity, memory for terminal list items reflected the intelligibility
of the speech signal rather than listeners' perceptual experience. Recall of bimodally presented lists is apparently supported by information retrieved from an acoustic memory trace that is unmodified by top-down influences. This finding
supports the PAS hypothesis, but is inconsistent with predictions generated by interactive processing models of speech
perception.
 2007 Elsevier Inc. All rights reserved.
Keywords: Serial recall; Modality effect; Short term memory; Echoic memory; Precategorical acoustic storage; PAS; Speech perception; Degraded speech; Interactive processing; TRACE

1. Introduction
All models of speech perception incorporate some
form of auditory short term memory as an integral component. This is necessary because comprehension of
speech requires the integration of information from successive elements within the acoustic signal. One essential
requirement for this to be achieved within a hierarchically
organised network is that activation of lower level units
should persist long enough to create the patterns that
map onto higher order units at the next level; for example,
recognition of a word normally requires simultaneous
*

Fax: +44 117 928 8588.
E-mail address: c.frankish@bris.ac.uk

activation of all the units corresponding to its constituent
phonemes. This type of persistence can be regarded as a
form of memory, although at this level there is no requirement for a process that might be described as directed
retrieval. Patterns of continued activation will also function as an episodic record in situations where directed
retrieval is required; for example, in perception of continuous speech whenever later context causes the listener to
reappraise perceptual cues for a previously misrecognised
word. When this happens, attention is directed towards
whatever residual traces remain of the activation that
led to the original perceptual response.
These considerations highlight the need to maintain a
dialogue between research on speech perception and
research on short term memory. Within the memory

0749-596X/$ - see front matter  2007 Elsevier Inc. All rights reserved.
doi:10.1016/j.jml.2007.06.003

816

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

domain, it is particularly important that theoretical
accounts of short term memory for speech should take
account of evidence for different levels of representation
within a perceptual hierarchy of linguistic processing. To
some extent this is already the case. Models of spoken
word recognition generally agree that at the prelexical
level there is a clear distinction between acoustic and
phonetic representations, although there is some debate
about the existence of further subdivisions (e.g. between
``simple'' and ``integrative'' acoustic features; Samuel &
Kat, 1996). A corresponding distinction is also made in
theories of short term memory. The working memory
model (Baddeley & Hitch, 1974) includes a dedicated
phonological subsystem, in which the code used to represent verbal items broadly corresponds to the phonetic
level. There have also been a number of theoretical formulations that include a separate acoustic subsystem of
memory, described variously as echoic memory (Neisser,
1967), a sensory buffer store (Atkinson & Shiffrin, 1968),
or precategorical acoustic storage (PAS; Crowder &
Morton, 1969).
Within this area of shared interest, one important
theoretical objective is to determine the relationship
between levels within this hierarchy. Models of spoken
word recognition are clearly divided on this issue. On
the one hand there are models in which the transmission
of information through successive levels of representation is represented as a unidirectional flow within a feedforward network. Perceptual analysis begins with
encoding of acoustic features, which are then translated
into phonetic and then lexical representations (e.g., Cutler, Mehler, Norris, & Segui, 1987; Massaro & Oden,
1995; Norris, McQueen, & Cutler, 2000, 2003). In contrast, interactive activation models such as TRACE
(McClelland & Elman, 1986; see also Connine & Clifton,
1987; Samuel, 1997; Samuel & Pitt, 2003) propose that
communication between these levels is bidirectional.
Whenever a lexical unit becomes active, feedback connections boost activation of the units that represent its
constituent phonemes. As a consequence, ``The interactive view predicts that lexical information actually
reaches down and reshapes the mental representation
of the sound that is heard'' (McClelland, Mirman, &
Holt, 2006, p. 363). This is a strong claim, and the evidence on which it is based will be reviewed later. For
now it should be noted that any prediction about mental
representations is ipso facto a prediction about short
term memory.
One specific example of the common ground occupied by research on auditory short term memory and
on speech perception is the ABX procedure used to
investigate categorical perception of speech sounds. This
is essentially a short term memory paradigm in which
participants hear a sequence of three speech sounds,
and have to report which of the first two sounds matches
the third. Listeners are less able to discriminate between

pairs of stimuli that are perceived as belonging to the
same phonetic category than they are to discriminate
between pairs for which the same marginal acoustic contrast places them on either side of a phonetic boundary.
This is true for a range of consonantal contrasts based
on articulatory features such as voice onset time or place
of articulation, but not for steady-state vowels (Fry,
Abramson, Eimas, & Liberman, 1962) unless these are
very brief (Pisoni, 1971). These results can be modelled
by supposing that responses in the ABX task are based
on information retrieved from two short term memory
systems; a sensory store, and a short term memory system that stores phonetic outputs (Fujisaki & Kawashima, 1970; Pisoni, 1975). If A and B are perceived as
allophonic, all three items in the ABX sequence have
identical phonetic representations and discrimination
must be based solely on the sensory trace. However, if
A and B are phonetically distinct the correct response
is reliably signalled by the phonetic code. This means
that the pattern of responses that characterise categorical perception will be observed if two conditions are
met. The first is that there is a perceptual mechanism
that generates a phonetic representation of the acoustic
input, and the second is that representation of the preperceptual input in sensory memory is relatively poor.
This conjunction is apparently true for stop consonants
but not for extended vowels.
In more conventional investigations of short term
memory, an almost identical pattern of data has been
obtained from experiments using the serial recall procedure. Some theoretical accounts of these findings also
correspond, although alternative interpretations have
also been proposed. The starting point for this research
is the robust finding that when participants recall a
sequence of spoken digits the last one is almost always
correctly recalled, but if the same sequence is presented
visually, recall of the final item is relatively poor. Crowder and Morton (1969) argued that this auditory advantage arises because recall of the final spoken item is
supported by information from a relatively peripheral
auditory memory system, which they termed precategorical acoustic storage (PAS). This was initially seen as a
limited capacity acoustic buffer, in which each incoming
speech sound overwrites any existing representations of
earlier items. For spoken lists, recall is thus based on
the combination of information from two sources; a
modality-independent, phonologically coded short term
memory trace and an acoustic record in PAS. Pronounced auditory recency in serial recall indicates that
an acoustically coded representation of the final list item
is effectively represented in the sensory store.
The link with research on categorical perception is
that there appear to be differences in the extent to which
vowels and consonants are represented in PAS. When
memory lists are constructed from spoken CV syllables
differing only in terms of stop consonants (e.g. /ba/,

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

/da/, /ga/) serial position data show no sign of strong
auditory recency. But the effect is found for lists of syllables differing only in their vowel sounds (e.g. /ba/,
/bi/, /bu/) (Crowder, 1971: Cole, 1973, de Gelder &
Vroomen, 1994). There are also indications that recency
is more pronounced for long than for short vowels
(Crowder, 1973). In other words, there is a general tendency for auditory recency to be weaker for speech
sounds that are categorically perceived. Data from both
paradigms, immediate serial recall and categorical perception, thus lead to the conclusion that there is some
aspect of task performance which reflects the extent to
which information can be retrieved from an acoustic
memory store in which consonants are less well represented than vowels.
The three experiments described here were
designed primarily to provide further evidence for a
PAS interpretation of auditory recency, and to challenge some of the alternative theoretical accounts
proposed by memory researchers. The experimental
technique used in these experiments is one that has
direct implications for theories of speech perception,
particularly with regard to the extent to which topdown feedback might influence representation of
speech at lower levels of the perceptual hierarchy.
However, the validity of any such conclusions
depends critically on first establishing that auditory
recency does indeed reflect retrieval of information
from an acoustic store.
Criticisms of the PAS account of auditory recency
Much of the empirical evidence for PAS was based
on experiments with the stimulus suffix effect. This is
the elimination of the end-of-list advantage for spoken
over written presentation when a predictable and
therefore informationally redundant item (such as the
spoken instruction ``recall'') is added to the end of
each list (Crowder, 1967). The effect is modality-specific; memory for visually presented lists is virtually
unaffected by the addition of a suffix, and auditory
recency is unimpaired by a written suffix that must
be copied before the list is recalled (Morton & Holloway, 1970). On the other hand, the impact of the suffix on auditory recency is reduced if it is spoken in a
different voice from list items or comes from a different spatial location (Morton, Crowder, & Prussin,
1971), but not if it is distinguished only by postcategorical attributes such as semantic category or meaningfulness (Crowder & Raeburn, 1970; Morton et al.,
1971). All of these findings are consistent with the
hypothesis that PAS is a limited capacity buffer store
in which traces are overwritten by successive auditory
events. However, subsequent researchers have variously argued that the memory processes responsible
for auditory recency are not precategorical, are not

817

specifically acoustic, and do not involve a separate
memory store.
Doubts about the precategorical status of PAS were
raised by the results of experiments using ambiguous
suffixes that could be perceived either as a spoken syllable or as a musical or animal sound (Ayres, Jonides,
Reitman, Egan, & Howard, 1979; Neath, Surprenant,
& Crowder, 1993). This research showed that the magnitude of the suffix effect could be influenced by context,
when this was manipulated in ways that led participants
to perceive the suffix as either a speech or nonspeech
sound. The idea that PAS is an acoustic store was called
into question when strong recency and suffix effects were
found for recall of silent speech, i.e. when verbal stimuli
are either lip-read or silently mouthed (Campbell &
Dodd, 1980; Nairne & Walters, 1983). Recency effects
for spoken and for silently lip-read lists appear to have
a common basis. The cross-modal suffix effects that
Morton and Holloway (1970) failed to find for spoken
lists with graphic suffixes are clearly apparent when spoken lists are followed by lip-read suffixes and vice versa
(de Gelder & Vroomen, 1992; Greene & Crowder, 1984;
Spoehr & Corin, 1978).
Although these results initially appear to be inconsistent with the PAS hypothesis, they can be accommodated by refining the original specification of a
buffer store. There is evidence that auditory recency
is generally associated with stimuli that are perceived
as speech (Crowder, 1983; de Gelder & Vroomen,
1997; Frankish, 1996; Healy & Mcnamara, 1996; Surprenant, Pitt, & Crowder, 1993). The implicit categorisation of sounds as speech or nonspeech can be seen
as a consequence of peripheral acoustic processing
that engages auditory or phonetic modes of perception
(Liberman & Mattingly, 1985). In marginal cases, the
balance between the two kinds of perceptual interpretation can be shifted by contextual cueing or by training, as in the case of ambiguous sounds or sine wave
speech (Remez, Rubin, Pisoni, & Carrell, 1981). Parallel effects for lip-read and mouthed stimuli can be
explained in terms of the intimate links between
speech perception and production; speech perception
is at least influenced (McGurk & McDonald, 1976),
and according to the motor theory is mediated by
interpretation of articulatory gestures (Liberman &
Mattingly, 1985). If modality effects in serial recall
are specifically associated with speech processing, the
PAS hypothesis should perhaps be modified in a
way that retains the principle of buffer storage, but
with this system relocated within the perceptual mechanisms responsible for speech processing. This reformulation could leave PAS very much as it was
initially conceived. Speech is normally conveyed by
means of an acoustic signal, and in this case, we
would predict that PAS would function as if it were
a purely acoustic buffer store.

818

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

A third critique of the PAS account departs more
radically from this general approach and rejects the
proposition that auditory recency is due to retrieval
of information from a separate buffer store. Instead,
it is seen as a specific instance of a general rule that
the probability of retrieving items from memory is a
function of their distinctiveness or discriminability.
Glenberg and Swanson (1986) proposed that temporal
distinctiveness is the critical factor, and that time of
arrival is more precisely coded for auditory than for
visual stimuli. Improved resolution of temporal cues
in memory for auditory events means that the final
item in a spoken list is more clearly distinguished from
its immediate predecessors and is therefore more likely
to be accurately retrieved. Although this hypothesis
was presented in the context of long-term modality
effects in paired associate learning, the underlying principle has been advanced as a possible basis for recency
effects in immediate serial recall (e.g. Brown, Preece, &
Hulme, 2000; Henson, 1998). However, there is a danger of circularity in this argument unless we can produce an a priori definition of distinctiveness, and
attempts to generalise this concept have met with little
success. For example, it has not been possible to create
strong recency effects for visually presented lists by
adding distinctive visual features to terminal list items
(LeCompte, 1992; McDowd & Madigan, 1991)
although weak effects have been reported by Bornstein,
Neely, and LeCompte (1995). Within the auditory
domain, the temporal distinctiveness hypothesis cannot
explain the lack of strong recency effects for highly discriminable nonspeech auditory stimuli (Frankish, 1996;
Surprenant et al., 1993).
In the context of more general models of ordered
recall, Page and Norris (1998) and Burgess and Hitch
(1999) retain the original PAS concept, arguing that
modality effects can be explained by a buffer that has a
capacity of just one item. Other recent models of serial
recall tend either to accept the PAS account of auditory
recency or to present alternative accounts that are functionally similar, if not identical. Penney (1989) proposed
that both auditory and visual inputs are represented in a
modality-independent format (the P code), but for auditory information there is an additional acoustic
representation (A code) that can be overwritten by subsequent auditory events. Nairne (1988, 1990) similarly
distinguishes between modality-dependent and modality-independent features of the memory trace, with rapid
overwriting of modality-dependent features. Both
formulations provide the functional properties of an
auditory buffer store, although both assert that the
effective capacity is greater than one item. In addition,
Nairne (1988) argues that sensory features are not
passively registered; context effects arise because these
features are optionally encoded during perceptual
processing.

Thus far, the debate about auditory recency has
focused almost exclusively on recall accuracy, without
regard to the kind of the errors that are made. This
approach has not provided a definitive answer to the
question whether pronounced recency in serial recall is
restricted to auditory or speech stimuli, largely because
there is no absolute criterion for judging the magnitude
of these effects. Some researchers have based their conclusions on error frequencies, while others have used
normalised data to compensate for differences in overall
levels of recall (e.g. Greene & Crowder, 1984; Nairne &
Walters, 1983). In many cases it is questionable whether
observed recency effects are comparable with those
obtained with spoken word lists. Detailed analysis of
errors may provide a way out of this difficulty, because
it is possible to derive strong predictions from the PAS
hypothesis about the types of error that should occur
in the recall of terminal list items. The experiments
described here were designed to test these predictions.
The starting point for these experiments is the observation that the standard modality effect is obtained
when items are presented visually and participants read
each item either silently or out loud as it is displayed;
this was the procedure used by Conrad and Hull
(1968). A virtually identical set of data can be generated
by using visual presentation with or without a synchronised auditory input from an external source (Crowder,
1970, Experiment 1). In Crowder's study the accompanying speech was clearly intelligible and error rates for
terminal list items were very low. The experiments
reported here use a modified version of this procedure
in which the speech signal is degraded by procedures
such as severe band-pass filtering. Previous studies have
shown that relatively mild degradation of the speech signal does have an impact on recency (Surprenant, 1999;
Surprenant & Neath, 1996), but in these experiments
there were no accompanying visual cues. The effect of
reducing the quality of the memory trace is therefore
confounded with the consequences of increased perceptual difficulty, which imposes an additional processing
load. With a bimodal presentation the identity of list
items is never in doubt because each is simultaneously
displayed in visual form. However, it may be supposed
that as the intelligibility of the speech signal is progressively reduced, a point will be reached where information about the identity of the memory items is derived
solely from the visual input. At this point all theories
would predict that recall performance will resemble that
for purely visual presentation (disregarding any possible
noise effects). But before this point is reached, what
effect will degradation of the auditory input have on
recall of the terminal list item?
The predictions from PAS theory are fairly straightforward, although more recent formulations leave some
room for uncertainty. If PAS is a buffer store that holds
sensory information that directly contributes to recall,

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

then errors for the final list item should reflect the quality of that information. This quality can be assessed in
terms of the nature and frequency of perceptual errors
in identifying the degraded speech signal when presented
in isolation. Thus there should be a high correlation
between confusion matrices that record the frequencies
of specific substitution errors in memory and in perceptual identification.
The uncertainty arises when PAS is defined not as an
acoustic buffer, but as an early component in a speech
processing module. Perceptual illusions such as the phoneme restoration effect (Warren, 1970) indicate that listeners are often unable to distinguish between
information contained in the speech signal and that
derived from contextual cues. Of immediate relevance
here is the observation by Frost, Repp, and Katz
(1988) that people overestimate the intelligibility of a
degraded speech signal when the words are simultaneously presented in a visual display. These aspects of
perceptual experience might be interpreted in terms of
interactive processes in speech recognition that include
top-down activation of units representing acoustic or
phonetic features (McClelland et al., 2006). If PAS really
is a component in such a system, then it could preserve
an encoded representation that corresponds to perceptual experience rather than a copy of the acoustic signal,
since the acoustic representation of this signal would be
enhanced by feedback from the lexical level. This would
perhaps explain the differential effects of suffixes that are
contextually cued as speech or nonspeech sounds (e.g.
Ayres et al., 1979). If PAS does preserve a record of perceptual experience, then the impact of degrading the
acoustic signal in the bimodal presentation will be smaller than would be predicted from measures of perceptual
accuracy for the speech presented in isolation. On the
other hand, evidence from Frost et al. (1988) on the
effects of combining visual displays and degraded speech
suggests that this is unlikely. Using signal detection techniques, they demonstrated that estimates of enhanced
intelligibility were based on shifts in bias rather than
increased discriminability of the underlying perceptual
representations.

Experiment 1
Experiment 1 was designed to provide detailed information about the frequency and nature of errors in
recall of terminal list items when speech input is
degraded. The stimuli were digits and intelligibility of
the speech signal was reduced by severe low-pass filtering (250 Hz cutoff) . This eliminates the higher formant
frequencies that differentiate many phonemes, and
removes most of the energy in unvoiced fricatives. However, amplitude modulation of the voiced components of
the speech signal remains more or less intact. One conse-

819

quence of this is that within the set of filtered digits,
some are easier to identify than others; for example,
the bisyllabic word ``seven'' is particularly identifiable
from its distinctive amplitude envelope. The experiments
reported here take advantage of this variability. To
begin with, we can use a perceptual identification task
to determine the mean intelligibility of the stimulus set
and relate this to the difference in recall errors for conditions with normal or degraded speech. In addition, we
can also calculate error rates and confusion matrices
for the final position in lists that end in each of the nine
digits, and compare these with data from the perceptual
identification task. Positive correlations between the two
sets of data would indicate that recall was based on a
memory trace that preserves the sensory characteristics
of the speech signal.
One concern that might be raised about this technique is that although Frost et al. (1988) reported that
the synchronised presentation is very compelling, participants may not attend fully to both modalities. To establish whether monitoring strategies might influence the
outcome, participants carried out the memory task
under two types of instruction. One specified that attention and rehearsal should be actively focused on the first
six items in the sequence, with the last three passively
monitored, while the other directed participants to
attend equally to all nine items. The experiment thus
included four conditions in a 2 * 2 factorial design; filtered and unfiltered speech combined with the two monitoring strategies.
Method
Participants
Data were collected from a total of 20 volunteer participants, who were all undergraduate students. There
were 8 males and 12 females, aged between 20 and 28.
Materials
Each stimulus list was a permutation of the digits 1
through 9. Four sets of lists were constructed from the
rows of 9 * 9 latin squares, selected so that no row contained a sequence of more than two digits in ascending
or descending order. Two different squares were used
to make 18 lists for each set, so that each digit appeared
exactly twice in each serial position. A further set of
eight lists was generated for practice trials.
For the experimental session, the 72 experimental
lists were arranged as four blocks of trials. The sequence
of lists within each block was randomised within the
constraint that no digit should appear in the same serial
position on two successive trials. An additional sequence
of 90 digits was assembled for the intelligibility test. This
consisted of ten permutations of the digits 1 through 9,
with no runs of more than two digits in ascending or
descending order.

820

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

Speech tokens of the digits 1 through 9 were digitally
recorded by a male speaker with a fundamental frequency centered at around 110 Hz. For the filtered
speech condition, these stimuli were re-recorded after
replaying through a hardware filter (Kemo VBF4;
250 Hz low-pass cutoff 48 dB/octave), with the amplitude boosted so that the output level matched that of
the unfiltered signal. To ensure perceptually isochronous
presentation of concatenated digit lists, interitem times
were adjusted to take account of p-center values (Morton, Marcus, & Frankish, 1976). The p-center of a spoken word is defined as its psychological moment of
occurrence, and the values for the spoken digits used
for this experiment were measured using a procedure
devised by Morton et al. (1976). For all pairwise combinations of digits, two continuous sequences are generated consisting of item A or B repeated at a constant
rate (in this instance 1.3 s per item). These sounds are
mixed with an appropriate temporal offset to produce
an interleaved A B A B. . . sequence. Control software
then allows listeners to adjust the temporal alignment
of the two input signals until the timing of the alternating sequence is perceived to be perfectly regular, at
which point the A-B onset asynchrony is recorded.
After presentation of all pairs, a least squares procedure
is used to determine the p-center offset of each sound relative to the others. If the timing of concatenated speech
sounds were determined solely by SOA, the point of
adjustment should be the same for all item pairs. However, this is not the case; the psychological moment of
occurrence of a spoken word is influenced by a range
of acoustic characteristics, with onset envelope as a
major factor (Marcus, 1981, Scott, 1998). For the stimuli
used in the present experiments, p-centers were separately determined for the filtered and unfiltered digit
sets. The measurement procedure was completed by
two observers and the resulting values averaged. The
effect of p-centre adjustment was that SOAs for items
in concatenated lists varied by 63 ms.
Procedure
Participants were tested individually in a soundattenuated room. Presentation of digit lists for the memory trials was computer-controlled, to ensure synchronisation of visual and auditory channels. Digits were
displayed in the centre of the computer screen in Times
00
00
New Roman font, with maximum dimensions of 3 * 2 .
Each trial began with a 500 ms warning tone and simultaneous visual display of a cross. Digit lists began 1 s
after tone offset, with spoken items presented at a p-centered SOA of 650 ms. In the synchronised visual display
each digit appeared on the screen for 500 ms, followed
by a 150 ms blank display. List presentation was followed by a 12 s interval for written recall. Response
sheets were marked with nine spaces, and participants
were instructed to fill these spaces from left to right,

guessing when uncertain. Recall was monitored by the
experimenter during the session to ensure compliance
with these instructions.
The four conditions were blocked and partially counterbalanced within the session. Participants were randomly assigned to two equal groups; one group
recalled the first 36 lists under the equal attention
instructions, and the remaining lists under the ``first
six'' instructions, which stressed the importance of accurate monitoring and recall of items in these early list
positions. This order of instructions was reversed for
the second group. Each half of the session began with
a sequence of eight practice trials with filtered speech,
to ensure familiarity with the stimuli and task instructions. For each type of instruction, all 18 lists in the filtered speech condition were presented first, followed by
18 in the unfiltered speech condition.
The final procedure was a perceptual identification
task consisting of the test sequence of 90 low-pass filtered digits, presented at a rate of one every 2 s. Participants listened to this sequence, attempting to identify
each digit immediately after it was heard. Responses
were written, and the instructions were again to leave
no blanks, guessing when uncertain.
Results
Mean error rates in the perceptual identification task
for each of the nine digits are shown in Table 1. The
impact of filtering on intelligibility of the speech signal
varied considerably, but the pattern of differences in
error rates was relatively stable across participants, with
error rates showing an average pairwise correlation of
0.46 (range 0.27 to 0.99). The mean correlation of individual error rates with the group mean was 0.69 (range
0.30-0.94).
For the recall data, items were scored as correct if
they were recalled in the correct serial position. The
resulting serial position data are shown in Fig. 1, in

Table 1
% errors in perceptual identification, Experiments 1-3
Experiment 1
250 Hz

Experiment 2
200 Hz

Experiment 3

300 Hz

One
Two
Three
Four
Five
Six
Seven
Eight
Nine

20
31
28
51
19
1
2
0
8

26
46
18
74
53
0
0
17
21

13
24
2
14
11
0
0
10
10

7
32
84
4
0
0
0
0
65

Mean

18

28

9

21

C. Frankish / Journal of Memory and Language 58 (2008) 815-836
70

821

60
equal attention

first six

60

50

50

% errors

% errors

40
40
30

filtered
unfiltered

30

20
20
filtered
unfiltered

10

10

0

0
1

2

3

4

5

6

7

8

9

serial position

1

2

3

4

5

6

7

8

9

serial position

Fig. 1. Serial position data for two monitoring conditions, Experiment 1. Error bars in this and subsequent figures represent 95%
confidence intervals for comparisons between conditions at each serial position.

which the error bars represent 95% confidence intervals
for comparisons between conditions at each serial position (Masson & Loftus, 2003). A preliminary analysis of
variance was conducted to test for possible asymmetric
transfer effects within the counterbalanced design. The
data were analysed using a four factor, mixed ANOVA
with participant group, monitoring strategy, filtering
and serial position as factors. There were significant
main effects of strategy (F(1, 18) = 11.7, p < .01), filtering (F(1, 18) = 19.2, p < .01), and serial position
(F(8, 144) = 29.3, p < .01), but no significant overall difference between participant subgroups (F(1, 18) <1).
Three of the interaction terms were significant; strategy * serial position (F(8, 144) = 2.63, p < .05), filtering * serial position (F(8, 144) = 10.5, p < .01), and
strategy * filtering * serial position (F(1, 144) = 2.09,
p < .05). There were no significant interactions involving
participant subgroup. Differences between filtered and
unfiltered conditions at each serial position were evaluated using multiple t-tests with Bonferroni correction.
For the equal attention condition, error rates were significantly higher in the filtered speech condition at positions 7, 8 and 9 (t(19) = 5.9, 4.7, and 8.1, respectively;
adjusted p < .05, <.01, and <.01). For the ``first six'' condition, error rates were again significantly higher in the
filtered speech condition at positions 7, 8, and 9
(t(19) = 3.8, 3.0, and 4.9, respectively; adjusted p < .01,
<.05, and <.01).
Given the absence of any interactions between monitoring strategy and any of the variables of primary
interest, data were pooled across the two strategic conditions in order to provide a larger corpus of data for a
detailed analysis of errors by item. Stimuli for the memory task were designed so that each of the digits 1
through 9 appeared twice in each serial position within
the set of 18 lists for each condition. This makes it possible to analyse the error data by item; i.e. to calculate

the mean error rate for each of the digits at each position
within the list. Correlations between these values and
error rates for each digit in the perceptual identification
task can then be used to assess the extent to which intelligibility affects recall of items at different positions
within the list. Correlations between identification and
memory errors across all serial positions are shown in
Table 2 for all except the first position, for which there
were insufficient error data from the memory task. For
the filtered speech condition, there were significant correlations between error rates for identification and memory at serial positions 7, 8, and 9, with an increasing
slope of the regression function across these three positions. One striking feature of these data is that the frequency of recall errors for the last item was almost
entirely determined by the intelligibility of the spoken
digit, regardless of attentional strategy; scatter plots
for these data are shown in Fig. 2. For the unfiltered
condition, none of the correlations were significant. This
rules out the possibility that correlations between perceptual and short term memory errors might reflect general characteristics of phonological coding in short term
memory.
There might perhaps be some concern that these correlations are based on an analysis by items that involves
pooling data across participants. As a further check, the
correlation between error rates in perception and recall
was analysed separately for each participant at each
serial position. Although the sparseness of the data sets
means that these data are rather noisy, the overall pattern of results was the same. One-sample t-tests were
used to determine whether the mean correlations at each
serial position were significantly different from zero. For
the filtered speech condition, these values were significant only at serial positions 7 (mean = 0.23,
t(19) = 2.77, p = .01), 8 (mean = 0.31, t(19) = 4.06,
p < .01) and 9 (mean = 0.50, t(19) = 7.91, p < .01). None

822

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

Table 2
Serial position analysis of correlations between errors in memory and in perceptual identification, Experiment 1
Serial position

2
3
4
5
6
7
8
9
**

Filtered
r(9)

Slope

.02
.44
.47
.44
.37
.81**
.88**
.98**

0.00
0.13
0.21
0.18
0.13
0.45
0.66
0.89

Unfiltered

Intercept

N recall errors

12.6
12.7
21.6
24.8
21.6
36.8
36.9
4.9

91
108
183
201
172
322
350
149

r(9)

Slope

Intercept

.56
.30
.04
.66
.10
.29
.58
.19

0.16
0.10
0.01
0.18
0.05
0.09
0.16
0.05

9.5
11.9
22.9
22.8
21.1
34.0
30.8
5.8

N recall errors
89
98
166
187
158
256
242
36

p < .01.

60

% recall errors at sp9

50

40

30

20
equal attention (r = 0.98)

10

first six (r = 0.96)

0
0

20

40

60

% identification errors

Fig. 2. Experiment 1 scatter plots for recall errors at serial
position 9 as a function of perceptual identification errors,
shown separately for two monitoring strategies.

of the mean correlations for the unfiltered speech condition were significant.
To test the prediction that the pattern of item confusions in the memory task would resemble those made in
perceptual identification, 9 * 9 input/output matrices
were analysed for serial positions 7 through 9 in the filtered speech condition, excluding correct responses. The
correlations between these confusion matrices and the
corresponding matrix from the perceptual identification
task were: position 7, r(72) = .26, p < .05; position 8,
r(72) = .35, p < .01; position 9, r(72) = .78, p < .01.
The corresponding analysis for the unfiltered speech
condition produced nonsignificant correlations of
0.03, 0.06, and 0.05, respectively.
Two further serial position analyses by items illustrate the relationship between speech intelligibility and
memory performance. The first contrasts recall data in
the filtered speech condition for the three digits with
the lowest and highest intelligibility scores (two, three,
four and six, seven, eight, respectively). Mean error rates

at each serial position for these two groups of items are
shown in Fig. 3(a). Multiple t-tests with Bonferroni correction indicated that error rates were significantly
higher for digits with low intelligibility at positions 7,
8, and 9 (t(19) = 3.8, 4.4, and 7.0, respectively; corrected
p < .01 in all cases). The second analysis uses the results
of the regression analysis shown in Table 2 to predict
serial position data for digits presented as unmodified
speech. For filtered speech, the regression analysis shows
that in general, items that are more intelligible are more
likely to be correctly recalled. The intercept of the
regression function for each serial position represents
the expected frequency of memory errors when the error
rate in perceptual identification is zero; i.e. the case
where speech is perfectly intelligible. In other words,
we can extrapolate data derived solely from the filtered
speech condition to predict expected serial position functions for unfiltered speech. These values are plotted in
Fig. 3(b), together with the actual data for the unfiltered
speech condition.
Discussion
In Experiment 1 participants attempted to recall
sequences of digits that were simultaneously seen and
heard. Phenomenologically, these sequences have a
number of characteristic qualities. The auditory signal
is experienced as homogeneous; the spoken digits were
perceived as coming from a consistent source because
the same frequency modulation was applied throughout.
Yet when these spoken words are heard in isolation,
some are much more intelligible than others. Awareness
of these differences is reduced when the degraded acoustic signal is accompanied by a synchronised visual display that unambiguously identifies the spoken word.
As Frost et al. (1988) and others have found, these conditions produce an illusory enhancement in clarity of the
speech.
For lists accompanied by unmodified speech, recall of
terminal items displays the strong recency that is characteristic of purely auditory presentation. Low-pass filter-

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

a 70

823

b 40

60
low intelligibility (2,3,4)

% errors

% errors

30

high intelligibility (6,7,8)

50
40
30
20

20

predicted

10

actual

10

0

0
1

2

3

4

5

6

7

8

9

serial position

1

2

3

4

5

6

7

8

9

serial position

Fig. 3. (a) Serial position data in the filtered speech condition for digits with high and low intelligibility, Experiment 1, (b) Serial
position curves for the unfiltered speech condition in Experiment 1: actual data and prediction from data for the filtered speech
condition.

ing resulted in poorer recall, but only for items in the last
three list positions; there is no sign of the overall decline
in performance reported by Surprenant (1999), which
thus appears to be a consequence of the processing load
imposed by the difficulty of identifying list items from a
degraded auditory signal. In the filtered speech condition the size of the recency effect was almost entirely
determined by the intelligibility of the uncued acoustic
signal, regardless of listeners' perceptual experience.
For the final serial position, the differences in error rates
across the item set and the pattern of confusions in recall
are both highly correlated with the corresponding data
for the perceptual identification task. This result is consistent with readout from an acoustic record of the kind
originally envisaged by Crowder and Morton's (1969)
formulation of PAS. For the terminal list item at least,
there is no evidence that the quality of the acoustic
record can be modified by external cues, whatever the
attentional strategy. Serial position data for the first
six positions in the equal attention and ``first six'' strategy conditions indicate that participants did modify
their behaviour in response to instructions. However,
the main effect seems to have been that rehearsal of early
list items was disrupted in the equal attention condition,
resulting in an overall increase in error rates. This deterioration in performance extended to the last three serial
positions, disconfirming the hypothesis that instructions
to pay more attention to terminal items in the degraded
speech condition might improve recall at the end of the
list.
It is conceivable that correlations between error rates
in perception and in short term memory could arise
because of the similarity between acoustic representations of speech and the corresponding phonological
codes in short term memory. This similarity has been
demonstrated using techniques similar in principle to
those reported here. Conrad (1964) found a highly sig-

nificant correlation between confusion matrices for
immediate recall of visually presented letters (encoded
in short term memory as phonological representations)
and identification of letters spoken in noise. However,
this interpretation is not sustainable. In Experiment 1,
correlations between speech intelligibility and memory
performance in the filtered speech condition were significant over the last three serial positions, with progressive
increases in the slope of the regression function. If these
correlations reflected confusions in phonological codes,
we would expect to find a similar pattern in the data
for lists with unmodified speech. There is no trace of this
in the data shown in Table 2. It must be acknowledged
that there were very few memory errors at serial position
9 in the unfiltered speech condition, and this reduces the
likelihood of finding a significant correlation. However,
this is not the case for positions 7 and 8. In addition, the
high values for the slopes of the regression functions in
the filtered speech conditions reflects the large variances
in memory error rates associated with differences in
intelligibility; the corresponding values for the unfiltered
speech condition are very much lower.
A related objection is that what appear to be memory
errors for items at the end of the list might in fact be perceptual errors that occur during the initial presentation.
On hearing a degraded speech token, participants who
misperceive the word could then encode this incorrect
representation in phonological short term memory. Correct retrieval of this episodic trace would then be scored
as a memory error. However, there are several reasons
for rejecting this interpretation. Firstly, synchronisation
of a visual display with the spoken presentation makes it
unlikely that such misperceptions will occur; rather, it
has been noted that integration of the two sources produces an illusory clarity in the auditory channel. Misperceptions could only occur if participants did not attend
to the visual display, a possibility that the comparison

824

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

of monitoring strategies in Experiment 1 was specifically
designed to detect. Instructions to pay equal attention
throughout the presentation should reduce the likelihood that participants would disregard the visual display of items at the end of the list, but there was no
effect of monitoring strategy on the correlation between
perceptual and memory errors. Secondly, the misperception hypothesis requires that an incorrect phonological
representation created when the terminal list item is initially misidentified is then accurately reproduced in
recall. However, it is well established that phonological
representations provide the basis for serial recall of visually presented lists, and in this case memory for terminal
list items is particularly poor. This is precisely the observation that makes it necessary to find an alternative
explanation for the near-perfect recall of the final item
in a spoken list. We are left with the conclusion that systematic errors in recall of terminal items in the degraded
speech condition are caused by successful retrieval of an
episodic trace from a memory system that cannot be
accessed by phonological recoding processes, and which
preserves the perceptual characteristics of the original
speech input.
It is now apparent that scoring of recall protocols by
item is an effective diagnostic technique for assessing the
relationship between sensory characteristics of auditory
stimuli and subsequent memory performance. This scoring procedure also offers a solution to a perennial problem in the interpretation of serial position data; namely,
the fact that recall of successive items in a list is not independent. When straightforward comparisons are made
between auditory and visual presentation in serial recall,
there is generally an auditory advantage that extends
across the last two or three list positions. Opinions differ
as to the origins of this extended modality effect. One
view is that there is a single mechanism based on retroactive interference, whereby each incoming item partially overwrites previous auditory traces. For example,
Nairne (1988) suggested that each item is represented
in memory as a set of elementary features, and that each
incoming item overwrites only those features that are
shared with previous items. Because some features will
not be overwritten, some trace of earlier items survives
to make a partial contribution to recall. An alternative
suggestion is that there are two quite different mechanisms. The echoic trace is restricted to a single item,
and all other items are retrieved from a separate short
term memory store. However, crosstalk between the
two systems can influence retrieval of items from the latter. Most models of this type assume that within the
short term store, order information is coded in terms
of positional cues or graded levels of activation. Recall
of the sequence involves selection of successive items
on the basis of these positional cues; errors of selection
result in the transposition errors that are a typical characteristic of serial recall. As this process nears the end of

the sequence, the selection of items from the memory
trace is influenced by information available in the echoic
store, which inhibits premature selection of the terminal
item (Page & Norris, 1998). The effects of such a mechanism would be to reduce the likelihood of order errors
in the positions immediately preceding the end of the
list, particularly those than involve item transpositions.
The item scoring technique bears directly on this
issue. It allows the derivation of a serial position curve
for a particular digit from different lists that contain that
digit in position 1, 2, 3, and so on. Each list thus contributes a single data point, and lists are independent of each
other. This means that in the resulting serial position
data, there are no sequential dependencies. Fig. 3(a)
shows the results of an analysis that contrasted sets of
digits with very high intelligibility scores (6, 7, and 8)
with those that were particularly difficult to identify (2,
3, and 4). The resulting serial position curves resemble
those found for conventional auditory and visual presentation, and there is a significant ``auditory'' advantage that extends over the last three serial positions.
The mechanism proposed by Page and Norris (1998)
cannot account for this difference in recall of high and
low intelligibility items at preterminal positions. Their
suggestion was that in comparisons of auditory and
visual presentation, better recall of these items for auditory lists is a direct consequence of the availability of an
echoic trace of the final item. This not only ensures its
correct recall at the end of the list, but also prevents it
from being prematurely selected for output. In the filtered speech condition of the present experiment, high
and low intelligibility items in the penultimate position
were followed by a random selection of items; some with
high intelligibility (i.e. highly available echoic traces, and
high probability of correct recall at the end of the list)
and some with low intelligibility. Post hoc analysis of
these contexts revealed that for lists with a highly intelligible final item (6, 7, or 8; mean error rate at
SP9 = 8%), the mean error rate at SP8 was 58%. The
corresponding figure for lists that ended in less well
recalled items (2, 3, and 4; mean error rate at
SP9 = 39%) was 40%, a difference in the opposite direction from that predicted by the Page & Norris hypothesis. On the other hand, this difference is exactly what
would be predicted if the probability of recalling the
item at SP8 is determined by the characteristics of the
item itself, rather than the context in which it appears.
Because there were no repeated items in the memory
lists, use of a low intelligibility item at the end of the list
meant that the preceding item was slightly more likely to
be drawn from the high intelligibility set. A further calculation determined the error rates that would be
expected for the population of items that actually
appeared at SP8 in the two sets of lists included in the
post hoc analysis reported above. This estimate was
based on the average error rates for these items at SP8

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

in the complete set of filtered lists, regardless of context.
For lists ending in high and low intelligibility items, this
calculation yielded expected error rates of 55% and 40%,
close to the observed values of 58% and 40%.
It is difficult to assess the statistical reliability of these
post hoc calculations, but the general approach is one
that can provide the basis for designed experiments in
future research. For the present, these data clearly suggest that auditory recency effects in preterminal list positions are not a secondary consequence of having a highly
accessible representation of the final list item. The alternative is that these effects are due to some form of overwriting or retroactive interference within an acoustic
store. We might then conclude that the auditory advantage that extends over the last few serial positions represents information that is retrieved independently for
each item from an echoic trace.

Experiment 2
The purpose of Experiment 2 was to extend the generality of these findings by using two levels of low-pass
filtering of spoken digits, so that correlations between
perceptual identification and memory performance
could be based on 18 rather than 9 data points. A second
modification was that participants were not given any
specific instructions about rehearsal or monitoring strategies, other than that they should attend to both the
visual and auditory channels. Finally, the perceptual
identification task was split into two halves; one was
completed at the beginning of the experimental session
and one at the end. The intervening memory trials can
be regarded as training trials, since they involve repeated
pairings of the degraded speech tokens with a visual display that indicates their identity. Comparison of the two
sets of data will show the extent to which identification
accuracy improves as a result of this training, and averaged data will provide a better overall estimate of recognition accuracy during the sequence of memory trials.
Method
Participants
Data were collected from a total of 20 volunteer participants, who were all undergraduate students. There
were 6 males and 14 females, aged between 20 and 25.
Materials
The procedure for constructing stimulus lists was the
same as for Experiment 1. A total of 72 lists were created
for the experimental trials, 36 for each level of filtering.
A further set of eight lists was generated for practice trials. The sequence of lists within the experimental session
was arranged so that successive blocks of 18 trials consisted of lists from one complete latin square for each

825

condition. Within these blocks, the sequence of trials
was randomised within the constraints that no digit
should appear in the same serial position on two successive trials, and that there should be no runs of more than
three consecutive trials in the same filtering condition.
Participants completed intelligibility tests before and
after the memory trials. Each test included 6 examples of
each digit at each level of filtering, to give a total
sequence length of 108. These trials were ordered so that
each successive block of 18 items contained one example
of each digit in each condition, randomised within the
constraints that there should be no digit repetitions
and no runs of more than three trials at the same level
of filtering.
Speech tokens for the digits 1 through 9 were the
same as those used in Experiment 1. For the filtered
speech condition, these stimuli were re-recorded after
low-pass filtering at 200 and 300 Hz, with the amplitude
boosted so that the output level matched that of the
unfiltered signal.
Procedure
The procedure was the same as for Experiment 1,
with the following exceptions. At the beginning of the
experimental session, participants listened to a demonstration sequence of the filtered digits without the visual
display. This consisted of the digits counting up from 1
to 9 and then down from 9 to 1, first with the 300 Hz,
then with the 200 Hz stimuli. This was followed by the
first perceptual identification test, and then the memory
trials. The only instructions for the recall task were that
both auditory and visual channels should be monitored,
together with the recall instructions used in Experiment
1. The second perceptual identification test was completed after the memory trials.
Results
Data from the two test sessions were assessed by
means of a two factor repeated measures ANOVA. This
showed significant main effects of test session
(F(1, 19) = 17.0, p < .001, og2 = .467) and filter cutoff
(F(1, 19) = 138.0, p < .001, og2 = .879), but no interaction (F(1, 19) <1). The significant main effects confirmed
that identification was more accurate in the second test
session than in the first (mean error rates 15.1% and
22.5%), and was more accurate with 300 Hz than
200 Hz filtering (mean error rates 9.3% and 28.4%).
For the remaining analyses, mean error rates in the perceptual identification task were averaged over the two
test sessions; these values are shown in Table 1.
Serial position data are shown in Fig. 4(a). Differences in error rates for the two levels of filtering were
assessed at each serial position using multiple t tests with
Bonferroni correction; these were significant only at
positions 8 and 9 (t(19) = 3.69, adjusted p = .01;

826

C. Frankish / Journal of Memory and Language 58 (2008) 815-836
60

b 60

50

50

40

40

% errors

% errors

a

30

20

30

20
200 Hz lpf
300 Hz lpf

10

low intelligibility
high intelligibility

10

0

0
1

2

3

4

5

6

7

8

9

1

2

3

serial position

4

5

6

7

8

9

serial position

Fig. 4. Serial position data, Experiment 2: (a) for digits with two levels of filtering, and (b) for digits with high and low intelligibility.

70
200 Hz lpf
300 Hz lpf

% recall errors at sp9

60
50
40
30
20
10
0
0

20

40

60

80

% identification errors

Fig. 5. Scatter plot for recall errors at serial position 9 as a
function of perceptual identification errors, Experiment 2.
80
predicted visual

70

predicted auditory
actual visual

60

actual auditory

% errors

t(19) = 6.62, adjusted p < .01). As in Experiment 1,
serial position data were also scored by item. To assess
the effects of intelligibility, items were ranked on the
basis of error rates in the perceptual identification task,
and split at the median into sets of items with ``high'' or
``low'' intelligibility. This classification almost coincided
with the two levels of filtering, with the exceptions that
200 Hz filtered ``six'' and ``seven'' were included in the
high intelligibility set, and 300 Hz filtered ``one'' and
``four'' in the low intelligibility set. Fig. 4(b) shows serial
position analysis of the recall data for these sets of items.
Mean error rates for the two sets were significant at positions 7, 8, and 9 (t(19) = 5.69, 5.38, and 7.02, adjusted
p < .01 in all cases).
Correlations between error rates in perceptual identification and in memory were calculated at each serial
position, as in Experiment 1. These data followed the
same pattern as those shown in Table 2; the only significant correlations were at serial positions 7, 8, and 9
(r(18) = .67, .76, and .94; regression slope = .31, .40,
and .75, respectively). The scatter plot for the data for
position 9 is shown in Fig. 5.
As in Experiment 1, the regression equations for
these correlations can be used to predict the serial position curve for items with perfect intelligibility scores; i.e.
auditory presentation with undegraded speech. By
extrapolation, it is also possible to predict the serial
position curve for purely visual presentation, modelled
as the situation in which performance in the perceptual
identification task is at chance level (i.e. 89% errors).
These two predicted functions are plotted in Fig. 6,
together with actual data for ungrouped auditory and
visual 9 item digit lists (from Frankish, 1989; Experiment 1).
Finally, errors in the memory task were further classified as either item or order errors. If a digit was not
recalled in the correct serial position, it was classified
as an order error if it appeared elsewhere in the

50
40
30
20
10
0
1

2

3

4

5

6

7

8

9

serial position

Fig. 6. Serial position curves for visual and auditory digit lists
predicted from filtered speech data from Experiment 2, with
actual data from Frankish (1989, Experiment 1) for
comparison.

C. Frankish / Journal of Memory and Language 58 (2008) 815-836
30
35

order errors

item errors

25

30
25

20

low intelligibility

% errors

% errors

827

high intelligibility

20
15

15
10

10

0

low intelligibility

5

5
1

2

3

4

5

6

7

8

9

serial position

0

high intelligibility

1

2

3

4

5

6

7

8

9

serial position

Fig. 7. Item and order errors for digits with high and low intelligibility, Experiment 2.

response, and an item error if it did not. The results of
this analysis are shown in Fig. 7. There were significantly
more item errors for the low intelligibility items at serial
positions 4 and 5 (t(19) = 3.7 and 3.4, respectively;
adjusted p < .05 in both cases), and also at positions 6,
7, 8, and 9 (t(19) = 4.4, 5.5, 4.9, and 6.9, respectively;
adjusted p < .01 in all cases). There were no reliable differences in the numbers of order errors, although the difference at position 9 approached significance
(t(19) = 2.75, adjusted p = .12).
Discussion
These data reproduce the pattern of results from
Experiment 1, under conditions where participants were
not instructed to adopt any particular strategy in the
memory task. The overall effect of manipulating speech
quality on recall of the final three list items was replicated, and the associated correlations between identification and memory scores for individual digits confirm the
relationship between intelligibility and recall accuracy.
The correspondence between predicted and actual
serial position data in Fig. 6 is striking. It shows that
analysis of recall data for bimodal lists with degraded
speech can be used to predict with considerable accuracy
the level of recall for conventional visual and auditory
presentation. The source data for these predictions come
from a sequence of trials where all the lists are presented
under the same conditions (disregarding the two levels
of filtering; the predictions are similar, though noisier,
when derived separately for each half of the data set).
This rules out any suggestion that modality differences
in serial recall might include a strategic element. If different strategies are used to deal with auditory and visual
presentation, this has little impact on memory performance. It also suggests that participants deal with bimodal presentation in the same way; this is consistent with
informal comments that these displays are natural and
well integrated.

The final analysis shows that changes in speech quality have an impact on item rather than order errors.
Reducing the intelligibility of items at the end of the list
does not make it more likely that they will be recalled in
the wrong order, but makes it less likely that they will be
recalled at all. Or from the opposite perspective, the
effect of adding intelligible sound to the visual channel
is that additional items are recalled at later positions,
and these items are generally reproduced in the correct
serial positions. This is consistent with the idea of an
acoustic buffer that retains an image of the speech signal.
For such a system, the sequence of items is an inherent
property of the representation.
The main theoretical conclusion to be drawn from
these data is that the contributions to recall made by
modality-independent and specifically auditory memory
systems are additive. This is consistent with the original
conception of PAS as an echoic store that functions as
an independent source of information about items at
the end of the list. The close correspondence between
errors in perception and recall also implies that information in PAS is directly acquired from the speech signal,
although this does not mean that speech is represented
in PAS as a faithful copy of the acoustic waveform.
The speech signal is multi-layered and highly redundant,
providing many types of acoustic cue that can be utilised
in word recognition. The information required for successful recognition in turn depends on the extent to
which bottom-up processing is supported by contextual
cues. In the experiments reported here the perceptual
identification task required participants to recognise digits; i.e. to discriminate between a small set of familiar
words. The perceptual cues required for this task are
far more limited than those generally required for word
recognition. After severe low-pass filtering, the speech
signal contains a subset of the cues available in the original waveform; these remaining cues reliably identify
some items, but not others. The fact that the same pattern of errors appears both in perception and recall

828

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

implies only that the reduced set of cues that identify
individual digits are faithfully represented in PAS. All
we can conclude about the fidelity of the representation
of an intact speech waveform is that it includes sufficient
cues for reliable discrimination of the complete set of
digits.

Experiment 3
The final experiment was designed to examine one
aspect of the relationship between speech perception
and PAS in greater detail, and to illustrate how this
approach might be developed in future research. It
focuses on the distinction between spectral and nonspectral cues in the speech signal.
Historically, models of speech recognition have
stressed the importance of phoneme-level decoding
based on detailed analysis of the short term acoustic
spectrum (e.g. Klatt, 1979; Pisoni & Luce, 1987). Even
at this level there is considerable redundancy in the
speech signal. High- and low-pass filtering can be used
to divide the speech signal into non-overlapping spectral
components that are both highly intelligible when heard
separately. Accuracy of word recognition can also be
relatively unaffected when the waveform is spectrally
limited by narrow band-pass filtering (Stickney & Assmann, 2001; Warren, Riener, Bashford, & Brubaker,
1995). However, information carried by short term spectral cues is only one of several components used by listeners to decode speech. Recent theories have taken a
broader approach, taking account of the many ways in
which the patterning of speech is determined by vocal
tract dynamics (e.g. Greenberg, 2005). Speech is produced by a biological system that is subject to physical
constraints, and the acoustic signal contains fine detail
that reflects vocal tract behaviour. The resulting coherence in the acoustic signal means that analysis of its
structure at different levels reveals many covariant features that contribute to redundancy. Parallel channels
for transmission of information in speech mean that if
one level of structure is eliminated, alternative cues
can fill in many of the gaps.
One feature of particular interest is the amplitude
modulation of the speech waveform envelope, which
has a major component corresponding to the typical syllabic rate of 3-4 Hz (Greenberg & Arai, 2004). The
speech envelope conveys information not only about
prosody and syllabic structure, but also about individual
phonemes (Rosen, 1992). It provides critical support for
speech perception in patients with sensorineural hearing
loss, and those with cochlear implants (Kaiser, Kirk,
Lachs, & Pisoni, 2003; Nie, Barco, & Zeng, 2006). Listeners with normal hearing can accurately identify a substantial proportion of words and individual phonemes in
speech from which all short term spectral cues have been

removed, provided that the amplitude envelope is preserved (Shannon, Zeng, Kamath, Wygonski, & Ekelid,
1995; Van Tasell, Soli, Kirby, & Widin, 1987).
The main effect of the low-pass filtering used in
Experiments 1 and 2 is that it severely degrades short
term spectral cues in speech, while leaving the amplitude
envelope more or less intact. However, this type of
manipulation is not entirely `clean', and the intelligibility
of filtered speech may be enhanced by spectral cues from
frequencies outside the nominal filter passband (Warren
& Bashford, 1999). Experiment 3 was therefore designed
to assess the relationship between intelligibility and
auditory recency for speech from which all formantrelated spectral cues have been entirely eliminated, and
all voicing and amplitude envelope cues entirely preserved. This has particularly relevance to the suggestion
made by Frankish (1996) that the effectiveness of PAS in
supporting recall of speech rather than nonspeech
sounds might be evidence of a memory representation
that selectively preserves spectral profiles that define formant characteristics. The experiment followed the same
pattern as Experiment 2, with a single set of processed
digit tokens used in the identification and memory tasks.
Method
Participants
A total of 20 undergraduate students participated in
the experiment, as part of a course requirement. There
were 7 males and 13 females, aged between 20 and 34,
with a mean age of 23.
Materials
A new set of spoken digits was recorded by the same
speaker as for Experiments 1 and 2, together with two
additional sounds; an extended fricative ([s] as in ``sit'')
and an extended vowel ([ ], as in ``father''), from which
one pitch period was extracted. Speech tokens for the
experiment were created by time-domain processing of
these source waveforms, using custom software developed for the purpose. The aim was to create modified
speech signals that not only preserved the amplitude
envelope of the original waveform, but also retained
the distinction between periodic and aperiodic sounds.
This was achieved by first calculating the amplitude
envelope, and then populating that envelope either with
steady-state noise, or a steady-state vowel sound.
The signal processing software was based on the following algorithm. For each spoken digit, the waveform
envelope was first divided into segments with one of four
classifications: silence, unvoiced fricative, voiced fricative,
and all other voiced sounds (this includes vowels, semivowels, nasals etc.). For all voiced segments the waveform
was divided into pitch periods, with markers placed at the
zero crossings; the peak amplitude within each pitch period was taken as the envelope amplitude for that period.

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

829

Fig. 8. Waveform and spectral plots of the digits ``one'', ``four'', and ``seven''; unmodified (left panel), and edited speech (right panel).

For unvoiced segments the envelope amplitude was taken
as the peak amplitude within a running 2 ms window. The
waveform was then reconstructed, with the fricative sample modulated at the envelope amplitude replacing the
segments classified as noise, and the similarly modulated
[ ] sample replacing all marked pitch periods. For regions
marked as voiced fricatives (the [v] in five and seven), the
two signals were mixed at equal amplitude. Finally, for
the voiceless stops in two, six and eight the closures were
treated as silence, and the bursts plus following aspiration
were replaced with the fricative sample. Examples of the
resulting waveforms and their corresponding spectra are
shown in Fig. 8.
For the memory task, 54 lists were constructed from
six 9 * 9 latin squares, together with an additional eight
lists for practice trials. There was a single perceptual
identification task, consisting of a sequence of eleven
permutations of the digits 1-9.
Procedure
At the start of the experimental session participants
listened to a demonstration sequence of the modified
digits without the visual display. This consisted of two
repetitions of the digits counting up from 1 to 9 and then
down from 9 to 1. This was followed by eight practice
trials on the memory task, followed by 54 experimental
trials, and then the perceptual identification task. In all
other respects the procedure was the same as for the previous experiments.
Results
The overall error rate in the perceptual identification
task was 21%, comparable with that for the previous

two experiments. However, the pattern of error rates
for individual digits (shown in Table 1) was quite different from that produced by low-pass filtering. The majority of digits were accurately identified, but even after the
training exposure provided by a long sequence of memory trials participants had difficulty in identifying ``two'',
``three'' and ``nine''.
There was a significant correlation between these
intelligibility scores and errors at serial position 9 in
the memory task (r(9) = .90). However, one outlier is
apparent in the scatter plot shown in Fig. 9. The digit
``four'' was rarely misidentified but incorrectly recalled
on 35% of trials in which it appeared as the final list
item. The reason for this anomaly was evident from
the confusion matrix for the identification task. The
modified waveforms for ``two'', ``three'' and ``four'' all
consisted of a noise burst followed by the vowel sound
[ ], which is sounds not unlike the [ ] vowel in ``four''.
This common profile produced a strong bias towards the
response ``four'' when any of these items appeared in the
identification task. This was particularly true for
``three'', which was identified as ``four'' on 84% of trials;
for ``two'' the rate was 33% (this lower rate reflects the
distinctiveness of the noise onset for the unvoiced stop
consonant [t]). In contrast, when ``four'' was presented
only 2% of the responses were ``two'' or ``three''.
Because of this response bias, ``four'' has been treated
as a special case for the remainder of this analysis. Without this item, the correlation between intelligibility
scores and errors was significant at positions 7 and 8
(r(8) = .73 and .75, p < .05 for both) as well as at position 9 (r(8) = .98, p < .01).
Four of the remaining digits were never misidentified
in the perceptual task, while the average error rate for

830

C. Frankish / Journal of Memory and Language 58 (2008) 815-836
60
60
50
"three"
40
40

% errors

% recall errors at sp9

50

"four"
30

30

20

20

low intelligibility (1,2,3,9)

10

10

high intelligibility (5,6,7,8)

0

0
0

20

40

60

80

100

1

2

3

4

5

6

7

8

9

serial position

% identification errors

Fig. 9. Experiment 3 scatter plots for recall errors at serial position 9 as a function of perceptual identification errors (left panel) and
serial position data for digits with high and low intelligibility (right panel).

the remainder (excluding ``four'') was 47%. Fig. 9 shows
recall data plotted for these high and low intelligibility
digit sets as a function of serial position. Differences in
error rates for the two sets were significant at serial positions 8 and 9 (t(19) = 3.5 and 6.4; adjusted p < .01 in
both cases), but not at position 7 (t(19) = 2.0), or at
any other serial position.
Regression analysis (excluding ``four'') was again
used to predict serial position functions for the cases
where digit intelligibility is either perfect or at chance
level, to model the effects of presentations consisting of
unmodified speech or a purely visual display. As for
Experiment 2, these curves closely match existing data
(Fig. 10).

80

predicted visual
predicted auditory

70

actual visual
actual auditory

% errors

60
50
40
30
20
10
0
1

2

3

4

5

6

7

8

9

serial position

Fig. 10. Serial position curves for visual and auditory digit lists
predicted from data from Experiment 3, with actual data from
Frankish (1989, Experiment 1) for comparison.

Recall errors were classified as either item and order
errors, and separately analysed for items with high and
low intelligibility scores (Fig. 11). There were significantly more item errors for low intelligibility items at
serial positions 7, 8, and 9 (t(19) = 3.4, 3.5, and 5.0,
respectively; adjusted p < .05, <.05, and <.01). The only
significant difference for order errors was at serial position 9 (t(19) = 4.4; adjusted p < .01).
Discussion
Data from Experiment 3 again confirmed that speech
intelligibility predicts recall of items at the end of a spoken list. This has now been shown both for the case
when spectral bandwidth of natural speech is restricted
by filtering, and also for a distortion that eliminates all
formant characteristics but preserves voicing and envelope modulation characteristics. It is particularly important to note that because the two methods of degrading
the speech signal produce quite different error patterns,
correlations between intelligibility scores and memory
performance cannot be explained in terms of confusability of postcategorical, phonological codes. There are no
significant cross-correlations between intelligibility data
in Experiment 1 and recall at serial position 9 in Experiment 3 (r(9) = .15), and vice versa (r(9) = .50).
A second implication of these findings is that they
make it less likely that PAS is critically dependent on
any single acoustic component of the speech signal.
One possible explanation of the failure to find strong
recency effects for nonspeech sounds such as pure tones
(Frankish, 1996; Surprenant et al., 1993) is that the relevant acoustic dimension (e.g. pitch) is not well represented in PAS. However, the artificial signals used in
Experiment 3 were capable of producing full-scale auditory recency effects. This strongly suggests that the PAS
exactly mirrors the characteristics of speech perception;

C. Frankish / Journal of Memory and Language 58 (2008) 815-836
30

45

item errors

831

order errors

40
25
35
low intelligibility

30

high intelligibility

% errors

% errors

20
15
10

25
20
15
10

low intelligibility

5

high intelligibility

5
0

0
1

2

3

4

5

6

7

8

serial position

9

1

2

3

4

5

6

7

8

9

serial position

Fig. 11. Item and order errors for digits with high and low intelligibility, Experiment 3.

if one component of the signal is eliminated, information
will be extracted from other perceptual features to compensate for the loss. This suggests that any speech sound
that can be accurately perceived will produce strong
auditory recency.
One possible problem with this account is that strong
recency effects are not found for stimuli consisting of
sine wave speech (Remez et al., 1981), even when participants are induced by training and/or contextual cues to
interpret them as spoken digits (Surprenant et al., 1993).
However, although the sine wave constituents of these
sounds directly represent formant characteristics of natural speech, this representation is based on an abstraction; the sounds themselves are not present in natural
speech. The perceptual processing of these sine wave
components is different from that of the formants on
which they are based. Although it is has been suggested
that the phonetic interpretation of both natural and sine
wave speech is accomplished by the same perceptual
module, the vocal quality remains highly unnatural.
Remez, Pardo, Piorkowski, and Rubin (2001) demonstrated that these ``unnatural'' auditory impressions
coexist with the phonetic interpretation of the signal,
and the consequences of having this divided percept on
retrieval from memory are unpredictable. In contrast,
although the sounds used in Experiment 3 were also
abstracted from natural speech, properties such as
amplitude modulation were directly reproduced in the
modified acoustic signal, using carrier waveforms taken
from natural speech. The signal sounds highly speech
like, and it is reasonable to suppose that the perceptual
mechanisms that process these attributes in normal
speech are fully engaged.
The boundary conditions for strong auditory recency
thus appear to be defined by the mechanisms responsible
for the perception of natural speech. It is not sufficient to
have sounds that can be labelled as spoken words, either
on the basis of artificially modelled speech parameters
(as in sine wave speech) or object naming (doorbells,

birdsong etc.). On the other hand, it may be sufficient
to have an acoustic signal that contains a subset of cues
derived from natural speech, provided that these are sufficient for word recognition. This embraces far more
than the spectral information that provides the basis
for tracking of formant trajectories, and is consistent
with recent theories of speech perception that stress
the importance of multi-level processing (e.g. Greenberg, 2005). Cues that are sufficient for word recognition
include not only acoustic characteristics such as amplitude envelope modulation, but also visual cues in lipreading and proprioceptive cues from silent articulation.
From this perspective, strong auditory recency is intimately associated with automatic processing of this multisensory array by a dedicated language system.

General discussion
We began with the observation that although PAS
was initially seen as a viable theoretical account of
modality effects in immediate serial recall, questions
had been raised about all three of its defining characteristics. The three experiments reported here support an
interpretation that is close to that originally proposed
by Crowder and Morton (1969), in the following
respects:
PAS is precategorical
Within the logogen model (Morton, 1964), the term
``precategorical'' refers to sensory processes that precede
word recognition. Central to that model was the concept
of logogens as lexical units that integrate evidence from
sensory systems with top-down activation from contextual influences. When activation of a logogen exceeds
its threshold value the lexical response that becomes
available is always the same, regardless of the relative
contributions of sensory evidence and contextual cueing.

832

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

The idea of a simple threshold device for word recognition is consistent with perceptual illusions such as the
phoneme restoration effect. This is one of many situations in which listeners make unreliable judgements
about the relative contributions made by bottom-up
and top-down processing to specific episodes of word
recognition.
The experiments with bimodal presentation reported
here provide an illustration of this phenomenon. Listeners heard degraded speech in which some words were
almost entirely unintelligible, but others could be clearly
heard. In all cases the identity of the word was simultaneously made available via a visual display, providing
top-down input to the speech recognition process.
Because the speech source is perceived as homogeneous
and the appropriate lexical response is always available,
listeners overestimate its overall intelligibility. If performance in the memory task were based on memory for
this lexical (i.e. postcategorical) output there would be
no differences between items of high and low intelligibility. The fact that we do find such differences for terminal
list items strongly indicates that this component of memory is based on memory representations established
before the combination of sensory and contextual information; i.e. precategorically, in terms of the logogen
model.
PAS is (primarily) acoustic
The quality of the acoustic signal determines the pattern of errors in listening, and this pattern is precisely
reproduced in memory errors. Replication of this result,
using acoustic distortions that produce different confusion matrices, rules out the possibility that this correspondence is due to a coincidental resemblance
between errors in listening and in phonological short
term memory. The evidence clearly points to the existence of a memory representation of terminal list items
that is a copy of the perceptual encoding of the acoustic
signal.
This characterisation of PAS seems paradoxical. On
the one hand, it appears to function as an echoic store,
maintaining representations that directly reflect the
acoustic characteristics of spoken words. On the other
hand, there is no evidence for echoic storage of environmental sounds, musical notes, or even attributes of
speech (such as pitch) that have no phonetic significance
(Frankish, 1996; Surprenant et al., 1993). Pronounced
recency in immediate serial recall seems to be limited
to stimuli that engage the perceptual mechanisms
involved in linguistic decoding of speech, whether or
not this is conveyed by an acoustic signal (although we
should note that recency for lip-read material is never
as pronounced as it can be for spoken lists). The evidence unequivocally points to the unique status of
speech in short term memory, and is consistent with the-

ories of speech perception that draw clear distinctions
between speech and nonspeech processing.
Liberman and Whalen (2000) contrast what they
term horizontal and vertical theories of speech perception (see also Liberman, 1997). Horizontal theories propose that speech is first processed by general auditory
mechanisms in the same way as other sounds, to create
a primary representation composed of non-linguistic
acoustic features. This representation is then interpreted
linguistically via a pattern recognition process that
translates these elements into a phonetic code. The vertical alternative, favoured by proponents of the motor
theory, is that the primitives underlying linguistic communication are not sounds, but rather the articulatory
gestures that generate these sounds. According to the
motor theory, phonetic interpretation of the speech signal is achieved by an encapsulated module that does not
permit direct access to the acoustic features that cue
phonetic categories.
Although it has been argued that this dichotomy has
been too sharply drawn (e.g. Bernstein, 2005), it provides a useful starting point for considering possible differences in the way that speech and nonspeech sounds
might be represented in relatively peripheral perceptual
systems. From the perspective of motor theory, PAS
might be characterised as persistent representation of
the perceptual encoding used as the input to a phonetic
module. In connectionist terms, this is an input vector; a
pattern of activation at the input layer of a neural network that will persist for some time if not overwritten.
The elements of this perceptual input are usually features in the acoustic signal, and when this is the case
PAS is functionally an acoustic system. But this representation can also include other kinds of sensory data
that provide information about articulatory gestures,
such as visual cues from lip-reading. Behavioural and
neurophysiological data provide support for the view
that speech is multimodal; speech perception is based
on primitives that are not tied to any particular sensory
modality (for a review see Rosenblum, 2005). This fits
very well with data that show not only that lipread
speech produces `auditory' recency effects, but also with
the cross-modal suffix effects observed when spoken lists
are followed by lipread suffixes and vice versa. The idea
that PAS is a perceptual trace within a speech recognition system would also explain why we do not find
strong recency effects in memory for sequences of pure
tones, or environmental sounds.
PAS is a memory store
The data reported here provide strong evidence for
PAS as a functionally distinct system that provides an
additional source of information in serial recall tasks.
In a bimodal presentation the contribution made by
the auditory channel to recall of terminal items is wholly

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

derived from information contained in the speech signal.
Accurate quantitative predictions of serial position data
for pure auditory and visual lists can be generated by
modelling this contribution as an additive component
of recall. In addition, the correspondence between confusion matrices for perception and recall indicate that
the information stored in this system is represented in
terms of auditory perceptual features rather than phonological codes. This clearly distinguishes it from postcategorical systems such as the phonological subsystem in
the working memory model (Baddeley & Hitch, 1974).
One theoretical alternative is Nairne's (1988, 1990)
feature model, which distinguishes between modalitydependent and modality-independent features of the
memory trace. In functional terms it is difficult to distinguish between this proposal and the idea of a distinct
buffer store. In the feature model, memory representations are based on two non-overlapping classes of elementary features that operate under different sets of
rules. It must also be supposed that there are different
sets of features for speech and nonspeech, and that there
is some commonality between audiovisual features for
speech. Whether there is a logical distinction between
this specification and the attribution of pre- and postcategorical representations to different subsytems of
memory is debatable. Two pieces of evidence that favour
the separate stores interpretation are first, that the effects
of adding an auditory trace can be modelled in additive
terms, and second, that information derived from the
perceptual trace seems to guarantee that additionally
recalled items are correctly sequenced. One necessary
property of a perceptual trace for an event that is
extended in time is that the temporal (or at least, sequential) structure should be preserved. This is not a property
of postcategorical memory systems such as the phonological subsystem of working memory (Baddeley &
Hitch, 1974).
Conclusion: PAS and the perception of speech
The experiments reported here set out to track in
detail the extent to which the perceptual qualities of
an auditory signal can be detected in recall protocols.
For the last few list items, and especially the terminal
item, the data strongly support the view that recall
from postcategorical short term memory is supplemented by retrieval of information from a perceptual
trace. The suggestion that this trace might represent
the input vector for a speech recognition module is
somewhat different from the acoustic buffer store postulated by Crowder and Morton (1969). Nevertheless
this conceptualisation retains all of the functional
properties attributed to PAS; it is precategorical, it
is normally (but not exclusively) based on acoustic
features of the speech signal, and it functions as a separate memory store.

833

These findings clearly have implications for the idea
that ``lexical information actually reaches down and
reshapes the mental representation of the sound that is
heard'' (McClelland et al., 2006, p. 363). The primary
evidence for this claim is that effects produced by speech
sounds that are fully specified by the acoustic input are
also produced by perceptually ambiguous stimuli that
are disambiguated by the lexical context in which they
appear. McClelland et al. identified three such effects;
selective adaptation, tuning of speech perception, and
compensation for auditory context. The first two involve
discrimination of phoneme pairs such as /s/ and , for
which it is possible to generate a series of intermediate
stimuli that can be used to determine the perceptual
boundary between the two phonemic categories. Adaptation occurs when repeated exposure to one or other
of the definitive stimuli produces a shift in the boundary
towards the adapter; i.e. repeated presentation of /s/
makes it more likely that intermediate stimuli will be
identified as . The critical finding is that this boundary
shift can be induced by an adapting stimulus that is itself
acoustically ambiguous, if its phonemic identity is cued
by lexical context. A neutral or ambiguous sound can
therefore shift the boundary towards /s/ following
repeated presentation stimuli such as bronchiti(?), but
towards
following presentation of words such as
aboli(?) (Samuel, 2001). Perceptual tuning is the opposite effect, where an ambiguous sound is assimilated into
a perceptual category as a result of being presented in a
variety of lexical contexts that always cause it to be
interpreted as a particular phoneme (Norris, McQueen,
& Cutler, 2003). Finally, compensation for auditory
context occurs when perceptual interpretation of an
ambiguous speech sound is influenced by the identity
of the preceding phoneme. This effect also occurs when
the preceding phoneme is itself acoustically underspecified, but is embedded in a lexical context that renders
it unambiguous (Elman & McClelland, 1988; Samuel
& Pitt, 2003). There is still some uncertainty about the
boundary conditions for these phenomena; for example,
it is not clear what factors determine whether exposure
to ambiguous stimuli will produce perceptual tuning
rather than adaptation shifts in phoneme boundaries.
However, for the present we can accept these data as evidence that should be accommodated within a theory of
speech perception.
What has been observed in these cases is that stimuli
for which identification of constituent phonemes relies
on top-down processing can have the same consequences as stimuli that are completely specified by
acoustic cues. However, this functional equivalence is
demonstrated only in terms of perceptual responses to
subsequent acoustic inputs; there is no direct evidence
that the episodic record of the original acoustic input
has itself been modified. The TRACE model (McClelland & Elman, 1986) permits this kind of modification,

834

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

and if it does occur this would account for the effects on
subsequent perceptual responses. On the other hand, a
feedforward model such as Merge (Norris et al., 2000)
can accommodate this set of findings without recourse
to lexical feedback. Boundary shifts and contextual
effects in speech perception are instead explained in
terms of dynamic retuning of perceptual mechanisms.
Reshaping of acoustic representations by top-down activation is thus a sufficient but not a necessary condition
for these target phenomena to occur. To distinguish
between these two theoretical positions, we must therefore look elsewhere.
One critical difference is that the interactive processes included within TRACE have the capacity to
rewrite the episodic record of acoustic events. This
implies that top-down influences should be detectable
not only in the original perceptual response to a
speech sound, but also in any subsequent response
that is based on the acoustic memory trace of that
event. The experiments reported here were designed
to test exactly this possibility. The perceptual consequences of bimodal presentation with degraded speech
resemble the classic phoneme restoration effect; listeners perceive an illusory clarity in the acoustic signal. If
there is feedback within the perceptual hierarchy then
this should be reflected in the pattern of activation at
the level of acoustic or phonetic units, and it is the
residue of this activation that constitutes the episodic
record. No support was found for this hypothesis. In
a task that involves retrieval of acoustic traces of
degraded speech, there was no evidence that these
traces were modified as a result of lexical information
provided at the time of the original presentation. In
other words, memory performance was determined
by the quality of the acoustic signal rather than perceptual experience. This outcome appears inconsistent
with the predictions that follow from TRACE.
This is a complex issue that will not be resolved by a
single set of experiments. However, the technique used
here does illustrate a novel and potentially fruitful
approach to the investigation of interactive processes
in speech perception. One critical question is whether
the effects of top-down activation can be found not only
in subsequent perceptual responses to speech sounds,
but also in responses that are directly based on the episodic record of the original acoustic event. The appropriate techniques for addressing this issue will
necessarily be based on investigations of auditory short
term memory.

Acknowledgments
I thank Dennis Norris and two anonymous reviewers
for helpful and constructive comments on an earlier
draft of this paper.

References
Atkinson, R. C., & Shiffrin, R. M. (1968). Human memory; A
proposed system and its control processes. In K. W. Spence
& J. T. Spence (Eds.). The apsychology of learning and
motivation (Vol. 2). Academic Press.
Ayres, T. J., Jonides, J., Reitman, J. S., Egan, J. C., & Howard,
D. A. (1979). Differing suffix effects for the same physical
suffix. Journal of Experimental Psychology-Human Learning
and Memory, 5, 315-321.
Baddeley, A. D., & Hitch, G. J. (1974). Working memory. In G.
A. Bowers (Ed.). The psychology of learning and motivation,
advances in research and theory (Vol. 8, pp. 47-90). Academic Press.
Bernstein, L. E. (2005). Phonetic processing by the speech
perceiving brain. In D. B. Pisoni & R. E. Remez (Eds.), The
handbook of speech perception (pp. 79-98). Blackwell.
Bornstein, B. H., Neely, C. B., & LeCompte, D. C. (1995).
Visual distinctiveness can enhance recency effects. Memory
& Cognition, 23, 273-278.
Brown, G. D. A., Preece, T., & Hulme, C. (2000). Oscillatorbased memory for serial order. Psychological Review, 107,
127-181.
Burgess, N., & Hitch, G. J. (1999). Memory for serial order: A
network model of the phonological loop and its timing.
Psychological Review, 106, 551-581.
Campbell, R., & Dodd, B. (1980). Hearing by eye. Quarterly
Journal of Experimental Psychology, 32, 85-89.
Cole, R. A. (1973). Different memory functions for consonants
and vowels. Cognitive Psychology, 4, 39-54.
Connine, C. M., & Clifton, C. (1987). Interactive use of lexical
information in speech perception. Journal of Experimental
Psychology: Human Perception and Performance, 13,
291-299.
Conrad, R. (1964). Acoustic confusions in immediate memory.
British Journal of Psychology, 55, 75-84.
Conrad, R., & Hull, A. J. (1968). Input modality and the serial
position curve in short-term memory. Psychonomic Science,
10, 135-136.
Crowder, R. G. (1967). Prefix effects in immediate memory.
Canadian Journal of Psychology, 21, 450-461.
Crowder, R. G. (1970). The role of ones own voice in immediate
memory. Cognitive Psychology, 1, 157-178.
Crowder, R. G. (1971). The sound of vowels and consonants in
immediate memory. Journal of Verbal Learning and Verbal
Behavior, 10, 587-596.
Crowder, R. G. (1973). Precategorical acoustic storage for
vowels of short and long duration. Perception & Psychophysics, 13, 502-506.
Crowder, R. G. (1983). The purity of auditory memory.
Philosophical Transactions Royal Society London, B302,
251-265.
Crowder, R. G., & Morton, J. (1969). Precategorical
acoustic storage (PAS). Perception & Psychophysics, 5,
365-373.
Crowder, R. G., & Raeburn, V. P. (1970). Stimulus suffix effect
with reversed speech. Journal of Verbal Learning and Verbal
Behavior, 9, 342-345.
Cutler, A., Mehler, J., Norris, D., & Segui, J. (1987). Phoneme
identification and the lexicon. Cognitive Psychology, 19,
141-177.

C. Frankish / Journal of Memory and Language 58 (2008) 815-836
de Gelder, B., & Vroomen, J. (1992). Abstract versus modalityspecific memory representations in processing auditory and
visual speech. Memory & Cognition, 20, 533-538.
de Gelder, B., & Vroomen, J. (1994). Memory for consonants
versus vowels in heard and lipread speech. Journal of
Memory and Language, 33, 737-756.
de Gelder, B., & Vroomen, J. (1997). Modality effects in
immediate recall of verbal and non-verbal information.
European Journal of Cognitive Psychology, 9, 97-110.
Elman, J. L., & McClelland, J. L. (1988). Cognitive penetration
of the mechanisms of perception--compensation for coarticulation of lexically restored phonemes. Journal of Memory and Language, 27, 143-165.
Frankish, C. (1989). Perceptual organization and precategorical
acoustic storage. Journal of Experimental Psychology:
Learning, Memory and Cognition, 15, 469-479.
Frankish, C. (1996). Auditory short-term memory and the
perception of speech. In S. E. Gathercole (Ed.), Models of
short-term memory (pp. 179-207). Psychology Press.
Frost, R., Repp, B. H., & Katz, L. (1988). Can speech
perception be influenced by simultaneous perception of
print? Journal of Memory and Language, 27, 741-755.
Fry, D. B., Abramson, A. S., Eimas, P. D., & Liberman, A. M.
(1962). The identification and discrimination of synthetic
vowels. Language and Speech, 5, 171-189.
Fujisaki, H., & Kawashima, T. (1970). Some experiments on
speech perception and a model for the perceptual mechanism. Annual Report of the Engineering Research Institute
Faculty of Engineering, University Tokyo, 29, 207.
Glenberg, A. M., & Swanson, N. G. (1986). A temporal
distinctiveness theory of recency and modality effects.
Journal of Experimental Psychology: Learning Memory and
Cognition, 12, 3-15.
Greenberg, S. (2005). A multi-tier framework for understanding
spoken language. In S. Greenberg & W. Ainsworth (Eds.),
Listening to speech: An auditory perspective (pp. 411-483).
Erlbaum.
Greenberg, S., & Arai, T. (2004). What are the essential cues for
understanding spoken language? IEICE Transactions on
Information and Systems, E87D, 1059-1070.
Greene, R. L., & Crowder, R. G. (1984). Modality and suffix
effects in the absence of auditory stimulation. Journal of
Verbal Learning and Verbal Behavior, 23, 371-382.
Healy, A. F., & Mcnamara, D. S. (1996). Verbal learning and
memory: does the modal model still work? Annual Review of
Psychology, 47, 143-172.
Henson, R. N. A. (1998). Memory for serial order: The startend model. Cognitive Psychology, 36, 73-137.
Kaiser, A. R., Kirk, K. I., Lachs, L., & Pisoni, D. B. (2003).
Talker and lexical effects on audiovisual word recognition
by adults with cochlear implants. Journal of Speech
Language and Hearing Research, 46, 390-404.
Klatt, D. H. (1979). Speech perception: A model of acousticphonetic analysis and lexical access. Journal of Phonetics, 7,
279-312.
LeCompte, D. C. (1992). In search of a strong visual recency
effect. Memory & Cognition, 20, 563-572.
Liberman, A. M. (1997). When theories of speech meet the real
world. Journal of Psycholinguistic Research, 27, 111-122.
Liberman, A. M., & Mattingly, I. G. (1985). The motor theory
of speech-perception revised. Cognition, 21, 1-36.

835

Liberman, A. M., & Whalen, D. H. (2000). On the relation of
speech to language. Trends in Cognitive Sciences, 4,
187-196.
Marcus, S. M. (1981). Acoustic determinants of perceptual
center (p-center) location. Perception & Psychophysics, 30,
247-256.
Massaro, D. W., & Oden, G. C. (1995). Independence of lexical
context and phonological information in speech perception.
Journal of Experimental Psychology: Learning, Memory, and
Cognition, 21, 1053-1064.
Masson, M. E. J., & Loftus, G. R. (2003). Using confidence
intervals for graphically based data interpretation. Canadian
Journal of Experimental Psychology, 57, 203-220.
McClelland, J. L., & Elman, J. L. (1986). The trace model of
speech perception. Cognitive Psychology, 18, 1-86.
McClelland, J. L., Mirman, D., & Holt, L. L. (2006). Are there
interactive processes in speech perception? Trends in Cognitive Sciences, 10, 363-369.
McDowd, J., & Madigan, S. (1991). Ineffectiveness of visual
distinctiveness in enhancing immediate recall. Memory &
Cognition, 19, 371-377.
McGurk, H., & McDonald, J. (1976). Hearing lips and seeing
voices. Nature, 264, 746-748.
Morton, J. (1964). A model for continuous language-behavior.
Language and Speech, 7, 40-70.
Morton, J., Crowder, R. G., & Prussin, H. A. (1971).
Experiments with the stimulus suffix effect. Journal of
Experimental Psychology, 91, 169-190.
Morton, J., & Holloway, C. M. (1970). Absence of a crossmodal `suffix effect' in short-term memory. Quarterly
Journal of Experimental Psychology, 22, 167-176.
Morton, J., Marcus, S., & Frankish, C. (1976). Perceptual
Centers (P-centers). Psychological Review, 83, 405-408.
Nairne, J. S. (1988). A framework for interpreting recency
effects in immediate serial recall. Memory & Cognition, 16,
343-352.
Nairne, J. S. (1990). A feature model of immediate memory.
Memory & Cognition, 18, 251-269.
Nairne, J. S., & Walters, V. L. (1983). Silent mouthing produces
modality- and suffix-like effects. Journal of Verbal Learning
and Verbal Behavior, 22, 475-483.
Neath, I., Surprenant, A. M., & Crowder, R. G. (1993). The
context-dependent stimulus suffix effect. Journal of Experimental Psychology: Learning Memory and Cognition, 19,
698-703.
Neisser, U. (1967). Cognitive Psychology. New York: AppletonCentury-Crofts.
Nie, K., Barco, A., & Zeng, F.-G. (2006). Spectral and temporal
cues in cochlear implant speech perception. Ear and
Hearing, 27, 208-217.
Norris, D., McQueen, J. M., & Cutler, A. (2000). Merging
information in speech recognition: Feedback is never
necessary. Behavioral and Brain Sciences, 23, 299-370.
Norris, D., McQueen, J. M., & Cutler, A. (2003). Perceptual
learning in speech. Cognitive Psychology, 47, 204-238.
Page, M. P. A., & Norris, D. (1998). The primacy model: A new
model of immediate serial recall. Psychological Review, 105,
761-781.
Penney, C. G. (1989). Modality effects and the structure of
short-term verbal memory. Memory & Cognition, 17,
398-422.

836

C. Frankish / Journal of Memory and Language 58 (2008) 815-836

Pisoni, D.G. (1971) On the nature of categorical perception of
speech sounds. Supplement to Status Report on Speech
Research, Haskins Laboratories, New York.
Pisoni, D. B. (1975). Auditory short-term-memory and vowel
perception. Memory & Cognition, 3, 7-18.
Pisoni, D. B., & Luce, P. A. (1987). Acoustic-phonetic
representations in word recognition. Cognition, 25, 21-52.
Remez, R. E., Pardo, J. S., Piorkowski, R. L., & Rubin, P. E.
(2001). On the bistability of sine wave analogues of speech.
Psychological Science, 12, 24-29.
Remez, R. E., Rubin, P. E., Pisoni, D. B., & Carrell, T. D.
(1981). Speech perception without traditional speech cues.
Science, 212, 947-950.
Rosen, S. (1992). Temporal information in speech--acoustic,
auditory and linguistic aspects. Philosophical Transactions of
the Royal Society of London Series B-Biological Sciences,
336, 367-373.
Rosenblum, L. D. (2005). The primacy of multimodal speech
perception. In D. Pisoni & R. Remez (Eds.), Handbook of
speech perception (pp. 51-78). Blackwell.
Samuel, A. G. (1997). Lexical activation produces potent
phonemic percepts. Cognitive Psychology, 32, 97-127.
Samuel, A. G. (2001). Knowing a word affects the fundamental
perception of the sounds within it. Psychological Science, 12,
348-351.
Samuel, A. G., & Kat, D. (1996). Early levels of analysis of
speech. Journal of Experimental Psychology; Human Perception and Performance, 22, 676-694.
Samuel, A. G., & Pitt, M. A. (2003). Lexical activation (and
other factors) can mediate compensation for coarticulation.
Journal of Memory and Language, 48, 416-434.
Scott, S. K. (1998). The point of P-centres. Psychological
Research, 61, 4-11.

Shannon, R. V., Zeng, F. G., Kamath, V., Wygonski, J., &
Ekelid, M. (1995). Speech recognition with primarily
temporal cues. Science, 270, 303-304.
Spoehr, K. T., & Corin, W. J. (1978). The stimulus suffix effect
as a memory coding phenomenon. Memory & Cognition, 6,
583-589.
Stickney, G. S., & Assmann, P. F. (2001). Acoustic and
linguistic factors in the perception of bandpass-filtered
speech. Journal of the Acoustical Society of America, 109,
1157-1165.
Surprenant, A. M. (1999). The effect of noise on memory for
spoken syllables. International Journal of Psychology, 34,
328-333.
Surprenant, A. M., & Neath, I. (1996). The relation between
discriminability and memory for vowels, consonants, and
silent-center vowels. Memory & Cognition, 24, 356-366.
Surprenant, A. M., Pitt, M. A., & Crowder, R. G. (1993).
Auditory recency in immediate memory. Quarterly Journal
of Experimental Psychology, 46A, 193-223.
Van Tasell, D. J., Soli, S. D., Kirby, V. M., & Widin, G. P.
(1987). Speech wave-form envelope cues for consonant
recognition. Journal of the Acoustical Society of America,
82, 1152-1161.
Warren, R. G. (1970). Perceptual restoration of missing speech
sounds. Science, 167, 392-393.
Warren, R. M., & Bashford, J. A. (1999). Intelligibility of 1/3octave speech: Greater contribution of frequencies outside
than inside the nominal passband. Journal of The Acoustical
Society of America, 106, L47-L52.
Warren, R. M., Riener, K. R., Bashford, J. A., Jr., & Brubaker,
B. S. (1995). Spectral redundancy: Intelligibility of sentences
heard through narrow spectral slits. Perception & Psychophysics, 57, 175-182.

