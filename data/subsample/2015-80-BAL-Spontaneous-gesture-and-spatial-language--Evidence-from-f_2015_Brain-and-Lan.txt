Brain & Language 150 (2015) 1-13

Contents lists available at ScienceDirect

Brain & Language
journal homepage: www.elsevier.com/locate/b&l

Spontaneous gesture and spatial language: Evidence from focal brain
injury
Tilbe Goksun a,, Matthew Lehet b,c,d, Katsiaryna Malykhina c, Anjan Chatterjee b,c
a

Department of Psychology, Koc University, Turkey
Department of Neurology, University of Pennsylvania School of Medicine, United States
c
Center for Cognitive Neuroscience, University of Pennsylvania, United States
d
Department of Psychology, Carnegie Mellon University, United States
b

a r t i c l e

i n f o

Article history:
Received 12 March 2015
Revised 27 July 2015
Accepted 30 July 2015
Available online 15 August 2015
Keywords:
Lesion studies
VLSM
Gesture
Spatial language
Motion events
Path-manner

a b s t r a c t
People often use spontaneous gestures when communicating spatial information. We investigated focal
brain-injured individuals to test the hypotheses that (1) naming motion event components of mannerpath (represented by verbs-prepositions in English) are impaired selectively, (2) gestures compensate
for impaired naming. Patients with left or right hemisphere damage (LHD or RHD) and elderly control
participants were asked to describe motion events (e.g., running across) depicted in brief videos.
Damage to the left posterior middle frontal gyrus, left inferior frontal gyrus, and left anterior superior
temporal gyrus (aSTG) produced impairments in naming paths of motion; lesions to the left caudate
and adjacent white matter produced impairments in naming manners of motion. While the frequency
of spontaneous gestures were low, lesions to the left aSTG significantly correlated with greater production of path gestures. These suggest that producing prepositions-verbs can be separately impaired and
gesture production compensates for naming impairments when damage involves left aSTG.
O 2015 Elsevier Inc. All rights reserved.

1. Introduction
How do we communicate spatial information using language?
What neural structures implement this type of information? We
use spatial language, such as prepositions and action verbs, to
describe spatial events in our environment and organize relational
thinking (Chatterjee, 2001, 2008). People also use hand gestures
spontaneously when they talk. Gestures, particularly iconic gestures are used commonly when individuals express spatial information such as giving directions or describing motion in space.
These spontaneous co-speech iconic gestures that accompany verbal spatial information (Alibali, 2005) are the focus of this study.
There is growing interest in understanding the neural underpinnings of spatial language (e.g., Amorapanth, Widick, & Chatterjee,
2009; Chatterjee, 2008; Damasio et al., 2001; Kemmerer, 2006)
and gesture comprehension (e.g., Dick, Goldin-Meadow, Hasson,
Skipper, & Small, 2009; Dick, Goldin-Meadow, Solodkin, & Small,
2012; Holle, Gunter, Rueschemeyer, Hennenlotter, & Iacoboni,
2008; Skipper, Goldin-Meadow, Nusbaum, & Small, 2009;
Willems & Hagoort, 2007; Willems, Ozyurek, & Hagoort, 2007).
 Corresponding author at: Department of Psychology, Koc University, Rumelifeneri Yolu, Sariyer, 34450 Istanbul, Turkey.
E-mail address: tgoksun@ku.edu.tr (T. Goksun).
http://dx.doi.org/10.1016/j.bandl.2015.07.012
0093-934X/O 2015 Elsevier Inc. All rights reserved.

However, little is known about the neural correlates of spontaneous gestures that naturally accompany spatial language production (but see recent papers by Marstaller & Burianova, 2015a,
2015b; Marstaller et al., 2015). In this study we test two main
hypotheses. First, if spatial representations and lexical-semantic
spatial information are organized similarly in the brain
(Chatterjee, 2008), patients with focal brain injury to left frontalparietal regions, known to process spatial information (e.g.,
Goksun, Lehet, Malykhina, & Chatterjee, 2013; Kemmerer, 2006;
Kemmerer & Tranel, 2003), would have difficulty verbally describing spatial events. Second, if spatial language and spatial gestures
rely on the same neural structures, then damage to areas needed
for spatial language would also impair gesturing spatial events.
That is, deficits of spatial knowledge would lead to deficits in both
verbally and gesturally expressing spatial information.
Alternatively, if spatial gestures compensate for verbal deficits
without being reliant on the same neural structures, deficits in spatial language would result in a greater use of gestures.
A dynamic spatial event consists of several components that are
encoded across world's languages (Talmy, 2000). The path and
manner of motion describe two of these components. Path refers
to a figure's trajectory relative to ground and manner refers to
how the action is performed. That is, the path of motion describes
an ``extrinsic dynamic relation" of the movement of a figure

2

T. Goksun et al. / Brain & Language 150 (2015) 1-13

relative to external landmarks and the manner of motion describes
an ``intrinsic dynamic relation" of the movement of figure parts relative to each other (Chatterjee, 2008). For example, in the sentence
``John is running into the room," running describes the manner and
into the room describes the path of the motion. In English, manner
of motion is expressed typically by the main verb of a sentence
whereas path of motion is expressed by a prepositional phrase.
In the following sections we first review the current understanding of the neural basis of spatial language, centering on
dynamic spatial events. Then we discuss the relation between
speech and gesture with respect to motion events before presenting the current study.
1.1. The neural correlates of spatial language: Motion events
Attention to path and manner of motion activates different neural networks (Wu, Morganti, & Chatterjee, 2008) as we demonstrated in a one-back matching task using a computer animated
starfish moving with different manners and paths. In some blocks,
participants attended to manner and in others to path. Within
regions sensitive to motion, dorsal areas (i.e., bilateral posterior
parietal and frontal areas) were preferentially activated in path
conditions and relatively ventral areas (i.e., bilateral posterior inferior/middle temporal cortex) were preferentially activated in manner conditions.
The neural parsing of attention to these perceptual components
of dynamic events parallels the linguistic parsing of path and manner represented by prepositions and verbs. Comprehending verbs
correlates with activation in the posterior middle temporal gyrus
(Kable, Kan, Wilson, Thompson-Schill, & Chatterjee, 2005; Kable,
Lease-Spellmeyer, & Chatterjee, 2002; Kemmerer et al., 2008)
whereas comprehending prepositions correlates with activation
in the left posterior inferior parietal and prefrontal cortices
(Amorapanth et al., 2009; Baciu et al., 1999; Noordzij, Neggers,
Ramsey, & Postma, 2008). Neuropsychological and other imaging
studies confirm the role of these areas and anatomic division of
processing verbs and prepositions (Amorapanth et al., 2009;
Damasio et al., 2001; Emmorey et al., 2002; Kemmerer, 2006;
Goksun et al., 2013; Kemmerer, 2006; Kemmerer & Tranel, 2003;
Kemmerer et al., 2012; MacSweeney et al., 2002; Tranel &
Kemmerer, 2004; Tranel, Kemmerer, Adolphs, Damasio, &
Damasio, 2003; Tranel, Manzel, Asp, & Kemmerer, 2008).
Together, these findings are consistent with Chatterjee's (2008)
suggestion that spatial perception and language have an analogous
organizational structure within the brain. That is, the left hemisphere contains a perceptual to verbal gradient, in which perceptual nodes serve as points of entry for their lexical
correspondences that are shifted toward peri-Sylvian cortex
(Chatterjee, 2008).
Here we examine the neural segregation of path and manner of
motion by testing focal brain injured individuals' production of
motion event sentences using voxel-based lesion symptom mapping (VLSM) analysis. In VLSM patients are not classified based
on lesion site, clinical diagnosis or behavioral performance. The
inferential strengths of lesion methods offer an important constraint on neural hypotheses generated by functional neuroimaging methods (Chatterjee, 2005; Fellows et al., 2005).
1.2. Gesture as a compensatory strategy for impaired speech
Speech and gesture form a tightly integrated communication
system; either part of one system or two highly interrelated systems (Alibali, Kita, & Young, 2000; Goldin-Meadow, 2003; Kita &
Ozyurek, 2003; McNeill, 1992; McNeill, 2005; for opposing views
see Krauss, Chen, & Gottesman, 2000). Although most theories
agree that spontaneous gesture production relates to speech

production, the proposed nature of this relationship differs (e.g.,
Alibali, 2005; Butterworth & Hadar, 1989; De Ruiter, 2007;
Hostetter & Alibali, 2008; Kita, 2000; Kita & Ozyurek, 2003;
McNeill, 1992, 2005). Some argue that speech and gesture originate from the same representational system, in which gesture carries a global-synthetic image of an utterance and speech carries the
linear-segmented hierarchical linguistic structure of an utterance
(McNeill, 1992, 2005) or that gestures are generated during subprocesses of speech production (Butterworth & Hadar, 1989).
Others claim that speech and gesture are generated by two separate but interrelated systems (e.g., Alibali et al., 2000; Kita, 2000;
Kita & Ozyurek, 2003; Krauss et al., 2000). For example, Krauss'
Lexical Gesture Process Model proposes that gestures are generated
from spatial imagery in working memory. These gestures prime
lexical items, increase their activation, and facilitate their access
to speech (Krauss et al., 2000). In this model, gestures are formed
before speech processes occur. Another view, the Interface Model
suggests that speech and gesture are generated by two separate,
but bidirectionally related systems. A message generator plans
speech whereas an action generator plans gesture, originating from
an interface representation between spatial thinking and speech
(Kita & Ozyurek, 2003). This model is also compatible with the
information-packaging hypothesis, which argues that gestures help
speakers to organize and package spatial information into units
that are compatible with the speech (Kita, 2000).
Evidence for the Interface Model comes from cross-linguistic
studies of gesture production. For instance, when an English
speaker expresses a ``roll down" event, the one-clause sentence
(e.g., he rolled down) accompanies a gesture that conflates path
and manner information (e.g., index finger makes circles while
moving down). In contrast, Turkish or Japanese speakers express
the same event in two clauses (e.g., he descended as he rolled)
and use two separate gestures for path and manner (e.g., one for
moving down and the other for circular movement).
Nevertheless, when English speakers use two separate clauses for
manner and path of motion, their gestures are similar to those of
Turkish speakers (Kita et al., 2007). These findings suggest that
spontaneous gestures are synchronized with speech and influenced by the form of sentences used, regardless of the surface
properties a particular language (Kita et al., 2007; Kita &
Ozyurek, 2003). Additionally, healthy people often gesture when
they communicate spatial information verbally (Alibali, 2005;
Alibali, Heath, & Myers, 2001; Feyereisen & Havard, 1999).
Only recently neurocognitive research has started to investigate
the neural correlations of co-speech gestures, suggesting that cospeech gestures and speech processing probably engage overlapping areas in the left inferior frontal gyrus (BA 45), superior temporal sulcus, and posterior middle temporal gyrus (Dick et al., 2009,
2012; Holle et al., 2008; Willems & Hagoort, 2007; Willems et al.,
2007; Willems, Ozyurek, & Hagoort, 2009). In two recent studies,
Marstaller and Burianova (2015a, 2015b) examined neural underpinnings of co-speech gestures. One was an fMRI study, showing
that co-speech gesture production engaged areas that were associated with language production such as left inferior frontal gyrus,
anterior superior temporal gyrus, bilateral posterior superior temporal sulcus, left hippocampus, parahippocampus, ventral and dorsal premotor areas, and primary motor cortex (Marstaller et al.,
2015a).
Neuropsychological evidence of neural correlates of gesture
production comes from studies with aphasic patients (e.g.,
Ahlsen, 1991; Cicone, Wapner, Foldi, Zurif, & Gardner, 1979;
Cocks, Dipper, Middleton, & Morgan, 2011; Cocks, Sautin, Kita,
Morgan, & Zlotowitz, 2009; Dipper, Cocks, Rowe, & Morgan,
2011; Feyereisen, 1983; Friederici, 1981, 1982; Glosser, Wiener,
& Kaplan, 1986; Hadar, Burstein, Krauss, & Soroker, 1998;
Kemmerer, Chandrasekaran, & Tranel, 2007; Le May, David, &

T. Goksun et al. / Brain & Language 150 (2015) 1-13

Thomas, 1988), patients with Parkinson's disease (e.g., Cleary,
Poliakoff, Galpin, Dick, & Holler, 2011), and split-brain patients
(e.g., Kita & Lausberg, 2008; Lausberg, Kita, Zaidel, & Ptito, 2003).
The key question raised by this research is whether verbal impairments lead to gestural impairments. The evidence to date is mixed.
If speech and gesture originate from the same representational system (McNeill, 1992, 2005), problems in speech would parallel
problems in gesture execution. Some studies support this hypothesis (Cicone et al., 1979; Glosser et al., 1986; Goodglass & Kaplan,
1963; McNeill, 1985). For example, Cicone et al. (1979) reported
that Broca's aphasics' gestures did not clarify their incomplete sentences. Additionally, some studies suggest that damage to the right
hemisphere is associated with more gesture production (e.g.,
Hadar & Krauss, 1999; Hadar et al., 1998; Kita & Lausberg, 2008;
Lausberg, Zaidel, Cruz, & Ptito, 2007; McNeill & Pedelty, 1995).
For example, McNeill and Pedelty (1995) suggested that the right
hemisphere injured patients who had intact language produce
many gestures because of impairment in visuo-spatial imagery.
In contrast, other findings provide evidence for separate, but
interrelated gesture and speech systems as proposed by Interface
Model (Kita & Ozyurek, 2003). For example, Broca's aphasics gesture less per minute compared to healthy control subjects, but they
also gesture more per word than control subjects (Feyereisen,
1983). Others report that aphasic patients use more iconic gestures
than healthy control subjects (Hadar et al., 1998; Lanyon & Rose,
2009). Aphasic patients may produce more meaning-laden gestures when they have trouble retrieving words than when their
production is fluent consistent with the idea that gestures facilitate
lexical retrieval (e.g., Hadar et al., 1998; Hermann, Reichle, LuciusHoene, Wallesch, & Johannsen-Horbach, 1988; Lanyon & Rose,
2009; Le May et al., 1988; Marshall, Best, Cocks, et al., 2012;
Pashek, 1998; Raymer et al., 2006; Rose & Douglas, 2001, 2008;
Rose, Douglas, & Matyas, 2002).
Even though gestures may compensate for impaired verbal
communication (e.g., Ahlsen, 1991; Fex & Mansson, 1998;
Feyereisen, 1983; Kemmerer et al., 2007; Rodriguez, Raymer, &
Rothi, 2006), less is known about how brain-injured individuals
produce spatial gestures spontaneously to accompany their
speech. If speech and gesture are generated by different but related
systems, and co-speech gestures originate from an interface representation between spatial thinking and speech (Kita & Ozyurek,
2003), how do brain-injured patients who have verbal problems
depict spatial information in gestures? In one case study,
Kemmerer et al. (2007) examined verbal and gestural descriptions
of motion events of a severely anomic patient who had a lesion
affecting the fronto-parietal and superior temporal parts of periSylvian cortex. Despite having deficits in describing motion events,
this patient used informative spontaneous gestures to communicate his knowledge about motion events. For example, when
describing a swinging event, he made an arc movement to represent the `swinging' action even when he did not use a proper verb.
We recently found that even though spontaneous gesture production correlated positively with degree of deficits in naming spatial
prepositions, patients with damage to the left posterior middle
frontal gyrus and the left inferior frontal gyrus, gestured spatial
information less often than expected (Goksun et al., 2013). Thus,
despite online coupling between gesture and speech, in cases
where information cannot be presented verbally, spontaneous gestures might step into express intact spatial knowledge.

3

whether spatial motion components of path and manner can be
selectively impaired. If comprehension and production of spatial
words are tightly linked, we predicted that patients who have damage in left peri-Sylvian fronto-parietal regions would have impairments in correctly producing words that describe paths of motion
(i.e., preposition). In contrast, patients who have damage to the left
posterior inferior/middle temporal gyrus would have problems in
correctly producing words that describe manners of motion (i.e.,
verbs). Thus, patients with left-hemisphere damage would have
problems in naming both paths and manners; but the specific neural
correlates with each would differ. Right hemisphere damaged
patients served as another control group and we predicted that they
would not have problems with speech (no impairment in naming)
and would look alike to healthy controls.
For the relationship between speech and spontaneous gestures,
if spatial language and spatial gestures share tightly intertwined
neural networks, deficits in the use of spatial prepositions and
verbs should lead to deficits in the use of gestures depicting path
and manner of motion, respectively. That is, left hemisphere damaged patients who have difficulty in naming path or manner of
motion would also have trouble generating analogous spontaneous
spatial gestures. In this case, the naming deficits could reflect a
conceptual deficit with downstream consequences regardless of
whether the output is verbal or gestural. However, if patients do
not have a conceptual problem, spontaneous spatial gestures
might be produced to compensate for deficits in retrieving words
to describe paths and/or manners of motion. Finally, for right
hemisphere damaged patients who do not have speech problems,
in line with the Interface Model (Kita & Ozyurek, 2003), we do
not expect to see compensatory gesture production.
2. Methods
2.1. Participants
We recruited 32 patients with chronic unilateral lesions from
the Focal Lesion Subject Database at the University of
Pennsylvania (Fellows, Stark, Berg, & Chatterjee, 2008). Sixteen
patients had unilateral left hemisphere damage (LHD) and 16
patients had unilateral right hemisphere damage (RHD). The database excludes patients with a history of other neurological disorders, psychiatric disorders, or substance abuse. We did not select
patients based on specific lesion locations or behavioral criteria.
VLSM analyses are more useful in patient populations with different lesion locations. LHD patients ranged in age from 37 to 79
(M = 64.69, SD = 11.49, 10 females) and RHD patients ranged in
age from 45 to 87 (M = 63.50, SD = 11.99, 11 females). The average
years of education for LHD (M = 13.6, SD = 2.02) and RHD patients
(M = 15.1, SD = 3.44) were comparable. Fourteen age-matched
(range: 38-77, M = 60.85, SD = 11.05, 9 females) and educationmatched (M = 16, SD = 2.12) elderly healthy adults served as a control group (HC). The three groups did not differ in age or years of
education, ps > .05. Additionally, LHD and RHD patients did not differ in lesion size, p > .05. Fig. 1 displays lesion overlap maps of
patients. All participants were right-handed, native Englishspeakers, had normal or corrected to normal vision, and no hearing
loss. They provided written, informed consent in accordance with
the policies of the University of Pennsylvania's Institutional
Review Board. Participants received $15/h for volunteering their
time. Table 1 presents demographic data for each patient.

1.3. The current study
2.2. Tasks and stimuli
In this study, by testing focal brain injured patients, we investigate (1) the neural organization of spatial motion event expressions
in English, and (2) the relation between verbal and spontaneous gestural information in describing these events. The first question is

2.2.1. Neuropsychological tasks
Patients were administered the language comprehension and
language production subtests of the Western Aphasia Battery

4

T. Goksun et al. / Brain & Language 150 (2015) 1-13

Fig. 1. Coverage map indicating the lesion locations for all participants. The colored scale represents the number of patients with a lesion in that pixel.

Table 1
Patient demographic and neuropsychological data.
Patient

Gender

Age

Education
(years)

Lesion
Side

Location

Lesion size (# of
voxels)

Cause

Chronicity
(months)

WAB
(AQ)

OANB
(Action)

OANB
(Object)

LT_85
CD_141
KG_215
TO_221
BC_236
XK_342
TD_360
IG_363
KD_493
DR_529
DR_565
MC_577
NS_604
UD_618
KM_642
CC_749
FC_83
MB_101
NC_112
RT_309
DF_316
DC_392
DX_444
TS_474
NS_569
DG_592
KG_593
KS_605
ND_640
CS_657
KN_675
MN_738

F
F
M
F
M
F
M
M
M
F
F
F
F
M
M
F
M
F
F
F
F
M
F
F
F
F
F
M
F
M
M
F

63
52
61
77
65
57
58
74
68
66
53
79
37
77
77
71
70
58
48
66
87
56
80
51
72
45
49
63
70
75
64
62

15
16
14
13
18
12
12
16
14
12
12
11
12
15
12
12
12
18
16
21
12
10
12
11
18
12
12
18
18
18
18
16

L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
L
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R
R

I
T
F
O
FP
OT
T BG
F
T
PA F
PA F
C
PO
F
P
P
FTP
T BG
O
T
P
PT
PT
P
FT BG
PT
FTP BG
C
PT
PO
FT
C

13,079
21,605
17,422
5886
155,982
42,144
38,063
16,845
22,404
8969
14,517
4191
79,231
48,743
7996
34,266
8040
10,543
4733
79,691
2981
39,068
41,172
22,208
37,366
130,552
170,128
23,217
64,603
33,568
23,779
32,154

Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Aneurysm
Stroke
Aneurysm
Stroke
AVM
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Hematoma
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke
Stroke

177
143
145
160
210
125
118
117
101
100
103
50
113
47
109
50
169
426
178
128
126
108
106
100
77
127
58
76
54
43
32
25

98.8
98.8
94.4
100
80.8
91.4
65.3
91.4
92.1
94.9
99.8
85.3
100
89.4
96.8
88.8
99.8
98.4
100
-
97.1
97.6
95.5
95.1
100
97.8
100
98.8
96.8
99.2
-
98.4

100
100
96
100
88
94
52
96
98
94
98
82
100
76
94
-
96
98
98
-
88
98
94
98
100
98
90
100
100
98
-
100

98.8
96
93.8
100
94
93
28
95
95
90.1
97.5
79
98
85
98
-
98
98
-
-
93
95
93
95
99
98
95
100
100
100
-
100

Key: F: frontal; T: temporal; P: parietal; O: occipital; BG: basal ganglia; C: cerebellum; I: insula; PA: pericallosal artery. WAB-AQ indicates a composite language score with a
maximum possible score of 100. OANB (action) and OANB (object) demonstrate knowledge of verbs and nouns with a maximum possible score of 100.

(WAB; Kertesz, 1982). They were also administered the Object and
Action Naming Battery (OANB; Druks, 2000). This task included 50
pictures of actions and 81 pictures of objects. The scores from these
neuropsychological tests are presented in Table 1.

2.2.2. Experimental tasks
The experimental task consisted of 22 dynamic movie clips,
depicting different motion events. Different combinations of 10
manners (hop, skip, walk, run, cartwheel, crawl, jump, twirl,

T. Goksun et al. / Brain & Language 150 (2015) 1-13

March, step) and 9 paths (through, to, out of, under, over, in front
of, around, across, into) were used. All actions in the video were
performed by a woman outdoors. The movies were created with
a Sony digital camera and were edited using iMovie (see Fig. 2
for sample stimulus). Each movie lasted for 3-4 s. The final set of
22 was selected from 32 movies based on ratings of familiarity
and descriptions of the actions (both path and manner) by 18
native English speakers with a mean age of 21.88 (range: 18-27,
SD = 2.76). After watching each movie clip, they first rated the
familiarity of the action in the clip on a 5-point scale (1 = not familiar at all, 5 = very familiar). Then, they described what the woman
was doing in each clip. For example, when the participants saw the
woman running across the street, they first rated the familiarity of
action by simply hitting 1-5 on the keyboard. Then, they described
the action by saying something like ``the woman ran across the
street." Two practice trials were presented before the start of the
task. Stimuli were presented on a Macbook Air computer using
MATLAB version 7.5.0., 2007 Psychtoolbox. The final set of 22
movie clips was selected based on the agreement among the participants. Movie clips with an average of at least 3.5 familiarity rating and 99% naming agreement were used.
2.3. Procedure
Participants were tested individually in the laboratory or in
their homes. After 2 practice trials, each participant received 22
test trials in a random order. After watching the short movie clip
on the screen, the experimenter asked the participants to describe
what the woman did in the clip. The experimenter presented the
movie clips on a Macbook Air computer using Matlab, 2007 and
advanced to the next trial when the participant was ready. The session was videotaped for further transcriptions of speech and gesture. The experimenter did not mention gestures to the
participants or encourage their gesturing during the tasks. The
neuropsychological tasks were administered on a different testing
session either before or after the experimental tasks.
2.3.1. Coding
2.3.1.1. Speech. Native English speakers transcribed all speech verbatim for participants' responses to each trial. First, speech for each
trial was coded for the correct use of manner (how the action was
performed) and path (the trajectory of action) information based
on what the participant said in each trial. That is, we coded the
accuracy of verb (manner) and preposition (path) in each trial.
We categorized each response into three categories: (1) manner
only (2) path only and (3) manner + path together. For example,
for the event in which the woman was running around a tree, manner only response would be ``she was running" (i.e., running is the
manner of the action). In contrast, the same event could be
described as ``she went around the tree," which omits the manner

5

and mentions only the path of the action (i.e., around). Finally, the
description ``she ran around the tree" constitutes both manner (i.e.,
run) and path (i.e., around) information.
2.3.1.2. Gesture. We transcribed the participants' spontaneous use
of gestures for each trial. A change in the shape of the hand or
motion signaled the end of a gesture. For each trial, the coders initially decided whether at least one gesture was produced or not.
The gestures in each trial were then classified as (1) static or (2)
dynamic. Static gestures referred either to objects or to locations.
These gestures included pointing to the objects, depicting a property of the objects, or illustrating the location of an object (e.g.,
pointing to the right side to refer to the location of an action).
Dynamic gestures involved the movement of the hand in one directional axis (e.g., from left to right or back and forth) or circular
movements of the hand. We focused on dynamic gestures that
were iconic. For the purposes of this study, we further classified
each dynamic gesture into three types: (1) manner only, (2) path
only, and (3) manner + path together. Manner only gestures
depicted the manner of motion without depicting path (e.g., repetitive up and down movement of index and middle fingers without
any forward motion to represent walking). Path only gestures
depicted changes of location without depicting manner (e.g., palm
faces down moves straight from left to right to represent across).
Manner + path together gestures encoded both of these components simultaneously (e.g., moving a hand forward while repetitively moving the index and middle fingers to represent `walk
over').
After classifying each gesture into a category, we coded whether
patients produced complementary gestures (same information in
gesture as in speech) or additional/compensatory gestures (appropriate gesture with inaccurate or absent path-manner verbal information; or gesture and speech contain different information for
path and manner). For each sentence type in speech (manner only,
path only, manner + path, and no dynamic information), we
recorded the frequency with which LHD and RHD patients produced different types of gestures (manner only, path only, manner + path).
2.4. Reliability
To establish the reliability of the coding system, we conducted
two types of reliability by a second person. First, the second coder
randomly chose and independently coded 20% participants' all
responses both for speech and gesture. That is, she fully coded
the speech and gesture for 9 participants. For speech, agreement
between coders was 93.4% (n = 198 trials) in assigning manner
only, path only, manner + path categories to the descriptions. For
gesture, agreement between coders was 92.1% (n = 198 trials) for
gesture identification, 94.3% for gesture category (static vs.

Fig. 2. Sample stimuli from the experimental task. The pictures are still frames from two motion events: jump over (left side) and walk across (right side). The yellow arrows
indicate the direction of the person's movement. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

6

T. Goksun et al. / Brain & Language 150 (2015) 1-13

dynamic, n = 198 trials), and 91.0% (n = 198 trials) for coding gestures that involved manner only, path only, and manner + path.
Second, she randomly chose and coded 20% of each patient's
responses both for speech and gesture (n = 184 trials in total, 4 trials from each person). For speech, agreement between coders was
93.6% in assigning manner only, path only, manner + path categories to the descriptions. For gesture, agreement between coders
was 97.8% for gesture identification, 97.6% for gesture category
(static vs. dynamic), and 93.7% for assigning gestures as involving
manner only, path only, and manner + path.

2.5. Analyses
2.5.1. Behavioral analyses
For speech, the dependent variable was the naming of manner
and path information in the video clips. First, for each trial we
coded whether a participant named the manner (verb) and/or
path (prepositions) of the specific video clip correctly. We next
calculated the percentage of trials for the use of manners (verbs)
and paths (prepositions). Then, for each trial, we categorized
whether a participant named manner only, path only or both
(manner + path) in a given trial. The percentages of manner only,
path only, and manner + path together responses were calculated
for each patient. In addition to these coding, we computed
whether speech was absent (no word related to an action) or
whether manner only and path only sentences contained errors
(e.g., in manner only, path is incorrect). For gestures, first, the
percentage of static vs. dynamic gestures to the overall gesture
use was calculated. Then, the percentage of trials in which participants produced at least one dynamic iconic gesture was calculated. We categorized these dynamic gestures as manner only,
path only, and manner + path together. As in speech, the percentage of trials in which participants produced manner only, path
only, and manner + path together was calculated. We also calculated the frequency of using complementary and additional/compensatory gestures for different types of gestures (manner only,
path only, manner + path).

2.5.2. Neuroanatomical analyses
Clinical CT or MRI scans for all patients were rendered to a common anatomical space (Colin27; http://imaging.mrccbu.cam.ac.
uk/downloads/Colin). In our database, all lesions are drawn manually on a slice-by-slice basis by two senior neurologists, each with
over 25 years of experience. Distortions caused by chronic large
vascular lesions, which result in focal areas of atrophy, and ventricular dilation ex vacuo are compensated for by aligning lesions with
preserved neuroanatomic landmarks in non-lesioned parts of the
brain. We then conducted voxel-based lesion symptom mapping
(VLSM; Bates et al., 2003) analyses, using Voxbo brain-imaging
analysis software developed at the University of Pennsylvania
(http://www.nitrc.org/projects/voxbo/). VLSM assessed the relation between behavioral measures and brain lesions on a voxel
by voxel basis. We restricted the analyses to the voxels in which
at least 2 patients had lesions. The analyses resulted in statistical
t-maps of lesioned brain areas that were significantly related to
impaired behavioral performances. The t-map for each analysis
was thresholded at q < .05 using the False Discovery Rate (FDR)
to control for multiple comparisons (Benjamini & Hochberg,
1995; Genovese, Lazar, & Nichols, 2002). We conducted VLSM analyses for speech and gesture dependent variables separately. Onetailed t-tests for speech accuracy and two-tailed t-tests for gesture
production compared behavioral scores between patients with and
without lesions at every voxel.

3. Results
3.1. Neuropsychological tests
Even though most of these patients were not severely impaired,
WAB scores were lower for the LHD patients compared to the RHD
patients, F(1, 27) = 4.713, p = .039, g2 = .17 (M = 92.62 and
M = 98.18, LHD and RHD, respectively). For naming objects and
actions, the groups did not differ significantly, ps > .05 (see
Table 1).

3.2. Speech
We first calculated participants' overall performance for producing verbs (manners) and prepositions (paths). Univariate
ANOVAs with the group (LHD, RHD, and HC) as the betweensubject variable and the correct use of verbs and prepositions as
the dependent variables revealed main effects of group, F(2, 43)
= 8.01, p < .001, g2 = .27 and F(2, 43) = 25.07, p < .001, g2 = .54. As
shown in Fig. 3, the LHD patients were less accurate than both
the RHD patients and HC participants in both naming manners
and paths (Scheffe, ps < .05). Further comparisons indicated that
the LHD patients were worse in naming paths (prepositions) than
manners (verbs), t(15) = 3.91, p < .001. No difference was found for
other groups. WAB scores also correlated with naming paths
(prepositions) and manners (verbs), r = .73 and r = .60, ps < .01.
Results remained the same when we controlled for age and
chronicity.
Second, we analyzed whether participants named manner only,
path only, or manner + path together in their description of the
events. As displayed in Fig. 4, the LHD patients produced fewer correct manner + path together sentences than both the RHD patients
and HC participants, F(2, 43) = 5.26, p < .01, g2 = .20. Additional
descriptive analyses showed that LHD patients omitted path
expressions more than other groups and produced only the manner expressions. For every trial, patients produced some words
(there was no absent speech). Yet, in 14% of the trials LHD patients
did not produce any motion verbs or prepositions (see Table 2 for
the number of trials in each category).
We found significant lesion-symptom relations for deficits in
producing manners expressions (verbs) and path expressions
(prepositions). The FDR corrected t-statistic thresholds with a significance level of q = .05 were 3.05 and 3.41 for paths and manners,
respectively. As displayed in Fig. 5a, lesions to the left posterior
middle frontal gyrus, the left inferior frontal gyrus, and the left

Fig. 3. The percentage of trials on which LHD patients, RHD patients and HC
participants correctly named manners (verbs) and paths (prepositions). * p < .05;
error bars referred to the standard error of mean.

T. Goksun et al. / Brain & Language 150 (2015) 1-13

7

patients with lesions to the left anterior superior temporal gyrus
and underlying white matter significantly produced more path
only gestures than those that did not have lesions in this location.
As no lesions were correlated to manner gestures and the use of
manner was low in number, we did not run residual analyses for
gesture production.
3.4. Speech-gesture relations

Fig. 4. The percentage of trials on which LHD patients, RHD patients and HC
participants correctly named manner only, path only, manner + path together, and
none. Error bars referred to the standard error of mean.

anterior superior temporal gyrus were associated with impairments in naming path of motion. Lesions to the left caudate and
white matter underlying middle frontal gyrus were related to
impairments in naming manners (Fig. 5b).
To contrast the neural bases particularly for naming manner
and path, we used VLSM analyses of residual scores (Amorapanth
et al., 2012). We calculated the residual scores of one dependent
variable (e.g., path) regressed onto another (e.g., manner) and
paired these scores with lesion data in VLSM analyses. Similar to
the fMRI analyses,
VLSM residual analyses would present a contrast between two
dependent variables while accounting for the shared lesions
between variables. Based on these analyses, we retrieved the FDR
corrected t-statistic thresholds (with a significance level of
q = .05) of 3.68 and 4.71 for manner > path and path > manner
residual analyses, respectively. As shown in Fig. 6, lesions to the
left posterior middle frontal gyrus, the left inferior frontal gyrus
were critical for naming path of motion (beyond what might be
expected with a deficit in naming manners of motion) while
lesions to the left caudate and white matter underlying middle
frontal gyrus were critical for impairments in naming manners.
3.3. Gesture analyses
As a group LHD patients produced gestures in significantly more
trials (67 trials, M = 19%, 81 gestures) than RHD patients (21 trials,
5.9%, 26 gestures) and HC (17 trails, 5.5%, 40 gestures), F(2, 43)
= 3.12, p < .05. g2 = .13. All path and manner gestures were accurate
for the specific trial. Participants also produced a few static gestures that referred to objects in the actions (LHD = 14 gestures,
RHD = 2 gestures, and HC = 2 gestures).
We then examined whether groups differed in using manner
only, path only, or manner + path together in their gestures. No
reliable differences were found in terms of how often each group
produced these types of gestures (see Fig. 7). However, we found
significant lesion-symptom relations for producing path only gestures that occurred with speech. The FDR corrected t-statistic
thresholds with a significance level of q = .05 was 3.29 for path
only gesture production. In particular, as displayed in Fig. 8,

Table 2
The number of trials each group used manner only, path only, and manner + path
expressions or the number of trials neither component was correct (error).

LHD (n = 352 trials)
RHD (n = 352 trials)
Controls (n = 308 trials)

Manner only

Path only

Manner
+ path

Errors

118
29
23

32
15
5

152
304
280

50
4
0

For patients, the use of spatial gestures correlated negatively
with the accuracy in naming manners and paths, rs = .37 and
rs = .23, ps < .001. For HC, spatial gestures correlated negatively
only with the accuracy in naming paths, r = .39, p < .001.
To further analyze whether patients produced complementary
gestures (same information in speech and gesture) or compensatory
gestures (inaccurate or absent path-manner information in speech
and appropriate gesture), for each sentence type in speech (manner only, path only, manner + path, and no dynamic information)
we counted how frequently LHD and RHD patients produced different types of gestures (manner only, path only, manner + path).
The total number of complementary and compensatory gesture
was low in the whole group. Nonetheless, LHD patients produced
complementary gestures in 42% of the trials (28 out of 67 gesture
trials) and compensatory gestures in 58% of the trials (38 out of
67). Seven different patients produced at least 1 compensatory gesture (range: 1-8). Among the compensatory gestures LHD patients
used mostly path only information (44%) - the other types (manner only and manner + path) were used in the rest of the trials
(28% each). In contrast, RHD patients' gestures were mostly complementary (90.5%, 19 out of 21 trials) and compensatory gestures
were used only in 9.5% of their gesture trials (2 out 21 trials). RHD
patients' gesture production was similar to HC, who produced
compensatory gestures only in 5.8% of their gesture trials (1 out
17 trials) and the rest was complementary gestures. Six different
RHD patients and 5 different HC produced complementary gestures at least in one trial.
In addition to the trial analyses, we counted how each patient
group used speech and gesture combinations for manners and
paths. As presented on Table 3, LHD patients produced many additional or compensatory gestures either presenting one component
in speech and one in gesture, or gesturing about the path or manner of action without relevant speech. In contrast, RHD patients'
gestures involved the same components in both speech and
gesture.
4. Discussion
To evaluate the neural organization of spatial language and
spontaneous spatial gestures, we examined focal brain injured
individuals' descriptions of events that contained path and manner
motion components. Two main hypotheses framed our investigation. First, if the neural organization of perceiving and producing
path (preposition) and manner (verb) of motion information are
aligned, patients who have damage in left peri-Sylvian frontoparietal regions would also have problems in naming paths (i.e.,
preposition) in speech whereas patients who have damage to the
left posterior inferior/middle temporal gyrus would have problems
in naming manners (i.e., verbs). Thus, even though LHD patients
would have more naming problems compared to RHD and HC, producing path (preposition) and manner (verb) of motion words
could be selectively impaired. Second, if spatial language and spatial gestures share tightly intertwined neural networks, deficits in
the use of spatial prepositions and verbs should lead to deficits
in the use of gestures depicting path and manner of motion,
respectively. In contrast, if speech and gesture are generated by

8

T. Goksun et al. / Brain & Language 150 (2015) 1-13

Fig. 5a. Representative slices from VLSM analyses for naming path of motion (prepositions). The maps show significant t-scores with a FDR of q = .05.

Fig. 5b. Representative slices from VLSM analyses for naming manner of motion (verbs). The maps show significant t-scores with a FDR of q = .05.

two different but related systems (Kita & Ozyurek, 2003), problems
in naming prepositions and verbs would not necessarily lead to
deficits in gesture use; rather gesture use might increase with lexical access deficits.
We found that LHD patients performed worse than RHD
patients and healthy controls in naming paths and manners of
motion in dynamic events. LHD patients were less accurate in
naming paths than manners, thus, had problems producing full
manner + path sentences. Lesions to the left posterior middle

frontal gyrus, the left inferior frontal gyrus, and the left anterior
superior temporal gyrus impaired the ability to produce prepositions describing paths of motion. Lesions to the left caudate and
adjacent white matter impaired the ability to produce verbs
describing manners of motion. Although the groups did not produce many spontaneous gestures, gesture analyses showed that
LHD patients, as a group, produced more spatial gestures than
other groups. Intriguingly, LHD patients who had lesions to the left
superior temporal gyrus produced significantly more path gestures

T. Goksun et al. / Brain & Language 150 (2015) 1-13

9

Fig. 6a. Representative slices from VLSM residual analyses for naming impairment of path > manner. The maps show significant t-scores with a FDR of q = .05.

Fig. 6b. Representative slices from VLSM residual analyses for naming impairment of manner > path. The maps show significant t-scores with a FDR of q = .05.

than other patients. In line with the Interface Model, these observations suggest that spatial language and spatial gestures can be
selectively impaired and speech and gesture are generated by
two separate, but related systems (Kita & Ozyurek, 2003).
4.1. The neural correlates of producing words for motion events
Chatterjee (2008) suggested that spatial perception and language have an analogous organizational structure within the brain.
Our predictions about the functional anatomy of producing prepositions (the path) and verbs (the manner) of dynamic events were

partially confirmed. As a group, LHD patients had more impaired
naming of motion components compared to RHD and HC.
Naming path and manner of motion in dynamic events can also
be dissociated and have different neural underpinnings. However,
perceiving and naming these spatial components share related
but not identical neural organization.
Paths of motion are verbally represented by prepositions in
English and our findings are consistent with the previous research
on both naming and comprehending prepositions (Amorapanth
et al., 2009; Damasio et al., 2001; Kemmerer & Tranel, 2003;
Noordzij et al., 2008; Tranel & Kemmerer, 2004). In particular,

10

T. Goksun et al. / Brain & Language 150 (2015) 1-13

Fig. 7. The percentage of trials on which LHD patients, RHD patients and HC
participants produced manner only, path only, manner + path together gestures.
Error bars referred to the standard error of mean.

these studies as well as research with Broca's aphasics (e.g.,
Friederici, 1981, 1982; Kemmerer & Tranel, 2003; Tesak &
Hummer, 1994) report the involvement of frontal regions in processing prepositions. These findings are also consistent with the
view that the left hemisphere is dominant for spatial processing
when the kind of spatial information is categorical and can be
named (Amorapanth et al., 2009; Kosslyn, Thompson, Gitelman,
& Alpert, 1998).
We provide additional evidence suggesting that the left anterior
superior temporal gyrus (aSTG) is involved in producing prepositions. This is consistent with our previous study that shows the
involvement of left aSTG in producing spatial static and dynamic
prepositions such as the book is on the table and the book is moving over the table (Goksun et al., 2013). Additionally, categorical
spatial relation deficits are associated with lesions in the white
matter undercutting the left aSTG (Amorapanth et al., 2009) and

impairments in matching locative prepositions to the proper pictures were linked to damage to the left aSTG (Wu et al., 2007).
The aSTG and parts of the dorsolateral prefrontal cortex are connected by the temporo-frontal extreme capsular fasciculus. The
importance of this pathway for language, while anticipated by
Wernicke as quoted in Petrides (2013), has not been emphasized
in neurolinguistics research. First described by Petrides and
Pandya (1988, 2009) in the macaque, the analog of this fasciculus
has been confirmed in humans (Frey, Campbell, Pike, & Petrides,
2008; Petrides, 2013). We propose that the temporo-frontal fasciculus helps organize the aSTG with ventral prefrontal cortices into
a functional unit. One function of this unit is to express path information in speech (and integrate it with gesture as we comment on
below).
Manners of motion are encoded by verbs in English. We
detected poor performance in naming manners of motion in
patients with lesions to the left caudate and adjacent white matter.
Left subcortical damage can be associated with agrammatism
(Mega & Alexander, 1994) and monitoring lexical and semantic
aspects of language (Crosson, 1992; Fabbro, Clarici, & Bava, 1996;
Wallesch & Papagno, 1988). Additionally, in an fMRI study,
Grossman et al. (2002) found bilateral caudate activation when
people processed motion verbs. We suggest that left caudate might
also be involved in producing motion verbs.
Our results on the neural vulnerabilities in naming paths and
manners of motion do not confirm two anatomic aspects of our
predictions. Since the left posterior middle temporal gyrus is frequently associated with comprehending verbs (Kable et al., 2002,
2005; Kemmerer et al., 2008) we would have predicted that this
area might also be involved in the production of words. The second
area is the left inferior parietal lobule (IPL) that is associated with
comprehending prepositions. One possibility is that the classic distinction between comprehension and production of language
applies to these systems with posterior regions instantiating core
lexical semantics linked to their production anteriorly by the relevant connecting pathways. On this view, the IPL and posterior middle temporal gyrus might be more relevant to language

Fig. 8. Representative slices from VLSM analysis for producing path only gestures. The map shows significant t-scores with a FDR of q = .05.

T. Goksun et al. / Brain & Language 150 (2015) 1-13
Table 3
The number of times LHD and RHD patients produced complementary (same
information in both speech and gesture) or additional/compensatory gestures
(different or extra information in gesture).
Manner S & G

Manner G + path S

Manner G (no relevant speech)

LHD
RHD

17
5
Path S & G

5
1
Path G manner S

16
1
Path G (no relevant speech)

LHD
RHD

23
20

10
0

17
0

comprehension rather than production of words to describe path
and manner information, respectively. Furthermore, their respective frontal connections through the inferior aspects of the superior
longitudinal fasciculus and parts of the arcuate fasciculus may segregate within the white matter (see Frey et al., 2008, for discussion
of these pathways) and in principle could be damaged selectively.
Another possibility is that we may have lacked sufficient power to
detect the significant effects of lesions in the IPL and posterior middle temporal gyrus.
4.2. Gesture for compensation to impaired speech
We report three key findings on the relationship between
speech and spontaneous gesture. First, LHD patients produced
more spatial gestures than both RHD patients and control participants. The use of spatial gestures correlated positively with the
degree of impairment in naming paths and manners. Second, even
though both patient groups complement their speech with gestures, only LHD patients used gestures to compensate for speech.
Last, patients who had damage to the left superior temporal gyrus,
seemed to produce relatively more spontaneous path gestures.
Our results show that patients use gestures for two purposes -
to complement spatial information expressed in speech or to compensate for the impaired expression of spatial information. In many
cases, particularly when individuals had intact speech, gesture was
tightly coupled with verbal content such as making an up-down
movement with right hand to represent ``hopping" when saying,
``she hops to the door." Yet, LHD patients also produced spatial gestures to add to or compensate for the spatial information they did
not express verbally. For example, to describe the event ``hopping
around a tree," some make a circle in gesture to represent ``around"
while only saying ``she hopped." More than half the LHD patients'
gestures made these compensatory gestures, supporting previous
findings on spontaneous gesture's role to compensate for speech
problems (e.g., Ahlsen, 1991; Fex & Mansson, 1998; Feyereisen,
1983; Kemmerer et al., 2007; Rodriguez et al., 2006). Participants
never produced spontaneous iconic gestures that had no meaningful relationship to the spatial characteristics of the target event.
These findings support the Interface Model (Kita & Ozyurek,
2003). When the speech is intact both the message generator
and action generator work in close alignment as they both originated from the same representational system. Because these two
systems are different subcomponents of the same representational
system, impairments in one system do not necessarily produce
deficits in the other system. In our case, the representation of
events could not be expressed properly in speech. The action generator is still intact and could represent the event through gestures.
Thus, we suggest that these LHD patients have problems linking
event representations to the message generator (lexical retrieval),
but are able to express their knowledge of spatial concepts using
gestures.
Our speech findings show that when individuals have lesions in
the left superior temporal gyrus, their naming of paths (prepositions) is impaired. Those patients also produced more spatial

11

gestures depicting path information. That is, brain damage in this
area produces more rather than less, of this communicative behavior. The patients with damage to the left posterior middle frontal
gyrus and the left inferior frontal gyrus, also had impairments in
naming paths of motion (prepositions), but did not produce more
gestures than expected. In a previous study, we found that when
the patients had damage to the left posterior middle and inferior
frontal gyri, they gestured less than expected (Goksun et al.,
2013). We suggest that the aSTG and dorsolateral parts of prefrontal cortex form a functional unit connected by temporofrontal extreme capsule fasciculus. One function of this unit is to
organize spatial path information. However, the subcomponent
of this system that instantiates the motor aspects, or spontaneous
gestures of this system is localized to the prefrontal and not temporal cortices. This claim is consistent with the view proposed by
Marstaller and Burianova (2015a) that the coordination of speech
and gesture involves a left lateralized motor control system
engaged in planning and execution of such actions. Thus, damage
in the aSTG that results in difficulties producing locative terms still
allows or even releases prefrontal motor control systems to
express gestures in order to facilitate communication. Despite
being consistent with Marstaller and Burianova's (2015a) conclusions from fMRI data, we are cautious about these claims about
the role of aSTG since, the patients and HC produced relatively
few gestures.
We did not assess the patients for apraxia of speech or limb
apraxia. There might have been some motor deficits resulting in
these impairments. Apraxia can occur at multiple levels, and spontaneous gestures vary from praxis in their explicitness and representational underpinnings (drawing on spatial relations here vs.
tool knowledge/skilled movements). Future studies will need to
address the link among spontaneous gesture use, limb apraxia,
and apraxia of speech.
Our findings have implications for the treatment of word retrieval impairments. Studies report that using gestures with verbal
treatment improves both noun and verb retrieval in patients with
aphasia (e.g., Pashek, 1997; Raymer et al., 2006; Rose & Douglas,
2001). In addition, case studies indicate that making iconic gestures for objects, but not pointing to the objects, facilitates naming
the objects (Rose & Douglas, 2001; Rose et al., 2002). However,
others suggest that verbal treatment might still facilitate word
retrieval more than gestural treatment, but the effects vary at individual levels (Marshall et al., 2012). Our study suggests that
because dynamic spatial gestures can compensate for speech, specialized treatments that encourage gesturing for lexical retrieval
might improve their communication abilities.

5. Conclusions
In conclusion, we investigated the neural organization of naming spatial motion event components (path and manner) and the
relationship of spontaneous gestures and naming of motion components of dynamic events. Our findings suggest that naming
and comprehending motion event components do not have identical neural structures. Damage to the left posterior middle frontal
gyrus, the left inferior frontal gyrus, and the left anterior superior
temporal gyrus produce deficits in producing spatial prepositions
or naming paths of motion whereas lesions in the left caudate
and white matter underlying left middle frontal gyrus produce deficits in producing action verbs or naming manners of motion.
Spontaneous spatial gestures help patients communicate when
they have difficulty in retrieving words for spatial information,
especially when their lesions involve the anterior superior temporal gyrus. This pattern suggests that spontaneous gestures represent intact conceptual knowledge.

12

T. Goksun et al. / Brain & Language 150 (2015) 1-13

Acknowledgments
This research was supported in part by NIH RO1DC012511 and
grants to the Spatial Intelligence and Learning Center, funded by
the National Science Foundation (subcontracts under SBE0541957 and SBE-1041707). We would like to thank everyone in
the Chatterjee Lab for their helpful comments in this research.
Special thanks to Marianna Stark and Eileen Cardillo for their help
in recruiting patients, Melissa Hansen for her assistance in creating
stimuli, and Olufunsho Faseyitan and Alex Kranjec for their help in
the VLSM analyses.

References
Ahlsen, E. (1991). Body communication as compensation for speech in a Wernicke
aphasic - A longitudinal study. Journal of Communication Disorders, 24(1), 1-12.
Alibali, M. W. (2005). Gesture in spatial cognition: Expressing, thinking,
communication about spatial information. Spatial Cognition and Computation,
5, 307-331.
Alibali, M., Heath, D., & Myers, H. (2001). Effects of visibility between speaker and
listener on gesture production: Some gestures are meant to be seen. Journal of
Memory and Language, 44(2), 169-188.
Alibali, M. W., Kita, S., & Young, A. J. (2000). Gesture and the process of speech
production: We think, therefore we gesture. Language and Cognitive Processes,
15(6), 593-613.
Amorapanth, P. X., Kranjec, A., Bromberger, B., Lehet, M., Widick, P., Woods, A., ...
Chatterjee, A. (2012). Language, perception, and the schematic representation of
spatial relations. Brain and Language, 120, 226-236.
Amorapanth, P. X., Widick, P., & Chatterjee, A. (2009). The neural basis for spatial
relations. Journal of Cognitive Neuroscience, 22(8), 1739-1753.
Baciu, M., Koenig, O., Vernier, M. P., Bedoin, N., Rubin, C., & Segebarth, C. (1999).
Categorical and coordinate spatial relations: fMRI evidence for hemispheric
specialization. NeuroReport, 10(6), 1373-1378.
Bates, E., Wilson, S. M., Saygin, A. P., Dick, F., Sereno, M. I., Knight, R. T., et al. (2003).
Voxel-based lesion symptom mapping. Nature Neuroscience, 6(5), 448-450.
Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate - A
practical and powerful approach to multiple testing. Journal of the Royal
Statistical Society Series B-Methodological, 57(1), 289-300.
Butterworth, B., & Hadar, U. (1989). Gesture, speech, and computational stages: A
reply to McNeill. Psychological Review, 96, 168-174.
Chatterjee, A. (2001). Language and space: Some interactions. Trends in Cognitive
Sciences, 5, 55-61.
Chatterjee, A. (2008). The neural organization of spatial thought and language.
Seminars in Speech and Language, 29(3), 226-238.
Cicone, M., Wapner, W., Foldi, N., Zurif, E., & Gardner, H. (1979). Relation between
gesture and language in aphasic communication. Brain and Language, 8(3),
324-349.
Cleary, R. A., Poliakoff, E., Galpin, A., Dick, J. P. R., & Holler, J. (2011). An investigation
of co-speech gesture production during action description in Parkinson's
disease. Parkinsonism & Related Disorders, 17, 753-756.
Cocks, N., Dipper, L., Middleton, R., & Morgan, G. (2011). What can iconic gestures
tell us about the language system? A case of conduction aphasia. International
Journal of Language & Communication Disorders, 46(4), 423-436.
Cocks, N., Sautin, L., Kita, S., Morgan, G., & Zlotowitz, S. (2009). Gesture and speech
integration: An exploratory study of a man with aphasia. International Journal of
Language & Communication Disorders, 44, 795-804.
Crosson, B. (1992). Subcortical functions in language and memory. New York:
Guilford.
Damasio, H., Grabowski, T. J., Tranel, D., Ponto, L. L. B., Hichwa, R. D., & Damasio, A. R.
(2001). Neural correlates of naming actions and of naming spatial relations.
Neuroimage, 13(6), 153-164.
De Ruiter, J. P. (2007). Postcards from the mind: The relationship between speech,
imagistic gesture, and thought. Gesture, 7(1), 21-38.
Dick, A. S., Goldin-Meadow, S., Hasson, U., Skipper, J. I., & Small, S. L. (2009). Cospeech gestures influence neural activity in brain regions associated with
processing semantic information. Human Brain Mapping, 30(11), 3509-3526.
Dick, A. S., Goldin-Meadow, S., Solodkin, A., & Small, S. L. (2012). Gesture in the
developing brain. Developmental Science, 15(2), 165-180.
Dipper, L., Cocks, N., Rowe, M., & Morgan, G. (2011). What can co-speech gestures in
aphasia tell us about the relationship between language and gesture? A single
case study of a participant with conduction aphasia. Gesture, 11, 123-147.
Druks, J. (2000). An object and action naming battery. Psychology Press.
Emmorey, K., Damasio, H., McCullough, S., Grabowski, T., Ponto, L. L. B., Hichwa, R.
D., et al. (2002). Neural systems underlying spatial language in American sign
language. Neuroimage, 17(2), 812-824.
Fabbro, F., Clarici, A., & Bava, A. (1996). Effects of left basal ganglia lesions on
language production. Perceptual and Motor Skills, 82(3), 1291-1298.
Fellows, L. K., Stark, M., Berg, A., & Chatterjee, A. (2008). Patient registries in
cognitive neuroscience research: Advantages, challenges, and practical advice.
Journal of Cognitive Neuroscience, 20(6), 1107-1113.

Fex, B., & Mansson, A. (1998). The use of gestures as a compensatory strategy in
adults with acquired aphasia compared to children with specific language
impairment (SLI). Journal of Neurolinguistics, 11(1-2), 191-206.
Feyereisen, P. (1983). Manual activity during speaking in aphasic subjects.
International Journal of Psychology, 18(6), 545-556.
Feyereisen, P., & Havard, I. (1999). Mental imagery and production of hand gestures
while speaking in younger and older adults. Journal of Nonverbal Behavior, 23(2),
153-171.
Frey, S., Campbell, J. S. W., Pike, G. B., & Petrides, M. (2008). Dissociating the human
language pathways with high angular resolution diffusion fiber tractography.
The Journal of Neuroscience, 28(45), 11435-11444.
Friederici, A. (1981). Production and comprehension of prepositions in aphasia.
Neuropsychologia, 19(2), 191-199.
Friederici, A. (1982). Syntactic and semantic processes in aphasic deficits - The
availability of prepositions. Brain and Language, 15(2), 249-258.
Genovese, C. R., Lazar, N. A., & Nichols, T. (2002). Thresholding of statistical maps in
functional neuroimaging using the false discovery rate. Neuroimage, 15(4),
870-878.
Glosser, G., Wiener, M., & Kaplan, E. (1986). Communicative gestures in aphasia.
Brain and Language, 27, 345-359.
Goksun, T., Lehet, M., Malykhina, K., & Chatterjee, A. (2013). Naming and gesturing
spatial
relations:
Evidence
from
focal
brain-injured
individuals.
Neuropsychologia, 51, 518-1527.
Goldin-Meadow, S. (2003). Hearing gesture: How our hands help us think. Cambridge:
Harvard University Press.
Goodglass, H., & Kaplan, E. (1963). Disturbance of gesture and pantomime in
aphasia. Brain, 86, 703-720.
Grossman, M., Koenig, P., DeVita, C., Glosser, G., Alsop, D., Detre, J., et al. (2002).
Neural representation of verb meaning: An fMRI study. Human Brain Mapping,
15(2), 124-134.
Hadar, U., Burstein, A., Krauss, R., & Soroker, N. (1998). Ideational gestures and
speech in brain-damaged subjects. Language and Cognitive Processes, 13(1),
107-126.
Hadar, U., & Krauss, R. (1999). Iconic gestures: The grammatical categories of lexical
affiliates. Journal of Neurolinguistics, 12, 1-12.
Hermann, M., Reichle, T., Lucius-Hoene, G., Wallesch, C. W., & Johannsen-Horbach,
H. (1988). Nonverbal communication as a compensative strategy for severely
nonfluent aphasics? A quantitative approach. Brain and Language, 33, 41-54.
Holle, H., Gunter, T. C., Rueschemeyer, S., Hennenlotter, A., & Iacoboni, M. (2008).
Neural correlates of the processing of co-speech gestures. Neuroimage, 39(4),
2010-2024.
Hostetter, A. B., & Alibali, M. W. (2008). Visible embodiment: Gestures as simulated
action. Psychonomic Bulletin & Review, 15, 495-514.
Kable, J. W., Kan, I. P., Wilson, A., Thompson-Schill, S. L., & Chatterjee, A. (2005).
Conceptual representations of action in the lateral temporal cortex. Journal of
Cognitive Neuroscience, 17(12), 1855-1870.
Kable, J., Lease-Spellmeyer, J., & Chatterjee, A. (2002). Neural substrates of action
event knowledge. Journal of Cognitive Neuroscience, 14(5), 795-805.
Kemmerer, D. (2006). The semantics of space: Integrating linguistic typology and
cognitive neuroscience. Neuropsychologia, 44(9), 1607-1621.
Kemmerer, D., Chandrasekaran, B., & Tranel, D. (2007). A case of impaired
verbalization but preserved gesticulation of motion events. Cognitive
Neuropsychology, 24(1), 70-114.
Kemmerer, D., Gonzales Castillo, J. G., Talavage, T., Patterson, S., & Wiley, C. (2008).
Neuroanatomical distribution of five semantic components of verbs: Evidence
from fMRI. Brain and Language, 107(1), 16-43.
Kemmerer, D., & Tranel, D. (2003). A double dissociation between the meanings of
action verbs and locative prepositions. Neurocase, 9(5), 421-435.
Kertesz, A. (1982). Western aphasia battery. San Antonio, TX: Psychological Corp.
Kita, S., & Lausberg, H. (2008). Generation of co-speech gestures based on spatial
imagery from the right-hemisphere: Evidence from split-brain patients. Cortex,
44, 131-139.
Kita, S. (2000). How representational gestures help speaking. In D. McNeill (Ed.),
Language and gesture (pp. 162-185). Cambridge: Cambridge University Press.
Kita, S., & Ozyurek, A. (2003). What does cross-linguistic variation in semantic
coordination of speech and gesture reveal? Evidence for an interface
representation of spatial thinking and speaking. Journal of Memory and
Language, 48(1), 16-32.
Kita, S., Ozyurek, A., Allen, S., Brown, A., Furman, R., & Ishizuka, T. (2007). Relations
between syntactic encoding and co-speech gestures: Implications for a model of
speech and gesture production. Language and Cognitive Processes, 22(8),
1212-1236.
Kosslyn, S., Thompson, W., Gitelman, D., & Alpert, N. (1998). Neural systems that
encode categorical versus coordinate spatial relations: PET investigations.
Psychobiology, 26, 333-347.
Krauss, R. M., Chen, Y., & Gottesman, R. F. (2000). Lexical gestures and lexical access:
A process model. In D. McNeill (Ed.), Language and gesture (pp. 261-283).
Cambridge: Cambridge University Press.
Lanyon, L., & Rose, M. L. (2009). Do the hands have it? The facilitation effects of arm
and hand gesture on word retrieval in aphasia. Aphasiology, 23(7-8),
809-822.
Lausberg, H., Kita, S., Zaidel, E., & Ptito, A. (2003). Split-brain patients neglect left
personal space during right-handed gestures. Neuropsychologia, 41, 1317-1329.
Lausberg, H., Zaidel, E., Cruz, R. F., & Ptito, A. (2007). Speech-independent
production of communicative gestures: Evidence from patients with complete
callosal disconnection. Neuropsychologia, 45, 3092-3104.

T. Goksun et al. / Brain & Language 150 (2015) 1-13
Le May, A., David, R., & Thomas, A. P. (1988). The use of spontaneous gesture by
aphasic patients. Aphasiology, 2, 137-145.
MacSweeney, M., Woll, B., Campbell, R., Calvert, G. A., McGuire, P. K., David, A. S.,
et al. (2002). Neural correlates of British sign language comprehension: Spatial
processing demands of topographic language. Journal of Cognitive Neuroscience,
14(7), 1064-1075.
Marshall, J., Best, W., Cocks, N., et al. (2012). Gesture and naming therapy for people
with severe aphasia: A group study. Journal of Speech, Hearing, and Language
Research, 55, 726-738.
Marstaller, L., & Burianova, H. (2015a). A common functional neural network for
overt production of speech and gesture. Neuroscience, 284, 29-41.
Marstaller, L., & Burianova, H. (2015b). High gamma oscillations in medial temporal
lobe during overt production of speech and gestures. PlosOne, 9(10), e111473.
MATLAB version 7.5.0. (2007). The MathWorks Inc., Natick, Massachusetts.
McNeill, D. (1985). So you think gestures are nonverbal. Psychological Review, 92(3),
350-371.
McNeill, D. (1992). Hand and mind: What gestures reveal about thought. Chicago:
University of Chicago Press.
McNeill, D. (2005). Gesture and thought. Chicago: University of Chicago Press.
McNeill, D., & Pedelty, L. L. (1995). Right brain and gesture. In K. Emmorey & J. Reilly
(Eds.), Language, gesture, and space (pp. 63-85). Hillsdale, NJ: Erlbaum.
Mega, M. S., & Alexander, M. P. (1994). Subcortical aphasia: The core profile of
capsulostriatal infarction. Neurology, 44, 1824-1829.
Noordzij, M. L., Neggers, S. F. W., Ramsey, N. F., & Postma, A. (2008). Neural
correlates of locative prepositions. Neuropsychologia, 46(5), 1576-1580.
Pashek, G. V. (1998). Gestural facilitation of noun and verb retrieval in aphasia: A
case study. Brain and Language, 65, 177-180.
Petrides, M. (2013). Neuroanatomy of language regions of the human brain. Elsevier
Science.
Petrides, M., & Pandya, D. N. (1988). Association fiber pathways to the frontal cortex
from the superior temporal region in the rhesus monkey. J. Comp. Neurol., 273
(1), 52-66.
Petrides, M., & Pandya, D. N. (2009). Distinct parietal and temporal pathways to the
homologues of Broca's area in the monkey. PLoS Biology, 7(8), e1000170. http://
dx.doi.org/10.1371/journal.pbio.1000170.
Raymer, A. M., Singletary, F., Rodriguez, A., Ciampitti, M., Heilman, K. M., & Rothi, L.
J. G. (2006). Gesture training effects for noun and verb retrieval in aphasia.
Journal of International Neuropsychological Society, 12, 867-882.

13

Rodriguez, A., Raymer, A., & Rothi, L. (2006). Effects of gesture plus verbal and
semantic-phonologic treatments for verb retrieval in aphasia. Aphasiology, 20
(2-4), 286-297.
Rose, M., & Douglas, J. (2001). The differential facilitatory effects of gesture and
visualization processes on object naming in aphasia. Aphasiology, 15(10-11),
977-990.
Rose, M., & Douglas, J. (2008). Treating a semantic word production deficit in
aphasia with verbal and gesture methods. Aphasiology, 22(1), 20-41.
Rose, M., Douglas, J., & Matyas, T. (2002). The comparative effectiveness of gesture
and verbal treatments for a specific phonologic naming impairment.
Aphasiology, 16(10-11), 1001-1030.
Skipper, J. I., Goldin-Meadow, S., Nusbaum, H. C., & Small, S. L. (2009). Gestures
orchestrate brain networks for language understanding. Current Biology, 19(8),
260-277.
Talmy, L. (2000). Toward a cognitive semantics: Concept structuring systems (Vol. 1).
Cambridge, MA: MIT Press.
Tesak, J., & Hummer, P. (1994). A note on prepositions in agrammatism. Brain and
Language, 46(3), 463-468.
Tranel, D., & Kemmerer, D. (2004). Neuroanatomical correlates of locative
prepositions. Cognitive Neuropsychology, 21(7), 719-749.
Tranel, D., Kemmerer, D., Adolphs, R., Damasio, H., & Damasio, A. (2003). Neural
correlates of conceptual knowledge for actions. Cognitive Neuropsychology, 20
(3-6), 409-432.
Tranel, D., Manzel, K., Asp, E., & Kemmerer, D. (2008). Naming dynamic and static
actions: Neuropsychological evidence. Journal of Physiology - Paris, 102(1-3),
80-94.
Willems, R. M., & Hagoort, P. (2007). Neural evidence for the interplay between
language, gesture, and action: A review. Brain and Language, 101(3), 278-289.
Willems, R. M., Ozyurek, A., & Hagoort, P. (2007). When language meets action: The
neural integration of gesture and speech. Cerebral Cortex, 17(10), 2322-2333.
Willems, R. M., Ozyurek, A., & Hagoort, P. (2009). Differential roles for left inferior
frontal and superior temporal cortex in multimodal integration of action and
language. Neuroimage, 47(4), 1992-2004.
Wu, D. H., Morganti, A., & Chatterjee, A. (2008). Neural substrates of processing path
and manner information of a moving event. Neuropsychologia, 46(2), 704-713.

