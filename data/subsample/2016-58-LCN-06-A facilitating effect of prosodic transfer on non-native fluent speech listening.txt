Language, Cognition and Neuroscience

ISSN: 2327-3798 (Print) 2327-3801 (Online) Journal homepage: www.tandfonline.com/journals/plcp21

A facilitating effect of prosodic transfer on non-native
fluent speech listening
Tuuli H. Morrill
To cite this article: Tuuli H. Morrill (2016) A facilitating effect of prosodic transfer on nonnative fluent speech listening, Language, Cognition and Neuroscience, 31:6, 801-816, DOI:
10.1080/23273798.2016.1167226
To link to this article: https://doi.org/10.1080/23273798.2016.1167226

Published online: 28 Apr 2016.

Submit your article to this journal

Article views: 316

View related articles

View Crossmark data

Citing articles: 1 View citing articles

Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=plcp21

LANGUAGE, COGNITION AND NEUROSCIENCE, 2016
VOL. 31, NO. 6, 801-816
http://dx.doi.org/10.1080/23273798.2016.1167226

A facilitating effect of prosodic transfer on non-native fluent speech listening
Tuuli H. Morrill
Department of English, Program in Linguistics, George Mason University, Fairfax, VA, USA
ABSTRACT

ARTICLE HISTORY

Native speakers of a language use metrical structure, phonotactics, and phonetic cues in word
segmentation; however, this knowledge can interfere with non-native speech perception and
second language learning. Prosodic transfer effects are typically investigated with isolated words
and phrases, or simple artificial languages; the current study examines how prosody affects
perception and word learning in acoustically complex natural speech. Two experiments test
effects of native language prosody on word identification in prosodically similar and dissimilar
languages; they also test the types of language exposure and learning tasks in which prosodic
transfer may occur. English speakers were exposed to one of two non-native languages (Finnish
or Japanese) in three training conditions. Results indicate that learning can occur thru exposure
to fluent speech in a prosodically similar language and suggest that effects of prosody on fluent
speech perception are an important factor in early stages of second language acquisition.

Received 5 December 2014
Accepted 10 February 2016

Introduction
Fluent speech perception involves the extraction of
meaningful linguistic content from highly complex
acoustic signals. In order to segment a continuous
speech stream, listeners use a variety of cues, many of
which consist of language-specific phonetic, phonotactic, and prosodic information (e.g. Akker & Cutler, 2003;
Cutler & Norris, 1988; Klatt, 1976; Norris, McQueen,
Cutler, & Butterfield, 1997; Vroomen, Tuomainen, & de
Gelder, 1998). Thus, successful speech segmentation
depends on knowledge of the phonological patterns of
the language, and evidence suggests that segmentation
strategies vary according to native language prosodic
structure (Cutler, Mehler, Norris, & Segui, 1986; Norris
et al., 1997; Cutler, Mehler, Otake, & Hatano, 1993). The
segmentation of fluent speech in a second language is
difficult in part because the accurate perception of phonetic information requires a high degree of proficiency -
this compounds the difficulty of perceiving non-native
prosodic structure (e.g. Best & Tyler, 2007; Flege, 2003;
Hisagi, Strange, & Sussman, 2010; Werker & Tees, 1984).
Moreover, the accurate production and perception of
prosody in the second language is often delayed relative
to other aspects of linguistic competence (Archibald,
1993; Sanders & Neville, 2002; White & Mattys, 2007).
Transfer effects, or the application of native language
knowledge and perceptual patterns to the second
language, have been proposed to affect listeners'
ability to accurately perceive word boundary locations
CONTACT Tuuli H. Morrill

tmorrill@gmu.edu

(c) 2016 Informa UK Limited, trading as Taylor & Francis Group

KEYWORDS

Prosody; speech perception;
word segmentation; second
language acquisition; word
learning

(Akker & Cutler, 2003; Dupoux, Peperkamp, & Sebastian-Galles, 2010, 1997; Goetry & Kolinsky, 2000; Tyler &
Cutler, 2009). Accurate word boundary perception is
necessary to identify and acquire new words in the
second language. However, partly due to the nature of
the stimuli in previous studies, consisting of single
words, short sentences, or highly simplified artificial
languages, it is still unclear to what extent prosody
plays a role in word learning during fluent speech listening in a second language. The current study consists of
two experiments testing for native (American English)
language prosodic transfer effects in non-native speech
segmentation, using fluent natural language stimuli in
two prosodically distinct languages, Finnish and
Japanese.

Prosody in segmentation
Evidence for the use of prosodic structure in speech perception comes from studies examining native language
segmentation, as well as studies of word boundary perception in a non-native or artificial language. For
example, Cutler and others (Cutler & Norris, 1988;
Cutler et al., 1986) have proposed that English speakers
employ a "Metrical Segmentation Strategy", incorporating knowledge of the high frequency of stress-initial
words in English causing listeners to associate the presence of a stressed syllable with a word boundary
(Cutler, 1990; Cutler & Carter, 1987). This information

802

T. H. MORRILL

facilitates lexical access (Akker & Cutler, 2003; Cooper,
Cutler, & Wales, 2002; Cutler & Norris, 1988; Cutler &
Van Donselaar, 2001). In addition to prosodic cues at
the word boundary itself, metrical patterning of
context speech can affect segmentation in English
(Dilley & McAuley, 2008; Morrill, Dilley, & McAuley,
2014, Morrill, Dilley, McAuley, & Pitt, 2014).
Cross-linguistic evidence for metrically based segmentation has come from typologically distinct
languages. Speakers of languages which do not
contain lexically defined stress patterns, like Catalan,
Spanish, and French, do not show patterns of stressbased segmentation and instead have been proposed
to utilise syllable structure for segmentation (Dumay,
Frauenfelder, & Content, 2002; Dupoux, Pallier, Sebastian,
& Mehler, 1997; Sebastian-Galles, Dupoux, Segui, &
Mehler, 1992, 1992). Dupoux et al. (2010) have shown
that even in simultaneous fluent French and Spanish
bilingual speakers, about half of the bilinguals had difficulties perceiving the cues of lexical stress in Spanish.
Thus, it appears that speakers do not easily acquire or
maintain multiple segmentation strategies, if at all.

Prosody in acquisition
A role for prosodic information in native language segmentation is found in early language acquisition; even
newborns are sensitive to linguistic rhythm (e.g. Bertoncini, Flocca, Nazzi, & Mehler, 1993; Jusczyk, Friederici,
Wessels, Svenkerud, & Jusczyk, 1993; Mattys, Jusczyk,
Luce, & Morgan, 1999; Mehler, Jusczyk, Lambertz, & Bertoncini, 1988; Turk, Jusczyk, & Gerken, 1995). Examinations of statistical learning in artificial languages, or
simple controlled language stimuli, have also provided
evidence of prosodically based segmentation behaviour.
Infants and adults track distributions of sounds and syllables in speech streams and use this information to
extract possible words (e.g. Hay, Pelucchi, Estes, &
Saffran, 2012; Mirman, Magnuson, Estes, & Dixon, 2008;
Newport & Aslin, 2004; Pelucchi, Hay, & Saffran, 2009;
Saffran, Newport, & Aslin, 1996). However, certain prosodic cues appear to affect word segmentation even in artificial languages. Listeners are better able to extract words
that occur at phrase boundaries, since these larger prosodic breaks by definition signal word boundary presence
(e.g. Endress & Mehler, 2009; Shukla, Nespor, & Mehler,
2007). Intonation contours, final-lengthening and pitch
declination across phrases can all over-ride information
that would be extracted from the transitional probabilities between syllables (Langus, Marchetto, Bion, &
Nespor, 2012; Shukla et al., 2007). More recently, global
phrasal structure from the utterance context has also

been shown to affect artificial language segmentation
(Morrill et al., 2014c).
One open question motivating the current study is
whether prosodic transfer in non-native speech perception could be beneficial for word learning in a prosodically similar second language. If the perception of
metrical patterns is automatically transferred to the
non-native language (e.g. Cutler, Mehler, Norris, &
Segui, 1992), are learners able to make use of prosodic
cues to word boundaries and acquire novel words
while listening to fluent speech? Although some
amount of prosodic transfer has been shown between
highly similar languages (e.g. the interpretation of information status-marking pitch accents in English and
Dutch (Akker & Cutler, 2003)), it is not yet clear that
this benefit would extend to segmentation and novel
word learning. Another open question addressed by
the current study is whether the ability to use stressbased cues in artificial language learning extends to
more naturalistic, highly complex speech stimuli.
Because listeners in general are sensitive to cross-linguistic differences in the phonetic realisation of phonological
information, including the realisation of prosodic cues (e.
g. Best & Tyler, 2007; Flege, 2003; Hisagi et al., 2010;
Werker & Tees, 1984) it cannot be assumed that prosodic
transfer would benefit word learning in exposure to
natural, fluent speech.

Current study
The current study includes two experiments in which
native speakers of American English were exposed to
natural, fluent speech in a non-native language and
then were tested on possible words they learned. Experiment 1 used Finnish language stimuli, and Experiment 2
used Japanese language stimuli. Neither Finnish nor
Japanese has an identical prosodic structure to that of
English, but Finnish is similar to English in a critical
way, with a regular stress pattern of obligatory wordinitial stress (as English has frequent word-initial stress),
whereas Japanese does not exhibit stress but has
lexical pitch accents. The prosodic structures of each
language are discussed further below. To address the
question of whether participants can learn words by listening to fluent speech, both experiments included a
Fluent Speech Listening task, in which participants
were exposed to continuous speech in the non-native
language. To test learning, participants then completed
a Word Identification test. While it is not clear whether
participants can learn novel words from fluent, natural
speech, it is known that a large portion of word learning
in a non-native language comes from exposure to words
in isolation - thus, each experiment also included a Word

LANGUAGE, COGNITION AND NEUROSCIENCE

Learning task which preceded the fluent speech listening, to examine effects of different exposure types.
Learned words then appeared in the fluent speech to
test whether this additional exposure could improve
the recognition of learned words as well as novel words.

Design
The experiments consisted of two learning tasks (the
Word Learning task and the Fluent Speech Listening
task), and a test (the Word Identification test); these are
described in more detail below for each language. In
each experiment, participants were assigned to one of
three language exposure protocols - both experiments
were conducted with Training Condition as a betweensubjects factor. The three Training Conditions were Full
Training, Word Learning Only, and Fluent Speech Only.
The Full Training group completed the Word Learning
task, and then the Fluent Speech Listening task, to
examine whether additional word learning could occur
with exposure to fluent speech. The Word Learning
Only group participated in the Word Learning task, but
was not exposed to fluent speech - this allowed for
direct comparison with the Full Training group. The
Fluent Speech Only group completed the Fluent
Speech Listening task without having done the Word
Learning task, to test whether listeners could learn
novel words from fluent speech exposure alone.
The Word Identification test asked participants to
choose between pairs of items one of which was a
word that had occurred in the Fluent Speech Listening
task (half of which had also been explicitly taught in
the Word Learning task), and one of which was an incorrectly segmented syllable sequence, or nonword. Participants in both experiments were predicted to correctly
identify the words that had been explicitly taught in
the Word Learning task. In addition, participants were
predicted to better recognise the learned words in the
Training Condition in which they completed the Word
Learning task and the Fluent Speech Listening task,
since the fluent speech included the learned words
and the participants had been exposed to the these
words in both tasks. Participants in both experiments
were predicted to better identify novel words that
were repeated often in the Fluent Speech Listening
task (12-15 occurrences) than those that only occurred
a few times (3 occurrences).
If native language prosodic structure affects the perception of non-native speech and the ability to extract
and identify possible words, participants in Experiment
1 (Finnish) should benefit from the word-initial stress
pattern which is common to both English and Finnish.
Thus, participants who were exposed to fluent speech,

803

from which they could segment and extract additional
words based on the stress pattern, should better identify
novel words than participants who were not exposed to
fluent speech. In addition, participants in Experiment 1
should better identify novel words presented with
initial stress than those presented with non-initial
stress, as they match native language prosodic
patterning.
However, participants in Experiment 2 (Japanese),
while exhibiting better recognition of learned words
than of novel words, would not be expected to benefit
from exposure to fluent speech in the identification of
novel words. They would also not be expected to identify
words based on non-native prosodic cues - for Japanese,
these were pitch accents patterns.

Experiment 1: Finnish
Experiment 1 addresses the question of whether learning
novel words can be facilitated by exposure to fluent
speech in a prosodically similar language; in this case,
English listeners were exposed to fluent speech in
Finnish. Finnish primary stress occurs obligatorily on
the first syllable of each word, which is similar to the predominant stress pattern English speakers use in segmentation (Cutler & Norris, 1988). Therefore, it was expected
that participants would perceive the stressed syllables in
Finnish as signalling the beginning of a word and use this
information to learn new words when listening to fluent
speech.
It is worth noting that there are differences between
Finnish and English in phonological structure and in
the phonetic implementation of prosodic structure;
these differences could be detrimental to non-native listeners' abilities to perceive word boundaries. For
example, pitch peak timing differs across the two
languages. In Finnish, pitch peaks associated with prominence occur towards the end of the first mora of the first
syllable; the pitch fall, which is perceptually salient, is
realised mainly on the second mora, whether this is in
the first or second syllable of the word. Suomi, Toivanen,
and Ylitalo (2003) after Lehtonen (1970), speculates that
this pitch peak realisation may cause non-native listeners
to perceive stress as occurring on the second syllable of
some Finnish words. Finnish also exhibits contrastive
phoneme length in both consonants and vowels; for
example, [muuta] "move - imperative", is distinct from
[muta] "mud - nominative", and from [mutta] "but". In
morphologically complex words, long vowels often
occur in the final syllable (e.g. [kalaa] "fish - accusative"
contains a long vowel in the second syllable). Since
English listeners do not have experience with phonemic
vowel length and may expect vowel duration to be a cue

804

T. H. MORRILL

to stress, they could misidentify stressed syllables in
fluent speech. Taking these differences into consideration, it is not entirely clear that listeners would learn
new words through exposure to fluent speech.

Materials and procedure
Word Learning task
In the Word Learning task, participants learned 28 words
of Finnish, known to them as a "foreign language", with a
computer program in E-Prime. The words included 4
monosyllabic words, 18 bi-syllabic words, and 6 tri-syllabic words (Appendix A). Words were selected so that
some contained long vowels or geminate consonants,
as is common in Finnish words. Stimuli were produced
by two female speakers and one male speaker of
Finnish. All recordings were made in a sound-proof
booth with a Marantz PMD-680 Digital Audio Recorder.
Participants heard each word pronounced twice on
presentation of a picture. Each picture was presented
three times, twice with a female voice and once with a
male voice, with pictures and voice pairings in random
order. The first time the picture was presented, text
with the name of the item in English appeared under
the picture, to avoid any ambiguity about which
English lexical item the non-native word represented.
The words that participants learned to associate with
the pictures were not the actual Finnish words for
those specific pictures; the words were chosen based
on structural properties as described below, and the
same pictures were used in Experiments 1 (with Finnish
words) and 2 (with Japanese words). After the presentation of the words, participants completed a word recognition task, in which they heard a word and chose
between two pictures. Participants were given feedback
on each trial, and if participants made more than five
errors, they completed the test again; all participants
reached criterion (82%) before proceeding.

Fluent Speech Listening task
In this task, participants listened to a Finnish narrative
that was approximately 12 minutes long. The narrative
in the Fluent Speech Listening task consisted of a
series of coherent, grammatical sentences, read as a
story. The sentences contained the 28 learned words, in
addition to 28 new words that matched the learned
words in syllable structure. Half of the learned words
and half of the new words were "high repetition"
words (each occurred 12-16 times), and half were "low
repetition" words (each occurred only three times).
Most sentences in the narrative contained one or more

stimulus words, in addition to the contextual non-stimulus words. Each time a given stimulus word occurred,
that word was followed by a word beginning with a
given syllable, so that that co-occurrence of syllables
within the stimulus word was identical to that of the syllables across the following word boundary. For example,
in (1) below, the stimulus word melaa is followed by a
word beginning with the syllable /li/. Therefore, the tobe-learned sequence me-laa occurs as many times as
the nonword laa-li.
(1) Kun joka paiva melaa, lihakset pysyvat hyvassa kunnossa.
when every day paddle - 3d sg., muscles remain-3d pl. good condition
When one paddles everyday, one's muscles remain in good condition.

The narrative was read by a female, native speaker of
Finnish, who had not produced stimuli for the Word
Learning task.
Participants were told that in the narrative, they would
hear the words they had learned. To ensure that participants were alert during this task, the narrative was
divided into 12 sections; after each section, two words
were played and participants responded with a button
press as to whether those words had occurred in the previous section.

Word Identification test
The Word Identification test assessed participants' recognition of the learned words, as well as the acquisition of
novel words that had occurred during the Fluent Speech
Listening task. In the test, participants were presented
with 56 pairs of items, one of which was a word of
Finnish that had occurred in the narrative and one of
which was a nonword (Table 1). The nonwords consisted
of sequences of syllables that co-occurred in the narrative, but spanned a word boundary. They included the
last syllable(s) of a stimulus word, combined with the
first syllable(s) of the word that followed it in the narrative. For example, from the sentence in (1), the test
item pair consisted of the heard word melaa and the
incorrectly segmented laali.
The incorrectly segmented word choices were presented with two stress patterns, non-initial and initial
Table 1. Sample stimuli from the Word Identification test.a
Stimulus word
Nonword syllable sequence
Pair with non-initial stress
uuni
ni-jo
melaa
laa-li
kuuma
ma-il-ma
Pair with initial stress
melaa
laa-li
kuuma
ma-ilma
a
Dashes represent the word boundaries and bold print represents the
stressed syllable. Words are presented in Finnish orthography, which transparently reflects the phonology.

LANGUAGE, COGNITION AND NEUROSCIENCE

stress, counterbalanced across participants. This was done
because Finnish stress is always word-initial; therefore, in
the narrative, the stress pattern heard by listeners for the
nonword syllable sequence was an unstressed syllable followed by a stressed syllable, since the first syllable in the
nonword would be an unstressed final syllable of a heard
word (Table 1, "Pair with non-initial stress"). However, in
the test pairs, the nonword was presented to half of the participants with initial stress so that in these pairs of items, the
heard word and the nonword both exhibited initial stress;
thus, initial stress could not be the only cue to the correct
word choice. This was important since participants were
expected to exhibit a preference for initial stress based
on their native language.
The Word Identification stimuli were produced by the
female speaker who read the narrative for the Fluent
Speech Listening task. She was a native speaker of
Finnish, a fluent second language speaker of English and
Dutch, and a trained actress; she produced the nonwords
with stress on varying syllables without difficulty.
Participants were asked to identify which of the
"words" was more likely to be a word of the language
they were learning. The inter-stimulus interval was
1500 ms, and participants responded by pressing "1" or
"2" on a button box.

Participants
Participants were adult, native speakers of American
English with minimal exposure to other languages and
no prior exposure to Finnish. Participants received
course credit or nominal financial compensation for
their participation. The Full Training, Word Learning
Only, and Fluent Speech Only conditions each contained
40 participants.

Results
Results were analysed in two phases. First, performance
(accuracy) in the Word Identification test for both
Learned and New Words was analysed in one statistical
model. Then, since Learned Words had been explicitly
taught to participants, and New Words had only
occurred in the Fluent Speech Listening task, these two
Word Types were analysed separately, to examine differences in identification of these items.
For the full model, a logit mixed effects model predicting accuracy on the Word Identification test was fitted
with the lmer function in R (Bates, Maechler, & Bolker,
2012, R version 2.15.1) with the following fixed effects
(Table 2): Training Condition (Full Training, Word Learning
Only, Fluent Speech Only), Word Type (Learned vs. New),
Stress Pattern (Initial vs. Non-initial), and Word Repetition

805

Table 2. A mixed effects regression model predicting accuracy
for all words in the Word Identification test in Experiment 1
(Finnish).

(Intercept)
Training Condition Contrasts:
Word Learning Only and Full
Training vs. Fluent Speech
Only
Full Training vs. Word Learning
Word Type (New Words vs.
Learned)
Stress Pattern
Low Repetition Words
Interactions:
Word Learning Conditions vs.
Fluent Speech Only
* Word Type (New Words)
Full Training vs. Word Learning
* Word Type (New Words)
Word Learning Conditions vs.
Fluent Speech Only
* Stress Pattern
Full Training vs. Word Learning
*Stress Pattern

Std.
error
0.09

z-value
9.88

p-value
<2e-16

1.52

0.14

10.52

< 2e-16

0.52
-0.93

0.13
0.12

4.13
-8.02

3.57e-05
1.05e-15

0.50
0.07

0.11
0.11

4.33
0.66

1.48e-05
0.51

-2.00

0.19

-10.60

<2e-16

-0.45

0.16

-2.77

0.00569

0.34

0.19

1.81

0.07

0.32

0.15

2.12

0.03415

Estimate
0.89

Notes: Fixed effects included contrasts for Training Condition, Word Type,
Stress Pattern and Word Repetition as fixed effects, as well as interactions
terms for Training Condition with Word Type and with Stress Pattern. Participants and Items were included as random effects. All p-values in bold
print are significant at the .05 level or less.

(number of occurrences in passage - High vs. Low). Interaction terms were also included between Training Condition and Word Type and Training Condition and Stress
Pattern. Fixed effects were centred and contrast coded.
Training Condition included two contrasts (1) a contrast
between the two conditions in which participants completed the Word Learning task (Full Training and Word
Learning Only) vs. the condition in which they did not
(Fluent Speech Only) and (2) a contrast between the Full
Training condition and the Word Learning Only condition.
Participants and Items were included as random effects
with random intercepts for Participants, and random intercepts and slopes (for Training Condition) for Items.
In the full model, Training Condition contrasts, Word
Type, Stress Pattern, and the interactions between the
Training Condition contrasts and Word Type, all
emerged as significant predictors of performance on
the Word Identification test. The Training Condition contrast between the two conditions in which participants
completed the Word Learning task (Full Training and
Word Learning Only) vs. the Fluent Speech Only condition indicated better performance at the Word Identification test in the word learning conditions, as expected
( = 1.52, z = 10.52, p < .001). The contrast between the
Full Training condition vs. the Word Learning Only condition was also a significant predictor, indicating better
performance in the Full Training condition ( = 0.52,
z = 4.13, p < .001) (Figure 1).

806

T. H. MORRILL

Table 3. A mixed effects regression model predicting accuracy
on Learned Words in Experiment 1 (Finnish).

Figure 1. Boxplot representing medians and quartiles for performance on the Word Identification test for Learned Words
(light grey boxes) and New Words (dark grey boxes) in each
Training Condition in Experiment 1 (Finnish).

The interactions between Training Condition contrasts and Word Type showed significantly worse performance at identifying New words than Learned
words (i.e. more of an effect of "learning") in the word
learning conditions than the Fluent Speech Only condition ( = -2.00, z = -10.6, p < .001) and a greater difference between the New and Learned words in the Full
Training condition than in the Word Learning Only condition ( = -0.45, z = -0.16, p < .01).
Stress Pattern was a significant predictor of accuracy ( = 0.50, z = 4.33, p < .001), with better performance on the Word Identification test when the
incorrect choice had an "incorrect" or non-initial
stress pattern. The Training Condition contrast
between Full Training and Word Learning showed a
significant interaction with Stress Pattern, with a
greater effect of Stress Pattern in the Full Training condition ( = 0.32, z = 2.12, p < .05). Word Repetition was
not a significant predictor of accuracy on word identification ( = 0.07, z = 0.66, p = .51).
To more closely examine differences in the factors
affecting accuracy at identifying New Words and
Learned Words, separate models were tested for each
word type. For Learned Words, a logit mixed effects
model predicting accuracy on the Word Identification
test was fitted with the same structure as the full
model, but without Word Type (Learned vs. New)
(Table 3). Results showed that the only significant predictors of performance on Learned Words were the
Training Condition contrasts, where as expected, the
conditions in which participants completed the Word
Learning task lead to higher accuracy at identifying
Learned Words than did the Fluent Speech Only condition ( = 2.74, z = 11.43, p < .001). The contrast
between Full Training and Word Learning Only also
showed higher accuracy for the Full Training condition
( = 0.75, z = 3.26, p < .01).

(Intercept)
Training Condition Contrasts:
Word Learning Only and Full
Training vs. Fluent Speech
Only
Full Training vs. Word Learning
Stress Pattern
Low Repetition Words
Interactions:
Word Learning Conditions vs.
Fluent Speech Only
* Stress Pattern
Full Training vs. Word Learning
* Stress Pattern

Std.
error
0.14

zvalue
10.64

p-value
<2e-16

2.74

0.24

11.43

<2e-16

0.75
0.24
-0.06

0.23
0.18
0.17

3.26
1.35
-0.36

0.00111
0.18
0.72

0.21

0.26

0.81

0.42

0.32

0.28

1.16

0.24

Estimate
1.53

Notes: Fixed effects included contrasts for Training Condition, Stress Pattern
and Word Repetition as fixed effects, as well as interactions terms for Training Condition with Stress Pattern. Participants and Items were included as
random effects. All p-values in bold print are significant at the .05 level or
less.

For New Words, a logit mixed effects model predicting
accuracy on the Word Identification test was fitted with
the same structure as for Learned Words (Table 4).
Results showed that for accuracy at identifying new
words, the Training condition contrasts and Stress
Pattern were both significant predictors. The conditions
in which participants completed the Word Learning
task lead to higher accuracy at identifying Learned
Words than did the Fluent Speech Only condition ( =
0.50, z = 3.43, p < .001), and the contrast between Full
Training and Word Learning Only also showed higher
accuracy for the Full Training condition ( = 0.29, z =
2.80, p < .01). The effect of Stress Pattern indicates that
participants were better at choosing the correct word
in the Word Identification test when the incorrect word
Table 4. A mixed effects regression model for predicting
accuracy on New Words in Experiment 1 (Finnish).

(Intercept)
Training Condition Contrasts:
Word Learning Only and Full
Training
vs. Fluent Speech Only
Full Training vs. Word
Learning
Stress Pattern
Low Repetition Words
Interactions:
Word Learning Conditions vs.
Fluent Speech Only
* Stress Pattern
Full Training vs. Word
Learning
* Stress Pattern

Estimate
0.36

Std.
error
0.11

zvalue
3.4

p-value
0.000670

0.50
0.29

0.15
0.10

3.43
2.80

0.000611
0.005049

0.70
0.18

0.15
0.15

4.74
1.21

2.19e-06
0.23

0.37

0.27

1.37

0.17

0.31

0.19

1.67

0.09

Notes: Fixed effects included contrasts for Training Condition, Stress Pattern
and Word Repetition as fixed effects, as well as interactions terms for Training Condition with Stress Pattern. Participants and Items were included as
random effects. All p-values in bold print are significant at the .05 level or
less.

LANGUAGE, COGNITION AND NEUROSCIENCE

Figure 2. Boxplot representing medians and quartiles for performance on the Word Identification test for New Words when
the incorrect word choice had Initial or Non-Initial (incorrect)
stress, in each Training Condition in Experiment 1 (Finnish).

had a non-initial stress pattern ( = 0.70, z = 4.74, p
< .001). For example, participants were more likely to
choose the correctly segmented uuni with initial stress
when the incorrect choice was nijo with non-initial
stress, than when both choices exhibited initial stress,
as in uuni and nijo. In other words, participants exhibit
a preference for initial-stress words; when both choices
exhibit initial stress, this preference cannot aid participants in identifying the word. Here, we see that this
effect emerges in accuracy for New Words, whereas
there was no effect of Stress Pattern in the model for
Learned Words. Figure 2 illustrates that the preference
for initial-stress words was present for all Training Conditions - when the nonword choice exhibits non-initial
stress, participants' performance is better than when
both the nonword and real word choices exhibit initial
stress. The effect appears strongest in the Full Training
condition - the interaction between Stress Pattern and
the contrast between Full Training and Word Learning
indicates a possible trend in this direction ( = 0.31, z =
1.67, p = .09).

Discussion
For overall accuracy on the Word Identification test,
Training Condition was highly predictive of performance,
with the best performance in the Full Training condition,
in which participants had completed both the Word
Learning task and the Fluent Speech Listening task. Performance was significantly higher in the Full Training
condition than in the Word Learning Only condition, in
which participants performed better than those in the
Fluent Speech Only condition. When Learned Words
and New Words were analysed separately, it became
clear that for Learned Words, the only factor affecting
performance was Training condition. In other words, if
participants had completed the Word Learning task,

807

they were more accurate at identifying Learned Words
(those presented in the task) than if they had not - this
pattern was expected. Interestingly, performance on
Learned Words was even higher for participants in the
Full Training condition, who had learned the words and
then listened to fluent speech. This improved accuracy
is most likely due to the additional exposure to the
learned words, which occurred in the fluent speech
passage.
The analysis of accuracy identifying New Words
(which had not been taught explicitly and only occurred
in the Fluent Speech Listening task) addresses the question of whether prosodic structure affects novel word
learning in an unknown language; the significant effect
of Stress Pattern in the analysis of New Words indicates
that it does. A preference for initial stress appears to
have been highly predictive of participants' decisions
about words of Finnish. This is most likely an effect of
transfer from their native language - in English, the
most common stress pattern is word-initial, and in
Finnish, stress is always word-initial, so a transfer of the
expected stress pattern from the native language
would help participants identify the Finnish words. The
trend showing the largest effect of Stress Pattern in the
Full Training condition (Figure 2) suggests that the tendency to choose words when they have initial stress is
strongest in the Full Training group. Listening to fluent
speech could have confirmed initial native language
biases towards a stress-initial prosodic pattern and influenced participants' word choices. Overall, participants in
the Full Training condition exhibited the highest accuracy in identifying new words (regardless of stress
pattern). Thus, these results support the hypothesis
that prosodic transfer can affect the initial stages of
second language learning, and also suggest that
exposure to fluent speech, in addition to word learning,
can facilitate novel word acquisition in a prosodically
similar language.
The lack of an effect of Word Repetition (i.e. the
number of times a word occurred in the fluent speech
narrative) may have been due to the overall length of
the narrative and the relatively low number of occurrences of any single item. Even the high repetition
words occurred only 12-16 times in the approximately
12 minutes of the narrative.

Experiment 2: Japanese
Experiment 1 demonstrated that when listeners are
exposed to a non-native language that has a similar prosodic structure to their native language, they may transfer a prosodically based word identification strategy.
Experiment 2 examines an additional aspect of a

808

T. H. MORRILL

prosodic transfer hypothesis - testing whether listeners
exposed to a prosodically dissimilar non-native language
exhibit patterns of native language stress-based segmentation and fail to benefit from exposure to fluent
speech in learning.
The prosodic system of Japanese differs from that of
English, and Finnish, in several ways. Japanese is considered to be a pitch accent language; instead of exhibiting lexical stress, words can be distinguished by whether
or not they bear a pitch accent, which takes the form of a
high tone late on the accented mora, followed by a fall to
a low tone on the next mora (e.g. Venditti, 2005). The
location of the accent within a word can vary, leading
to distinct pitch patterns. Several pairs of bisyllabic
lexical items contrast directly in their pitch accent patterns (e.g. hashi, with a high-low (HL) tone pattern for
"chopsticks" and hashi with a low-high (LH) tone
pattern for "bridge"). At the level above the word, Japanese includes a clearly demarcated Accentual Phrase,
with a low boundary tone at the beginning of the
phrase, and a rise on the second mora (Pierrehumbert
& Beckman, 1988). Whereas in English, phrase-final
rises only occur in specific contexts, a phrase-final
rising pattern is quite common in Japanese. Thus, if
English listeners perceive high-pitched syllables as
accented and as stressed syllables, and hypothesise
those syllables to be word-initial, they may make errors
in the segmentation of fluent speech in Japanese.
Native Japanese speakers have been shown to
segment speech based on moraic structure (e.g. Otake,
Takashi, Cutler, & Mehler, 1993). Pitch accent patterns
can constrain lexical access, used by listeners to eliminate word candidates which do not bear the perceived
pitch pattern (Cutler & Otake, 1999). In addition, intonational rises at the beginnings of Accentual Phrases may
be used by native speakers as a cue to word boundaries
(Warner, Otake, & Arai, 2010). However, as with the patterns of lexical pitch accents, these cues to word boundaries for native speakers of Japanese would not
necessarily be expected to aid non-native listeners in
segmentation.

Design
The experiment design was identical to that of Experiment 1, with Training Condition as a between-subjects
factor. Minor differences from the procedures and
materials from Experiment 1 are described below.
These differences pertain mainly to the Fluent Speech
Listening task, for which two versions were presented
in Experiment 2. Because Experiment 1 did not show
an effect of word repetition, with one possible reason
for this being the relatively long duration of the fluent

speech narrative, the narrative in Experiment 2 was shortened so that the high frequency words would occur in
closer temporal proximity. The first version of the
Fluent Speech Listening task presented the shortened
(7 minute) narrative. To address possible concerns
that a hypothesised decrease in learning for the prosodically distinct language could then be caused by the
shorter exposure to fluent speech, a second version of
the Fluent Speech Listening task was presented to half
of the participants. In this version, the narrative was
played twice, resulting in an exposure to fluent speech
similar in length to that of Experiment 1 (14 minutes
in Experiment 2 and 12 minutes in Experiment 1).
However, no effect of exposure duration was found in
model comparisons with and without the factor (AIC
scores of model fit were 3280.3 and 3280.2 respectively,
 2 = 1.87, p = .17), and all participants' performance in the
Word Identification test in Experiment 2 is subsequently
analysed in one statistical model (presented and discussed in the Results section).

Materials and procedure
Word Learning task
As a result of the narrative shortening in the Fluent
Speech Listening task, the number of stimulus words
was also reduced. Participants learned 12 words of Japanese. All words were bisyllabic (Appendix B). Half of the
words contained the HL accent pattern, and half contained the LH accent pattern. All stimuli were produced
by native speakers of the Tokyo dialect of Japanese.
The procedure for the task was identical to Experiment
1, and participants had to reach criterion (91% - a
maximum of one word incorrect) on the word recognition task.

Fluent Speech Listening task
The procedure for this task was identical to Experiment 1,
except that the total duration of the narrative was
approximately 7 minutes (divided into 8 instead of 12
sections). As mentioned above, for half of the participants, the narrative was played twice and the task
lasted approximately 14 minutes.

Word Identification test
The procedure was identical to Experiment 1, but with 24
pairs of items in which one was a heard word from the
narrative and one was a nonword (Table 5). Items were
constructed as in Experiment 1.

LANGUAGE, COGNITION AND NEUROSCIENCE

Table 5. Sample stimuli from the word identification test in
Experiment 2.
Stimulus word
doro
asa
ashi

Nonword syllable sequence
ro-ga
sa-no
shi-o

Because the Japanese word choices contained either
a HL or LH accent pattern, each nonword was presented
with both accent patterns, counterbalanced across participants. As with the stress patterns in Experiment 1,
this manipulation was conducted to test for possible
effects of specific prosodic patterns on word identification accuracy.
The Word Identification stimuli were produced by the
speaker who read the narrative. The speaker was a native
speaker of Japanese, fluent in English, and a trained phonetician. She was able to produce the nonwords with
accent placement on varying syllables without difficulty.

Participants
Participants were adult, native speakers of American
English with minimal exposure to other languages and
no prior exposure to Japanese. Participants in each condition (Full Training n = 41, Word Learning Only n = 40,
and Fluent Speech Only n = 41) received course credit
or nominal financial compensation for their participation.

Results
As in Experiment 1, the results were analysed first with
accuracy in the Word Identification test for both
Learned and New Words together, and then with the
two types of items separately. A logit mixed effects
model was fitted with the lmer function in R (Bates
et al., 2012, R version 2.15.1) with the following fixed
effects: two Training Condition contrasts (as in Experiment 1) - (1) the word learning conditions (Full Training
and Word Learning Only) vs. Fluent Speech Only and (2)
Full Training vs. Word Learning Only), Word Type
(Learned vs. New), Accent Type in the word (LH or HL),
Accent Type in the nonword (LH or HL), and Word Repetition (High vs. Low). An interaction term was included
for Training Condition and Word Type. Participants and
Items were included as random effects with random
intercepts.
Several factors emerged as significant predictors of
performance on the Word Identification test in Experiment 2 (Table 6). As expected, participants were
overall less accurate at identifying New Words than
Learned words ( = -.79, z = 5.24, p < .001). Results for
the Training Condition contrasts indicated that

809

Table 6. A mixed effects regression model predicting accuracy
for all words for Experiment 2 (Japanese) with Training
Condition contrasts, Word Type, Accent Type in the nonword,
Accent Type in the word, Word Repetition, and an interaction
terms for Training Condition and Word Type.

(Intercept)
Training Condition Contrasts:
Word Learning Only and Full
Training vs. Fluent Speech
Only
Full Training vs. Word
Learning
Word Type (New Words vs.
Learned)
Accent - Nonword
Accent - Word
Low Repetition Words
Interactions:
Word Learning Conditions vs.
Fluent Speech Only
* Word Type (New Words)
Full Training vs. Word
Learning
* Word Type (New Words)

Std.
error
0.16

zvalue
4.71

p-value
2.51e-06

1.59

0.20

8.16

3.29e-16

-0.26

0.18

-1.43

0.15

-0.79

0.15

-5.24

1.64e-07

-0.01
0.35
0.33

0.15
0.15
0.15

-0.04
2.32
2.18

0.97
0.0202
0.0291

-1.21

0.24

-4.94

7.87e-07

0.23

0.24

0.95

Estimate
0.75

0.3428

Note: Subjects and items were included as random effects. All p-values in bold
print are significant at the .05 level or less.

participants in the word learning conditions (Full Training and Word Learning only) exhibited better performance than those in the Fluent Speech Only condition
( = 1.59, z = 8.16, p < .001); however, unlike in Experiment 1, performance in the Full Training condition was
not better than in the Word Learning Only condition
( = -0.26, z = -1.43, p = .15) (Figure 3). There was an
interaction between the Training Condition contrast of
word learning conditions vs. Fluent Speech Only and
Word Type, indicating that participants were significantly
better at identifying the Learned words than the New
words in the word learning conditions ( = -1.21, z =
-4.94, p < .001). As with Experiment 1, this pattern was

Figure 3. Boxplot representing medians and quartiles for performance on the Word Identification test for Learned Words
(light grey boxes) and New Words (dark grey boxes) in each
Training Condition in Experiment 2 (Japanese).

810

T. H. MORRILL

Table 7. A mixed effects regression model for predicting
accuracy on Learned Words in Experiment 2 (Japanese) with
Training Condition contrasts, Accent Type in the nonword,
Accent Type in the word, and Word Repetition.

(Intercept)
Training Condition Contrasts:
Word Learning Only and Full
Training vs. Fluent Speech
Only
Full Training vs. Word Learning
Accent - Nonword
Accent - Word
Repetition - Low

Std.
error
0.24

zvalue
5.89

2.32

0.29

8.09

-0.37
-0.12
0.35
0.10

0.28
0.22
0.22
0.22

-1.36
-0.53
1.60
0.46

Estimate
1.39

p-value
3.90e09
9.87e16
0.18
0.60
0.11
0.65

Note: Subjects and items were included as random effects. All p-values in bold
print are significant at the .05 level or less.

expected since in these two conditions, the Learned
Words were explicitly taught.
An effect of Accent Type in the word choice showed
that participants were slightly better at identifying
words when those words exhibited the LH accent
pattern ( = 0.35, z = 2.32, p < .05). There was also an
effect of Repetition, indicating that participants better
identified the low repetition words ( = 0.33, z = 2.18, p
< .05). These findings are considered below.
Next, separate logit mixed effects models were fitted
for the Learned Words and New Words. A model for
Learned Words was fitted with Training Condition,
Accent Type in the word choice, Accent Type in the
nonword, and Repetition (Table 7). Subjects and Items
were included as random effects with random intercepts.
The results of the model for Learned Words demonstrate that Training Condition was, as expected, a predictor of accuracy in word identification - the two
conditions in which participants learned words (Full
Training and Word Learning Only) showed better performance than the Fluent Speech Only condition ( =
2.32, z = 8.09, p < .001). However, there was no difference

Table 8. A mixed effects regression model for predicting
accuracy on New Words in Experiment 2 (Japanese) with
Training Condition contrasts, Accent Type in the nonword,
Accent Type in the word, and Repetition.

(Intercept)
Training Condition Contrasts:
Word Learning Only and Full
Training vs. Fluent Speech
Only
Full Training vs. Word Learning
Accent - Nonword
Accent - Word
Repetition - Low

Estimate
0.19

Std.
error
0.19

zvalue
0.99

0.96

0.20

4.88

1.08e06

-0.14
0.08
0.34
0.52

0.18
0.19
0.19
0.19

-0.79
0.44
1.80
2.70

0.43
0.66
0.07
0.00694

p-value
0.32

Note: Subjects and items were included as random effects. All p-values in bold
print are significant at the .05 level or less.

Table 9. Mean accuracy by Training Condition for each New
Word presented in the Word Identification test, by Repetition
and Accent Type.
Item
asa
asa
asa
kumo
kumo
kumo
neko
neko
neko
eki
eki
eki
matsu
matsu
matsu
nasu
nasu
nasu
iro
iro
iro
machi
machi
machi
yume
yume
yume
ashi
ashi
ashi
natsu
natsu
natsu
yuki
yuki
yuki

Condition
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining
FluentSpeechOnly
WordLearningOnly
FullTraining

Repetition
high
high
high
high
high
high
high
high
high
low
low
low
low
low
low
low
low
low
high
high
high
high
high
high
high
high
high
low
low
low
low
low
low
low
low
low

Correct
accent
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
HL
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH
LH

Accuracy (%)
39.02
75.00
51.22
63.41
62.50
73.17
46.34
37.50
41.46
56.10
75.00
80.49
56.10
65.00
73.17
53.66
72.50
75.61
78.05
77.50
82.93
36.59
70.00
26.83
41.46
85.00
73.17
56.10
77.50
82.93
60.98
77.50
78.05
63.41
75.00
80.49

between the Full Training and Word Learning conditions
( = -0.37, z = -1.36, p = .18), and the Accent Type
factors and Repetition were not predictors of accuracy
for Learned Words.
A mixed effects regression model for predicting accuracy on New Words in Experiment 2 (Japanese) (Table 8)
was fitted with the same structure as for Learned Words.
Again, the word learning conditions predicted higher
accuracy than the Fluent Speech Only condition ( =
0.96 z = 4.88, p < .001), but there was no significant difference between Full Training and Word Learning Only.
Two other patterns emerged for identification of New
Words. First, word Repetition emerged as a significant
predictor of accuracy, but with higher accuracy for low
repetition words ( = 0.52 z = 2.70, p < .01). Examination
of accuracy across items in the New Words (Table 9)
shows an overall high degree of variability across conditions for the low repetition words; thus, a few atypical
items may be driving this effect and it should not be
over-interpreted. A trend towards better performance
for the Low-High Accent Type in the word choice

LANGUAGE, COGNITION AND NEUROSCIENCE

( = 0.34 z = 1.80, p = .07) suggests that the effect of
Accent Type found in the model for all words (Table 6)
may be driven by specific items that were New Words.
These factors are considered below.

Discussion
The results of Experiment 2 indicate that Training Condition was a significant predictor of performance on the
Word Identification test; overall, participants in the conditions which included word learning performed better
than those in the Fluent Speech Only condition.
However, the Full Training condition did not lead to
higher accuracy for either Learned or New Words,
and as can be seen in Figure 3, accuracy appears
slightly higher in the Word Learning Only condition
than in the Full Training condition (although these
differences were not significant). The lack of increased
accuracy for the Full Training over the Word Learning
Only conditions indicates that hearing fluent speech
did not help listeners to better identify words in the
non-native language. These results support the view
that prosodic transfer in this case did not help listeners
identify words of Japanese.
An overall effect of Accent Type and the trend
towards this effect specifically for New Words indicates
that participants were slightly better at identifying
words when they contained a LH accent pattern than
a HL pattern. If participants showed a preference for
the LH accent pattern, one reason for this preference
could be a familiarity with the LH intonation contour
in English list intonation. In addition, the LH accent in
Japanese is the default accent pattern, and occurs
with greater frequency than HL (including at initial
phrase boundaries); this could have affected participants' preferences. Interestingly, a prosodic transfer
effect from English may have been expected to work
in the opposite direction; a preference for words with
initial stress (possibly realised with a high accent on
the first syllable) could manifest as a preference for
the HL accent pattern. This could have resulted in
higher accuracy when the words were HL, but this
pattern was not observed.
It is not clear why an effect emerged for higher accuracy on low repetition words. Most likely, this effect was
driven by a few items in the New Words. A few of the
high repetition words were followed by the syllables no
and wa (grammatical particles in Japanese, which
occurred frequently in the fluent speech passage).
Thus, the nonwords containing these specific syllables
may have been preferred over the words in those
specific test pairs (e.g. the word "neko" vs. nonword
"ko-wa"); this would lead to relatively lower accuracy

811

for high repetition words and higher accuracy for low
repetition words. However, the current data set does
not allow for further examination of this uncontrolled
factor.
Overall, the results of Experiment 2 suggest that while
native English listeners were able to acquire words of
Japanese and correctly identify both learned and new
words in the Word Identification test, listening to fluent
speech did not aid in acquisition. Unlike in Experiment
1, listeners in the Full Training condition did not
perform better than those in the Word Learning Only
condition; here, the reason for the overall high accuracy
in the Word Learning Only condition is not clear.
However, participants may have acquired information
about possible phonological patterns of Japanese
words through the Word Learning task - recent research
has demonstrated that even limited amounts of
exposure to second language words can result in generalised learning (e.g. Daland & Pierrehumbert, 2011;
Endress & Mehler, 2009).
It is worth noting the high degree of variability across
subjects in all of the Training Conditions in Experiment
2. One potential source of variability not measured in
the current study (and which may not be possible to
accurately measure) is casual exposure to the nonnative language, Japanese. Many native English speakers
may have encountered Japanese words or phrases in
popular media, which could have influenced their
hypotheses about possible words of Japanese.
However, despite the variability across items in these
results, the overall pattern of results suggests there was
still no beneficial effect of fluent speech listening for
novel word learning in Japanese.

General discussion
The results of Experiment 1 support the hypothesis that
transfer between a prosodically similar native and nonnative language can have beneficial effects on language
acquisition. Listeners' identification of novel words was
improved by listening to fluent speech. The results of
Experiment 2 suggest that this advantage is not
observed when listeners are exposed to a prosodically
dissimilar language. There are three main findings from
the current study: (1) Listeners exhibit effects of prosodic
transfer when listening to natural speech, and this affects
word learning, (2) When exposed to a prosodically similar
language, a combination of word learning in isolation
and fluent speech listening may result in the most learning, and (3) Listeners may be sensitive to the prosodic
structure of words in a non-native language, even
when the phonetic correlates of prosody are not identical to those of their native language. Thus, the current

812

T. H. MORRILL

study contributes to our understanding of the role of
prosody in learning novel words in an unknown
language - prosodic structure appears to affect learning
even in the naturalistic environment of a highly complex
and variable speech signal.

Prosodic transfer
The native English-speaking participants in the current
study appear to have used a stress-based segmentation
strategy when listening to Finnish (Experiment 1); in a
language with similar stress patterns to English, this
would have allowed them to successfully extract words
from fluent speech, thus improving learning. Experiment
1 suggests that a bias towards identifying words with
initial stress as new words of the non-native language
may exist even for listeners who are not explicitly
taught any words; then, additional exposure confirms
or strengthens the use of this strategy (Figure 2). On
the other hand, Experiment 2 suggests that applying a
stress-based word identification strategy to a prosodically distinct language (Japanese) would not be effective.
For example, listeners could perceive high-accented syllables as stressed and therefore as signalling a word
boundary - a non-optimal strategy since high accents
do not have to occur word - initially in Japanese. In
the current study, although participants did not exhibit
a preference for HL accent patterns in the Word Identification test, applying this strategy could still have affected
fluent speech listening. This ineffective segmentation of
the narrative would be unlikely to facilitate the acquisition of novel words, and we see no added benefit of
fluent speech listening in word identification in Experiment 2. There is a possibility that the limited number
of words participants were explicitly taught in Experiment 2 (about half as many as in Experiment 1) could
have affected their ability to learn additional words in
fluent speech exposure. While this possibility cannot be
eliminated in the current study, the relatively high performance for both the Full Training and the Word Learning only conditions in Experiment 2 (where performance
in the Word Learning appears slightly higher than Full
Training) suggests that fluent speech exposure would
be unlikely to aid in novel word learning in a prosodically
distinct language.

Word segmentation and learning
The current results support findings from previous work
that used artificial language learning paradigms,
suggesting that known words act as anchors which learners can use to extract other units from the speech
stream (Cunillera, Laine, Camara, & Rodriguez-Fornells

2010; Dahan & Brent, 1999). When listeners hear a
known word, they are able to detect the word-final
boundary (i.e. and the following word-initial boundary),
as well as hypothesise boundaries for preceding words.
Thus, the number of correctly segmented word forms
increases and judgments about additional possible
words improve. In Experiment 1 of the current study,
this type of effect could have contributed to learners'
ability to correctly identify words that they had not
been explicitly taught. This suggests that even when
known word anchors occur relatively infrequently in a
natural language (in this case, about once per sentence),
this kind of boot-strapping could be beneficial for learning novel words.
Extraction of words adjacent to known word boundaries does not appear to be the only factor accounting
for learning in Experiment 1 (Finnish), as this strategy
was also available to listeners in Experiment 2 (Japanese).
The learning exhibited by participants in the Full Training
condition of Experiment 1 might therefore be the result
of a combination of learning words in isolation, and the
using a stress-based segmentation strategy in fluent
speech listening. Transfer of a stress-based segmentation
strategy from English to Finnish could initially have been
automatic, with subsequent exposure to known and new
words in fluent speech then confirming hypotheses
about the stress patterns of the non-native language.
The effect of stress pattern on word identification indicated that participants were better at identifying new
words when the nonword choice exhibited an incorrect
(non-initial) stress pattern. Crucially, the effect of stress
pattern interacted with training condition, with a
greater preference for initial stress in the Full Training
condition than in the Word Learning Only condition. In
other words, an expectation for word-initial stress did
not necessarily affect participants' word identification
behaviour unless they had also been exposed to fluent
speech. The findings of the current study suggest a possible benefit of combining word learning with fluent
speech listening, as a way to allow for an accumulation
of evidence for phonological patterns and the learning
of novel words in the non-native language.

Prominence perception
The results of the current study also suggest that listeners display flexibility in the perception of prominence
across languages. In Experiment 1, listeners exhibited
behaviour consistent with having perceived word-initial
syllables as stressed - this occurred despite differences
in the phonetic realisation of stress cues in Finnish and
English (Suomi, 2007; Suomi et al., 2003; Suomi &
Ylitalo, 2004). For example, numerous Finnish words in

LANGUAGE, COGNITION AND NEUROSCIENCE

the fluent speech narrative contained phonemically long
vowels on non-initial syllables. However, the results of
Experiment 1 suggest that listeners perceived evidence
of word-initial stress in enough instances to use this
information for word identification. The ability or tendency to utilise prosodic cues even when they do not
correspond directly to those of the native language has
also been suggested by Langus et al. (2012), who
showed that prosodic cues such as pitch declination
and final-lengthening were used by listeners in an artificial learning paradigm. In Experiment 2, where listeners
did not exhibit evidence of learning from exposure to
fluent speech, perceived prominence patterns may
have contributed to mis-segmentation; if non-native listeners perceived the pitch accent as stress and therefore
as signalling a word-initial syllable, they would not correctly segment the fluent speech. Thus, speech segmentation and subsequent word identification may have
been negatively affected by a tendency to perceive the
non-native pitch pattern as signalling prominence,
even though it did not correspond directly to the form
of an American English pitch accent.

Attention and word learning
The pattern of results across the two experiments may
also be considered with respect to the role of attention
in perception and learning. Task and input conditions
in non-native speech segmentation, in particular, have
been shown to affect whether listeners use prosodic
information, distributional information, or both in nonnative segmentation. For example, Toro, Sinnett, and
Soto-Faraco (2005) found that participants' statistical
learning abilities were detrimentally affected when
attention was directed towards unrelated information
within the speech stream, in the form of altered pitch
on certain syllables. In Experiment 2 of the current
study, unexpected or unfamiliar pitch patterning may
have functioned as a kind of distractor, so that even in
the Full Training condition, listeners did not benefit
from the perception of learned words occurring in the
fluent speech stream or any possible boot-trapping
based on these "anchor" words (Cunillera et al., 2010;
Dahan & Brent, 1999).
Recent investigations of the neural correlates of
speech segmentation and language learning have
revealed differences in selective attention mechanisms
in successful vs. unsuccessful second language learners
(Batterink & Neville, 2014); if learning is dependent on
selective attention, then attention directed towards the
appropriate cues to prosodic boundaries could be a
necessary prerequisite to learning. Temporally selective
attention found in native language perception provides

813

evidence of improved processing of sounds such as syllable onsets at time points that are expected or predictable to the listener (Astheimer & Sanders, 2009, 2011);
native language prosodic structure may contribute to
this predictability and elicit the corresponding correlates
of neural activity (Breen, Dilley, McAuley, & Sanders,
2014). The results of the current study suggest that selective attention mechanisms may influence the use of prosodic and distributional information in non-native speech
perception and have effects on learning. Prosodic patterning that matches expectations has recently been
shown to enhance learning in artificial language experiments (Morrill et al., 2014c; Shukla et al., 2007), and it is
likely that the neural indices of selective attention
would be found in online examinations of non-native segmentation, as well. The results of the current study
suggest that selective attention may influence perception
in real-world listening environments and have effects on
the processing of natural non-native speech input.

Conclusion
The results of these experiments suggest that natural
language perception and acquisition can be affected
by prosodic structure - specifically, transfer from native
language prosody to an unknown language. These
effects may be beneficial for novel word learning when
listeners are exposed to a prosodically similar language,
but it appears that this advantage may not exist when listeners are exposed to a prosodically dissimilar language.
The effects of prosodic structure on non-native speech
perception seem to be present even for relatively naive
adult listeners in their initial stages of exposure to the
non-native language, and these effects may occur even
in highly complex natural speech with distinct phonetic
implementations of prosodic cues. The finding that a
combination of word learning and fluent speech
exposure may facilitate the identification of novel word
forms supports the idea that known words act as
anchors for attention in the speech stream. These findings suggest directions for future research in examining
the specific ways in which non-native rhythmic cues
affect attention and perception in non-native languages.

Acknowledgements
I would like to thank Bob McMurray and two anonymous
reviewers for their thoughtful feedback and comments, which
greatly improved the manuscript. I thank Lisa Davidson, Maria
Gouskova, Alec Marantz, Adamantios Gafos, Winifred Strange,
Kathleen Currie Hall, Devin McAuley, and Laura Dilley for guidance and input at various stages of this project, and audiences
at the Acoustical Society of America and Speech Prosody meetings for comments on portions of this work. I am grateful to the

814

T. H. MORRILL

speakers who recorded stimuli, especially the narrative readers,
Johanna Telander and Mieko Sperbeck. Thanks also to Trisha
Zdziarska and Karli Nave for assistance with data collection.
This work was partially supported by the New York University
Department of Linguistics and the Michigan State University
Department of Psychology.

Disclosure statement
No potential conflict of interest was reported by the author.

References
Akker, E., & Cutler, A. (2003). Prosodic cues to semantic structure
in native and nonnative listening. Bilingualism: Language and
Cognition, 6, 81-96. doi:10.1017/S1366728903001056
Archibald, J. (1993). Language learnability and L2 phonology: The
acquisition of metrical parameters. Dordrecht: Kluwer
Academic.
Astheimer, L. B., & Sanders, L. D. (2009). Listeners modulate temporally selective attention during natural speech processing.
Biological Psychology, 80, 23-34. doi:10.1016/j.biopsycho.
2008.01.015
Astheimer, L. B., & Sanders, L. D. (2011). Predictability affects
early perceptual processing of word onsets in continuous
speech. Neuropsychologia, 49, 3512-3516. doi:10.1016/j.
neuropsychologia.2011.08.014
Bates, D., Maechler, M., & Bolker, B. (2012). lme4: Linear mixedeffects models using S4 classes. R package version 0.999999-0.
Batterink, L., & Neville, H. J. (2014). ERPs recorded during early
second language exposure predict syntactic learning.
Journal of Cognitive Neuroscience, 26, 2005-2020. doi:10.
1162/jocn_a_00618
Bertoncini, J., Flocca, C., Nazzi, T., & Mehler, J. (1993). Morae and
syllables: Rhythmical basis of speech representations in neonates. Language and Speech. doi:10.1177/002383099503800
401
Best, C. T., & Tyler, M. D. (2007). Nonnative and second-language
speech perception: Commonalities and complementarities.
Language Experience in Second Language Speech Learning:
In Honor of James Emil Flege, 13-34.
Breen, M., Dilley, L. C., McAuley, J. D., & Sanders, L. D. (2014).
Auditory evoked potentials reveal early perceptual effects
of distal prosody on speech segmentation. Language,
Cognition and Neuroscience, 29, 1132-1146. doi:10.1080/
23273798.2014.894642
Cooper, N., Cutler, A., & Wales, R. (2002). Constraints of Lexical
stress on lexical access in English: Evidence from native
and non-native listeners. Language and Speech, 45(3), 207-
228. doi:10.1177/00238309020450030101
Cunillera, T., Laine, M., Camara, E., & Rodriguez-Fornells, A.
(2010). Bridging the gap between speech segmentation
and word-to-world mappings: Evidence from an audiovisual
statistical learning task. Journal of Memory and Language, 63,
295-305. doi:10.1016/j.jml.2010.05.003
Cutler, A. (1990). Exploiting prosodic probabilities in speech
segmentation. Cognition Models Speech Processes, 105-121.
Cutler, A., & Carter, D. M. (1987). The predominance of strong
initial syllables in the English vocabulary. Computer Speech
& Language, 2, 133-142. doi:10.1016/0885-2308(87)90004-0

Cutler, A., Mehler, J., Norris, D., & Segui, J. (1986). The syllable's
differing role in the segmentation of French and English.
Journal of Memory and Language, 25, 385-400. doi:10.1016/
0749-596X(86)90033-1
Cutler, A., Mehler, J., Norris, D., & Segui, J. (1992). The monolingual nature of speech segmentation by bilinguals.
Cognitive Psychology, 24, 381-410. doi:10.1016/0010-0285
(92)90012-Q
Cutler, A., Mehler, J., Otake, T., & Hatano, G. (1993). Mora or syllable? Speech segmentation in Japanese. Journal of Memory
and Language, 32(2), 258-278. doi:10.1006/jmla.1993.1014
Cutler, A., & Norris, D. G. (1988). The role of strong syllables in
segmentation for lexical access. Journal of Experimental
Psychology: Human Perception and Performance, 14, 113-
121. doi:10.1037/0096-1523.14.1.113
Cutler, A., & Otake, T. (1999). Pitch accent in spoken-word recognition in Japanese. The Journal of the Acoustical Society of
America, 105(3). doi:10.1121/1.426724
Cutler, A., & Van Donselaar, W. (2001). Voornaam is not (really) a
homophone: Lexical prosody and lexical access in Dutch.
Language and Speech, 44, 171-195. doi:10.1177/
00238309010440020301
Dahan, D., & Brent, M. R. (1999). On the discovery of novel wordlike units from utterances: An artificial language study with
implications for native- language acquisition. Journal of
Experimental Psychology: General, 128, 165-185. doi:10.
1037/0096-3445.128.2.165
Daland, R., & Pierrehumbert, J. B. (2011). Learning diphonebased segmentation. Cognitive Science, 35, 119-155. doi:10.
1111/j.1551-6709.2010.01160.x
Dilley, L. C., & McAuley, J. D. (2008). Distal prosodic context
affects word segmentation and lexical processing. Journal
of Memory and Language, 59, 294-311. doi:10.1016/j.jml.
2008.06.006
Dumay, N., Frauenfelder, U. H., & Content, A. (2002). The role of
the syllable in lexical segmentation in French: Word-spotting
data. Brain and Language, 81, 144-161. doi:10.1006/brln.
2001.2513
Dupoux, E., Pallier, C., Sebastian, N., & Mehler, J. (1997). A destressing "deafness" in French? Journal of Memory and
Language, 36, 406-421. doi:10.1006/jmla.1996.2500
Dupoux, E., Peperkamp, S., & Sebastian-Galles, N. (2010). Limits
on bilingualism: Stress "deafness" in simultaneous FrenchSpanish bilinguals. Cognition, 114, 266-275. doi:10.1016/j.
cognition.2009.10.001
Endress, A. D., & Mehler, J. (2009). The surprising power of statistical learning: When fragment knowledge leads to false
memories of unheard words. Journal of Memory and
Language, 60, 351-367. doi:10.1016/j.jml.2008.10.003
Flege, J. E. (2003). Assessing constraints on second-language
segmental production and perception. Phonetics and
Phonology in Language Comprehension and Production:
Differences and Similarities, 319-355.
Goetry, V., & Kolinsky, R. (2000). The role of rhythmic cues for
speech segmentation in monolingual and bilingual listeners.
Psychologica Belgica, 40, 115-152.
Hay, J., Pelucchi, B., Estes, K. G., & Saffran, J. R. (2012). Linking
sounds to meanings: Infant statistical learning in a natural
language. Cognitive Psychology, 63, 93-106. doi:10.1016/j.
cogpsych.2011.06.002
Hisagi, S., Strange, W., & Sussman, E. (2010). Perception of a
Japanese vowel length contrast by Japanese and American

LANGUAGE, COGNITION AND NEUROSCIENCE

English listeners: Behavioral and electrophysiological measures.
Brain Research, 89-105. doi:10.1016/j.brainres.2010.08.092
Jusczyk, P. W., Friederici, A. D., Wessels, J. M., Svenkerud, V., &
Jusczyk, A. M. (1993). Infants' sensitivity to the sound patterns of native language words. Journal of Memory and
Language, 32, 402-420. doi:10.1006/jmla.1993.1022
Klatt, D. H. (1976). Linguistic uses of segmental duration in English:
Acoustic and perceptual evidence. The Journal of the Acoustical
Society of America, 59, 1208-1221. doi:10.1121/1.380986
Langus, A., Marchetto, E., Bion, R. A. H., & Nespor, M. (2012). Can
prosody be used to discover hierarchical structure in continuous speech? Journal of Memory and Language, 66, 285-
306. doi:10.1016/j.jml.2011.09.004
Lehtonen, J. (1970). Aspects of quantity in standard Finnish.
Jyvaskyla: Jyvaskyla University Press.
Mattys, S. L., Jusczyk, P. W., Luce, P. A., & Morgan, J. L. (1999).
Phonotactic and prosodic effects on word segmentation in
infants. Cognitive Psychology, 38, 465-494. doi:10.1006/
cogp.1999.0721
Mehler, J., Jusczyk, P. W., Lambertz, H., Bertoncini, A.-T. (1988). A
precursor of language acquisition in young infants.
Cognition, 29, 143-178. doi:10.1016/0010-0277(88)90035-2
Mirman, D., Magnuson, J. S., Estes, G., & Dixon, J. A. (2008). The
link between statistical segmentation and word learning in
adults. Cognition. doi:10.1016/j.cognition.2008.02.003
Morrill, T., Dilley, L. C., & McAuley, J. D. (2014). Prosodic patterning
in distal speech context: Effects of list intonation and f0 downtrend on perception of proximal prosodic structure. Journal of
Phonetics, 46, 68-85. doi:10.1016/j.wocn.2014.06.001
Morrill, T., Dilley, L., McAuley, J. D., & Pitt, M. (2014). Distal
rhythm influences whether or not listeners hear a word in
continuous speech: Support for a perceptual grouping
hypothesis. Cognition, 131, 69-74. doi:10.1016/j.cognition.
2013.12.006
Morrill, T., McAuley, J. D., Dilley, L. C., Zdziarska, P. A., Jones, K. B.,
& Sanders, L. D. (2014c). Distal prosody affects learning of
novel words in an artificial language. Psychonomic Bulletin
Review, 1-9. doi:10.3758/s13423-014-0733-z
Newport, E. L., & Aslin, R. (2004). Learning at a Distance I. Statistical
learning of non-adjacent dependencies. Cognitive Psychology,
48, 127-162. doi:10.1016/S0010-0285(03)00128-2
Norris, D., McQueen, J. M., Cutler, A., & Butterfield, S. (1997). The
possible-word constraint in the segmentation of continuous
speech. Cognitive Psychology, 34, 191-243. doi:10.1006/cogp.
1997.0671
Otake, T., Takashi, H., Cutler, A., & Mehler, J. (1993). Mora or syllable? Speech segmentation in Japanese. Journal of Memory
and Language, 32, 258-278.
Pelucchi, B., Hay, J., & Saffran, J. R. (2009). Statistical learning in
natural language by 8-month-old infants. Child Development,
80, 674-685. doi:10.1111/j.1467-8624.2009.01290.x
Pierrehumbert, J., & Beckman, M. (1988). Japanese tone structure. Cambridge, MA: MIT Press.

815

Saffran, J. R., Newport, E. L., & Aslin, R. (1996). Word segmentation: The role of distributional cues. Journal of Memory and
Language, 35, 606-621. doi:10.1006/jmla.1996.0032
Sanders, L. D., & Neville, H. J. (2002). Speech segmentation by
native and non-native speakers: The use of lexical, syntactic,
and stress-pattern cues. Journal of Speech Language and
Hearing Research, 45, 1301-1321. doi:10.1044/1092-4388
(2002/041)
Sebastian-Galles, N., Dupoux, E., Segui, J., & Mehler, J. (1992).
Contrasting syllabic effects in Catalan and Spanish. Journal
of Memory and Language, 31, 18-32. doi:10.1016/0749596X(92)90003-G
Shukla, M., Nespor, M., & Mehler, J. (2007). An interaction
between prosody and statistics in the segmentation of
fluent speech. Cognitive Psychology, 54, 1-32. doi:10.1016/j.
cogpsych.2006.04.002
Suomi, K. (2007). On the tonal and temporal domains of accent
in Finnish. Journal of Phonetics, 35, 40-55. doi:10.1016/j.
wocn.2005.12.001
Suomi, K., Toivonen, K. J., & Ylitalo, R. (2003). Durational and
tonal correlates of accent in Finish. Journal of Phonetics, 31,
113-138. doi:10.1016/S0095-4470(02)00074-8
Suomi, K., & Ylitalo, R. (2004). On durational correlates of word
stress in Finnish. Journal of Phonetics, 32, 35-63. doi:10.1016/
S0095-4470(03)00005-6
Toro, J. M., Sinnett, S., & Soto-Faraco, S. (2005). Speech segmentation by statistical learning depends on attention. Cognition,
97, B25-B34. doi:10.1016/j.cognition.2005.01.006
Turk, A. E., Jusczyk, P. W., & Gerken, L. (1995). Do English-learning infants use syllable weight to determine stress?
Language
Speech,
38,
143-158.
doi:10.1177/
002383099503800202
Tyler, M. D., & Cutler, A. (2009). Cross-language differences in
cue use for speech segmentation. The Journal of the
Acoustical Society of America, 126, 367-376. doi:10.1121/1.
3129127
Venditti, J. J. (2005). The J_ToBI model of Japanese intonation.
In S. A. Jun (Ed.), Prosodic typology: The phonology and intonation of phrasing (pp. 172-200). Oxford: Oxford University
Press.
Vroomen, J., Tuomainen, J., & de Gelder, B. (1998). The roles of
word stress and vowel harmony in speech segmentation.
Journal of Memory and Language, 38, 133-149. doi:10.1006/
jmla.1997.2548
Warner, N., Otake, T., & Arai, T. (2010). Intonational structure as a
word-boundary cue in Tokyo Japanese. Language and
Speech, 53, 107-131. doi:10.1177/0023830909351235
Werker, J. F., & Tees, R. C. (1984). Phonemic and phonetic factors
in adult cross-language speech perception. The Journal of the
Acoustical Society of America, 75, 1866. doi:10.1121/1.390988
White, L., & Mattys, S. L. (2007). Calibrating rhythm: First
language and second language studies. Journal of
Phonetics, 35, 501-522. doi:10.1016/j.wocn.2007.02.003

816

T. H. MORRILL

Appendix A. Learned words in Experiment 1 (Finnish - IPA)
Frequency
12-16 times
12-16 times
3-4 times
3-4 times

Position
Phrase boundary
Phrase-medial
Phrase boundary
Phrase-medial

m
v
pu
soi

kt
pi
mt
luk

Appendix B. Learned words in Experiment 2 (Japanese - IPA)
Frequency
12 times
12 times
3 times
3 times

Accent pattern
High-low
Low-high
High-low
Low-high

s
i
m
i

nk
jm

mts

nts

km
mti
ks
nmi

sitae
ph
rh
kir

sri
mu
jaedae
uni

kl
kylae
sm
lj

ikunt
sili
kaepy
ritli

tityst
trinn
jaervlae
smti

