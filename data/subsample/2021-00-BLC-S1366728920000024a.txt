Bilingualism: Language and
Cognition
cambridge.org/bil

Concurrent verbal working memory load
constrains cross-linguistic translation
activation: A visual world eye-tracking study on
Hindi-English bilinguals

Research Article

Seema Prasad and Ramesh Kumar Mishra

Cite this article: Prasad S, Mishra RK (2021).
Concurrent verbal working memory load
constrains cross-linguistic translation
activation: A visual world eye-tracking study on
Hindi-English bilinguals. Bilingualism:
Language and Cognition 24, 241-270. https://
doi.org/10.1017/S1366728920000024

Center for Neural and Cognitive Sciences, University of Hyderabad

Received: 12 August 2018
Revised: 21 November 2019
Accepted: 20 December 2019
First published online: 9 June 2020
Keywords:
Working memory load; bilingual; crosslinguistic language activation; visual world;
eye movements
Address for correspondence:
Ramesh Kumar Mishra,
E-mail: rkmishra@uohyd.ac.in

(c) Cambridge University Press 2020

Abstract
Does a concurrent verbal working memory (WM) load constrain cross-linguistic activation?
In a visual world study, participants listened to Hindi (L1) or English (L2) spoken words
and viewed a display containing the phonological cohort of the translation equivalent (TE
cohort) of the spoken word and 3 distractors. Experiment 1 was administered without a
load. Participants then maintained two or four letters (Experiment 2) or two, six or eight letters (Experiment 3) in WM and were tested on backward sequence recognition after the visual
world display. Greater looks towards TE cohorts were observed in both the language directions in Experiment 1. With a load, TE cohort activation was inhibited in the L2 - L1 direction and observed only in the early stages after word onset in the L1 - L2 direction suggesting
a critical role of language direction. These results indicate that cross-linguistic activation as
seen through eye movements depends on cognitive resources such as WM.

Introduction
Translating a concept in both languages is fundamental to the bilingual experience. When a
bilingual looks at an image, two names of that object in the bilingual's two different languages
are said to be automatically activated. Much psycholinguistic research has focused on the magnitude, and the directionality of such translation activation (Hoshino & Kroll, 2008; Kroll &
Ma, 2018; Kroll & Stewart, 1994; Sunderman & Kroll, 2006; Van Hell & Tanner, 2012).
More recent studies have shown that translation is spontaneous and unconscious even in bilinguals who have good proficiency in both their languages (Thierry & Wu, 2007). Studies using
the visual world paradigm have revealed the spontaneous nature of both within- and crosslanguage activation at phonological and semantic level (Canseco-Gonzalez, Brehm, Brick,
Brown-Schmidt, Fischer & Wagner, 2010; Ju & Luce, 2004; Lagrou, Hartsuiker & Duyck,
2011; Marian, Spivey & Hirsch, 2003; Mercier, Pivneva & Titone, 2014; Mishra & Singh,
2014; Spivey & Marian, 1999; Weber & Cutler, 2004). Models such as BLINCS (Shook &
Marian, 2013) predict the nature of eye movements during bilingual audio-visual language
processing and how these dependent variables reflect subtle aspects of phonological and
semantic activation in bilinguals. Working memory as a critical resource plays a key role in
many aspects of bilingual language processing (Hadar, Skrzypek, Wingfield & Ben-David,
2016; Ito, Corley & Pickering, 2018). On several occasions, we encounter the need to perform
cross-modal information processing under a concurrent working memory load. For instance,
when we are trying to integrate some spoken and visual information while simultaneously
rehearsing a telephone number for later retrieval. In this study, we ask if working memory
load influences this so-called "automatic" or spontaneous nature of translation activation in
bilinguals. The issue is whether, and if so, which cognitive systems constrain language
non-selective activation as seen in multiple domains and modalities of bilingual language
processing. Here, we do not explore if working memory capacity affects an individual's
language non-selective activation but use working memory as a cognitive load to track its
influence on audio-visual processing.
In the visual world paradigm, eye movements are tracked to visual objects as a function of
simultaneous spoken language input (Huettig & Altmann, 2005; Mishra, 2009; Tanenhaus,
Spivey-Knowlton, Eberhard & Sedivy, 1995). As participants listen to fragments of spoken language, their eye movements are driven towards visual objects that are related to the spoken
language in some manner. We chose this paradigm for two reasons: 1) It has been extensively
used to examine bilingual parallel language activation during processing of spoken information in the co-presence of visual information and robust evidence for cross-linguistic activation
in bilinguals has been observed; 2) the online nature of the eye movement data obtained with
this paradigm allows tracking the time-course of cross-linguistic activation. For example,
Mishra and Singh (2014), in a visual world eye-tracking study found that Hindi-English

242

bilinguals looked immediately at the Hindi word bandar/monkey
among distractors as they listened to the English word gun/bandook. That is because bandar is phonologically related to the
translation of gun (bandook). This activation was seen in both
L1-L2 (i.e., L1 spoken word) and L2-L1 (i.e., L2 spoken word)
directions suggesting that bilinguals activate cross-language
orthographic information in either language direction. In a later
study, the authors demonstrated such cross-linguistic translation
activation using pictures as well (Mishra & Singh, 2016).
Importantly, these studies show that unbalanced bilinguals who
have acquired their second language much later in life but have
reasonable proficiency in it activate cross-linguistic information
with input in either of their languages.
Can such cross-linguistic activation as seen through eye movements be constrained by an external factor? Few recent studies
have provided evidence in favour of this (Blumenfeld & Marian,
2013; Mercier et al., 2014). For instance, Blumenfeld and
Marian (2013) observed that Spanish-English bilinguals with better inhibitory control on a non-linguistic Stroop task showed
lesser parallel activation in the later period (approximately 600-
800 ms after word onset). Similarly, Merceir et al. (2014) found
that bilinguals with better scores on oculomotor inhibitory control tasks showed smaller within-language and cross-language
activation during the visual world task. These studies suggest
that general-purpose executive control processes can help in the
conflict resolution between two languages during spoken language processing and constrain the degree of parallel language
activation.
Several researchers have advanced theories on the mechanism
of language-mediated eye movements assigning a critical role
to working memory (Huettig, Olivers & Hartsuiker, 2011a).
Huettig et al. (2011a) have suggested that working memory acts
as the buffer which integrates the incoming visual information
from the display and the linguistic information evoked due to
the processing of the spoken words and directs eye movements
to appropriate locations. Working memory has also been suggested to play a role in the retrieval of visual information during
the integration of linguistic and visual information (Knoeferle &
Crocker, 2006).
In the only other study to examine the role of working memory load on cross-linguistic activation, Prasad, Viswambharan and
Mishra (2019) administered the visual world paradigm under a
visual working memory load. Participants heard a spoken word
and viewed a display containing the spoken word referent, the
cohort of the translation equivalent and two unrelated distractors.
Before the visual world display, these were asked to remember an
array of five colours and were presented a test array after the visual
world display. The task was to judge if the test array was the same
or different compared to the earlier array. The study showed
reduced cross-linguistic activation but only when the participants
were asked to click on the spoken-word referent (Experiment 1).
No such effects were observed when there was no task-demand
(Experiment 2) suggesting that a visual working memory load
can constrain irrelevant cross-linguistic activations under certain
conditions. In another study, Huettig and Janse (2016) examined
anticipatory spoken language processing using the visual world
paradigm where participants listened to spoken sentences and
looked at four line drawings. Their interest was to see if participants looked at the context-appropriate objects anticipatorily
and whether their working memory capacity modulated the
looks to the objects. It was found that working memory capacity
positively correlated with the extent of language-mediated eye

Seema Prasad and Ramesh Kumar Mishra

movements during prediction. These findings suggest that cognitive resources, specifically working memory, are required for
language-mediated eye movements during spoken word
comprehension.
Considering the paucity of research on the role of working
memory on language-mediated eye movements in bilinguals, we
examined it using the visual world paradigm on Hindi-English
bilinguals. Participants were asked to maintain a set of digits in
memory which was followed by a visual world display of written
word along with spoken word input. One of the written words in
the display was a phonological competitor of the translation of the
spoken word (TE cohort), and others were unrelated distractors.
We adapted the design and the stimuli of Mishra and Singh
(2014) who had found activation of TE cohorts when the spoken
words were presented in both L1 or L2 using similar bilinguals.
To make the participants attentive to the visual world segment
of the study, they were asked to report the identity of the words
presented on the display on some trials. Previous visual world
studies have used written words in place of pictures and have
found similar effects for language-mediated eye movements to
targets (Huettig & McQueen, 2007; Salverda & Tananhaus,
2010) as well as competitors (Veivo, Jarvikivi, Porretta &
Hyona, 2016; Veivo, Porretta, Hyona & Jarvikivi, 2018).
An important variable in our study was the direction of language input. Since we were expecting listeners to spontaneously
translate the spoken word into the other language which was
then expected to activate the phonological cohort, it is important
to know how such a mechanism is affected by input in L1 and L2.
The revised hierarchical model (RHM, Kroll & Stewart, 1994) is a
developmental model for bilingual language processing which
assumes that early learners of L2 have weaker conceptual links
between L2 words and their meaning. So, when the input is in
L2, they access the meaning via the L1 translation of the word.
Thus, backward translation (L2 to L1) is mediated through lexical
links between L2 and L1 whereas forward translation (L1 to L2)
can proceed through the strong conceptual links of L1 words.
The model, thus, predicts faster backward translation through
the direct lexical route which has been backed up by several studies using translation recognition/production tasks (Kroll, Michael,
Tokowicz & Dufour, 2002; Poarch, Van Hell & Kroll, 2015;
Talamas, Kroll & Dufour, 1999). However, the opposite pattern
of results has also been found with faster translation in the forward
direction (Christoffels, De Groot & Kroll, 2006; Sunderman &
Priya, 2012). For instance, Sunderman and Priya (2012) found
that Hindi-English bilinguals were faster in translating from L1
to L2 in a translation recognition task. The authors suggested
that this reversed pattern of results could be attributed to the
higher L2 proficiency of their participants. This suggests that as
the proficiency in L2 increases, the reliance on L1 translation to
access the meaning of L2 words decreases which weakens the direct lexical activation of L1 upon hearing/reading L2. Thus, the pattern of faster translation from L2 to L1 observed in L2 learners
becomes reversed with increasing L2 proficiency.
Although it is to be noted that several researchers have criticised the basic tenets of RHM on the grounds of lack of evidence
for some of its key predictions (Brysbaert & Duyck, 2010). One of
the fundamental assumptions of RHM is the existence of separate
lexicons for the two languages and as a consequence selective
access to either of the languages. But, cross-linguistic activation
seen in the visual world paradigm indicates non-selective access
(Spivey & Marian, 1999). Studies using word recognition tasks
have shown that the other language is active even when the entire

Bilingualism: Language and Cognition

task is run in one language (Thierry & Wu, 2007). Also, the translation asymmetry (faster translation in L2 to L1 direction) predicted by RHM in early L2 learners is not always observed. As
described above, this asymmetry is expected to even out with
increasing L2 proficiency, but equivalent translation effects are
observed even for L2 learners (e.g., Duyck & Brysbaert, 2004).
This suggests that the assumption of conceptual mediation only
through L1 in L2 learners, as assumed by RHM, is not necessarily
true. L2 words also seem to have conceptual links. In contrast, the
BIA+ model (Dijkstra & van Heuven, 2002) assumes an integrated lexicon and predicts cross-linguistic activation given any
language input. Importantly, BIA+ architecture proposes that
non-linguistic factors such as WM and executive control cannot
directly influence lexical activations and are slower in exerting
their influence. Thus, according to BIA+ cross-linguistic activations are more bottom-up and automatic in the early stages of
word recognition, whereas later activations are more susceptible
to control (Blumenfeld & Marian, 2013). Thus, we would expect
the WM load to influence cross-linguistic activations only in the
later stages of comprehension.
Our participants in this study were Hindi (L1) - English (L2)
bilingual university students who predominantly used L2 in their
daily communication and were expected to be highly proficient in
L2. In previous studies on the same population, similar sample of
Hindi-English bilinguals have been found to be more proficient
in their L2 than L1 (Bhatia, Prasad, Sake & Mishra, 2017;
Prasad, Viswambharan & Mishra, 2019). Thus, we expected the
translation from L2 to L1 to not be mediated by the lexical
route and thus be more effortful and slow. If this is indeed the
case, any additional working memory load should affect
language-mediated search in the L2-L1 direction more so than
the L1-L2 direction. Since eye movements are an outcome of
the translation process itself, the load should affect this very
process.
We chose the proportion of fixations as our main dependent
measure as eye movements are considered to be a direct correlate
of linguistic processing. Researchers have shown that a greater
number of saccades are made to an object which has an enhanced
representation in the mind (Altmann & Kamide, 2007). This
occurs because the spoken language input increases the activation
of the referred objects. As discussed in the previous section, bilinguals also routinely activate the translations of the words they
encounter and their related words. Thus, listening to a word triggers activations of neighbouring words in that language, the translation word and the words related to the translation. Objects in
the display whose representations match the spoken-word triggered activations then attract eye movements towards them.
Different aspects of eye movements are used to indicate languagemediated behaviour. Typically, the proportion of fixations - the
likelihood of fixating an object in a given time window relative
to overall looks in that time window - is said to index the level
of attention deployed at the location of the object. Some researchers have also measured the proportion of time participants look at
an object (dwell time) or the latency of the first saccade to an
object, although the proportion of fixations remains the most
commonly used measure (Huettig, Rommers & Meyer, 2011b).
It is to be noted that in spite of many visual word eye-tracking
studies on such issues with bilinguals, many things related to the
core mechanisms remain unclear. It is very difficult to say with
confidence what exactly these looks towards visual objects mean
- linguistic activation or ocular responses related to search?
Nevertheless, we assume here that eye movements towards various

243

objects indicate the actual psycholinguistic process of translation
triggered by the spoken word input. Therefore, we expect the
working memory load to reduce the competitor activations to a
greater extent in the direction in which translation activation is
more effortful, which in this case is the L2 - L1 direction.
The other very important point is the effect of load on the time
course of activation. Since eye movements measured with this
paradigm provide online data, we can map the degree of activation or deactivation over a period and compare them across
objects. Based on the predicted translation asymmetry in this
set of bilinguals, it is likely that the translation will be fast in
the L1 - L2 direction and, therefore, corresponding eye movements towards the competitors should show a peak in the early
time windows (200-600 ms). Further, the effect of load might
be seen more in the later period since previous studies have
shown that only later activation is under cognitive control whereas
early parallel activation is more automatic (Blumenfeld & Marian,
2013, although there is no definite consensus regarding what
exactly constitutes "late" and "early"). This is also in line with
the precautions of BIA+ model. Therefore, we will explore if
working memory load affects activation of TE cohorts on different
time scales depending on the language of the input spoken word.
In the absence of previous background research, we do not have
exact predictions regarding the specific time course effects and
its interaction with language direction.
The first experiment was administered without any load to
replicate the basic effect of cross-linguistic activation observed
in Mishra and Singh (2014). The effect of the load was examined
in Experiment 2 (load: 2 and 4 letters) and Experiment 3 (load: 2,
6 and 8 letters). Thus, we aimed to examine the effect of increasing load on cross-linguistic activation. If working memory load
constrains the processing of spoken words, we predicted that
the load should lead to either delayed or reduced looks towards
the TE cohorts. However, if language-mediated eye movements
are automatic and independent of working memory, we predicted
that there would be no difference in the time course and magnitude of cross-linguistic activation as a function of load.

Experiment 1
Methods
Participants
Nineteen Hindi-English bilinguals (7 females, mean age = 23.4
years, SD = 0.6) participated in the main experiment (Table 1).
All participants belonged to the students' community of the
University of Hyderabad. Hindi (L1) was their native language
and they had learnt English (L2) early in life (at 4.8  1.9 years)
through formal schooling. Participants completed a series of control tasks which included tasks measuring their proficiency in L1
and L2. A language questionnaire (Bhatia et al., 2017; Kapiley &
Mishra, 2018, 2019; Roychoudhuri, Prasad & Mishra, 2016) was
also administered to each participant which contained questions
that required self-rating (on a scale of 1-10, 10 being the highest)
on their level of proficiency in reading, speaking and auditory
comprehension. It is apparent from the data that participants
had acquired English at an early age and used English throughout
their education in college and university. Therefore, these were
early bilinguals. Informed written consent was taken from all participants for their participation in the study. The study was
approved by the institutional ethics committee of University of
Hyderabad.

244

Seema Prasad and Ramesh Kumar Mishra

Table 1. Language proficiency and Demographic data

Age (in years)
Age of acquisition of L2

Experiment 1

Experiment 2

Experiment 3

Mean (SD)

Mean (SD)

Mean (SD)

23.4 (0.6)

22.5 (0.87)

24.5 (0.7)

4.8 (1.9)

5.15 (2.33)

6.5 (2)

Years of education in L1

5.7 (2.8)

6.06 (3.77)

6.7 (5.4)

Years of education in L2

11.3 (4.7)

10.06 (5.21)

12.54 (7.2)

Score on L1 verbal fluency test(words/min)

13.5 (4.7)

12.73 (5.13)

13.21 (4.1)

Score on L2 verbal fluency test (words/min)

12.8 (4.1)

12.4 (4.4)

13.55 (4.3)

Score on L2 vocabulary test

51% (4)

49% (14)

48.5% (11)

Self-report score (L2)
Speaking

8.9 (1.4)

8.8 (1.08)

7.8 (1.05)

Auditory comprehension

8.7 (1.3)

9 (1.07)

8.1 (0.95)

Reading

8.4 (0.8)

9.13 (0.99)

8.87 (0.79)

Self-report score (L1)
Speaking

7.1 (1.1)*

6.67 (1.23)**

6.95 (0.99)*

Auditory comprehension

6.8 (1.3)**

7.67 (0.97)**

7.37 (0.87)*

Reading

7.9 (1.4)

9.07 (0.96)

8.08 (1.02)*

10 point scale. 1 = poor, 10 = excellent.
Note: t-tests were performed between L1 and L2 self-rating scores. The significance values are mentioned in the table.** p < 0.01, * p < 0.05

Vocabulary test
All the participants completed an online vocabulary test
(WordORnot, Center for Reading research, Ghent University)
which was administered to test their proficiency in L2. The test
consisted of categorising the displayed string of letters as a
"word" or a "non word". The total score was the difference in
the percentage of trials in which a word was categorised correctly
and the percentage of trials a non-word was judged as a word
(Table 1). Based on their research so far, the researchers who developed this test estimate that a second language speaker of English
with high proficiency would judge approximately 33% of the
words correctly (Brysbaert, Stevens, Mandera & Keuleers, 2016).
Verbal fluency tests
Verbal fluency tests were administered in both L1 and L2. The
participants were given three letters (F, A, S in L2 and  (Ka),
(Pa), (Ma) in L1) and asked to produce as many words as possible for each letter in one minute. FAS was chosen for English as
it is one of the commonly used tasks, especially in clinical settings,
to examine language impairment (Spreen & Strauss, 1998). We
similarly chose the Hindi letters based on a battery of neuropsychological tests adapted for the Indian context (Hariprasad, Koparde,
Sivakumar, Varambally, Thirthalli, Varghese, Basavaraddi, &
Gangadhar, 2013; Rao, Subbakrishna & Gopukumar, 2004). The
number of unique words spoken were recorded and noted down
for each letter. A cumulative score for each language was calculated
by averaging the scores of the three letters. There was no difference
in verbal fluency scores(Table 1) between L1 (M = 11.65) and L2
(M = 11.25), p = .72.
Material & stimuli
Eighty common L1 nouns with their L2 translations and eighty L2
nouns with their L1 translations were selected. Out of these, forty
L1 nouns and forty L2 nouns were taken from a previous study of

one of the authors (Mishra & Singh, 2014). A total of 160 sets of
word stimuli along with 160 spoken words were created
(Appendix 1). Each set of word stimuli consisted of four written
words where one of the words was a phonological competitor of
the translation equivalent of the spoken word (TE cohort) and
others were unrelated distractors. For example, if the spoken
word was /SaDak (Road), the display contained the word
Rope which is a phonological cohort of the translation word
Road and 3 unrelated ditsractors. In the "L1-L2" language direction, the spoken words were in L1 and the display consisted of L2
written words. Similarly, for the "L2 - L1" direction, L2 spoken
words were coupled with the display containing L1 written
words. All the words were displayed at the center of equal sized
quadrants of size 256 * 192 pixels. The L1 words were written
in Krutidev font and the L2 words in Times New Roman, font
size 26.
Stimuli rating
Ten Hindi-English bilinguals who did not take part in the eye
tracking study performed the translation agreement task on the
160 set of stimuli. Participants were asked to rate the translation
pairs for their accuracy in both the language directions.
Agreement on the translation of the spoken word was rated on
a 5-point scale (5 indicating highest agreement). The ratings
showed that the agreement was high, both for L2 spoken words
(M = 4.99, SD = 0.03) and L1 spoken words (M = 4.95, SD =
0.06). The participants also rated the extent of phonological and
semantic similarity between the translation word and the four
written words on the display. For the L1 - L2 direction, the
phonological similarity of translation words to the TE cohorts
(M = 4.71, SD = 0.20) was significantly higher (p < .001) compared to the distractors (M = 1.00, SD = 0.002). We also made
sure that the TE cohort and the distractors were semantically
different from the translation word, (M = 1.05, SD = 0.11 and

Bilingualism: Language and Cognition

245

Fig. 1. Sequence of events in Experiment 1 and 2. In Experiment 1, visual world display was presented for 3000 sm along with a spoken word. In this example,
spoken word is in L1: Mor (Peacock). The display consists of a phonological cohort of Peacock: Peanut and three unrelated distractors. In Experiment 2, the visual
world display was preceded by the presentation of letters to be remembered (2 or 4 letters). The memory test question was presented just after the visual world
display on every trial. Participants were asked to recognise if the set of letters ("HF") was the correct backward sequence of the letters presented before. In this
example, the correct answer is Yes. On 10% of the trials, a question was asked on the visual world display. Participants had to judge if the given word (Tree) was
present in the display.

M = 1.02, SD = .05 respectively). Similarly, for the L2 - L1 direction, the TE cohort was significantly more similar sounding
(M = 4.44, SD = 0.62) to the translation word compared to the distractors (M = 1.15, SD = 0.31), p < .001. The TE cohort and the
distractors were also compared semantically with the translation
word (M = 1.22, SD = .34 and M = 1.18, SD = .28) which showed
that they belonged to different semantic categories. We also calculated the Levenshtein distance for an objective measure of the
similarity between the words. Levenshtein distance is the number
of insertions, deletions, substitutions required to transform one
string to the other. We found that the levenshtein distance
between the translation of a spoken word and it is cohort (M =
2.75, SD = 1.18) was significantly lower ( p < .001) than the distance between the spoken word and unrelated distracters (M =
5.14, SD = 1.09).
The spoken words were recorded using the software "Audacity"
by a native speaker of L1 and saved as .wav files. The mean duration of the spoken words in L1 and L2 was 413.13 ms (SD =
86.3 ms) and 495.33 (SD = 103.8) ms, respectively.

containing four written words and a spoken word were presented
simultaneously for 3000 ms through Creative Labs SBS A300
speakers. In order to make sure that participants paid attention
during the visual world task, they were asked a question regarding
one of the words on 10% of the trials (total 16 trials). On these
10% of the trials, the question was "did you see the word X?",
where X referred to one of the distractor words present in the display on half of the trials. On the other half, X referred to a word
which was not present in the display. Participants were expected
to press Yes (left shift key) or No (right shift key) on the keyboard
in response to this question. The key-to-response mapping was
counterbalanced across participants. Participants were given
20 practice trials before the start of the main experiment. A selfpaced break was given halfway through the experiment after
which drift correction was performed to ensure proper
calibration.
There were a total of 160 trials divided into two blocks. Trials
were divided equally and blocked between language-directions
(L1 - L2, L2 - L1). The order of presentation of the blocks as
well as the trials within the blocks were randomised.

Procedure
Participants sat at a distance of 60 cm from a 19 inch LCD monitor with a screen resolution of 1024 x 768 pixels and a refresh rate
of 60 Hz. Stimuli were prepared using SR Research Experiment
Builder software (SR Research Ltd, Ontario, Canada) and eye
movement data was controlled by a computer running Eyelink
1000 eye tracking system. The experiment began after a successful
13 point calibration (see Figure 1 for sample trials). Every trial
started with a black fixation cross (1 by 1) presented for
1000 ms against a white background at the center. Then, a display

Data analysis
Eye movements
Fixations to the TE cohort and distractor words were extracted
using the Data viewer software (SR research, Ontario). Each quadrant containing the written word was considered as an area of
interest (AOI). The fixations falling in these AOIs from the
onset of the spoken word till 1000 ms were extracted and considered for analysis. This time range was divided into 20 ms bins and
fixations to each AOI for each bin was calculated (Figure 2). The

246

Seema Prasad and Ramesh Kumar Mishra

Fig. 2. Comparing TE cohort activation observed in Mishra and Singh (2014) with the current study results (Experiment 1). Parallel activation is indicated by
the significantly higher proportion of looks to TE cohorts compared to distractors. Significant parallel activation was observed in Experiment 1 (without load) for
both language directions replicating findings from Mishra and Singh (2014).
Note: "L2 - L1" - L2 spoken words. "L1 - L2" - L1 spoken words. Proportion of fixations, denoted by P(fix), were only used for plotting for ease of visualisation.
Statistical analysis was performed on logit transformed data. Error bands denote  1 SE

proportion of fixations to an object was calculated as the number
of fixations to that object in the given time bin divided by the total
number of fixations in that time bin. The proportion of fixations
to the three distractors were averaged (see Table 2 for proportion
of fixation values for all conditions).
For statistical analysis, the proportion data was logit transformed (Barr, 2008) and mixed effects logistic regression analysis
was performed with package "lme4" in R (Bates, Maechler, Bolker
& Walker, 2015, version 1.1-10). Glmer function was used with
family specified as binomial and link logit. Language of the
spoken word (L1, L2; henceforth referred to as "spoken word language") and object type (TE cohort, distractor) were considered as
fixed effects. L2 spoken word and TE cohorts were considered as
baseline. To examine how cross-linguistic activation unfolds over
time, the time period was divided into 200 ms time windows consistent with previous visual world studies (Huettig et al., 2011b,
Mishra & Singh, 2014; Prasad et al., 2019). It is to be noted
that although the trial period was divided into 20 ms bins for
the sake of visualisation, statistical analysis was done on 200 ms
time periods which we will henceforth refer to as "time-windows".
Separate models were then created for each time-window (TW1:
0-200 ms, TW2: 200-400 ms, TW3: 400-600 ms, TW4: 600-
800 ms, TW5: 800-1000 ms) with language direction and object
type as fixed effect. In the eye movements analysis, "crosslinguistic activation" is used to denote a significant difference
between proportion of looks/fixation duration for TE cohorts
compared to distractors.
Manual responses
Accuracy analysis was performed on the responses in the visual
world task. Trials were dummy coded as 1 ("Correct trials")
and 0 ("Incorrect trials"). Mixed effects regression models with
family specified as binomial and link logit were fitted for
Accuracy data. Spoken word language (L1, L2) was considered

Table 2. Proportion of fixations under no-load (Experiment 1)
Experiment 1 (No load)
L1

Spoken word
Object type

L2

TEC

D

TEC

D

0-200

0.24

0.23

0.23

0.22

200-400

0.25

0.23

0.24

0.22

400-600

0.24

0.23

0.24

0.22

600-800

0.25

0.23

0.22

0.22

800-1000

0.25

0.23

0.22

0.22

Time (in ms)

Note: Bold values indicate significant differences between looks to TE cohorts (TEC) and
distractors (D) for that condition.

as the fixed effect. Response time data was not analysed as the visual world task was not a speeded task.
In all the models, two-way and 3-way interactions were
included in the mixed effects models only when they significantly
improved the model fit as determined by forward model comparison. For all the models, Participants and Items were considered as
random effects. Random slopes were not included in the lmer
model on overall fixations with Time as a factor as the model
failed to converge. Subject-wise random slopes were included
for each of the fixed effects. A random slope was kept in the random effect structure if including it improved the model fit. We
did not include item-wise random slopes as the model often failed
to converge and also because item-wise variation is usually lesser
than subject-wise variation. Statistical significance was determined using the p values obtained through the default output
of the glmer function in R based on asymptotic Wald tests

Bilingualism: Language and Cognition

247

(Luke, 2017)."multcomp" package in R was used to perform pairwise comparisons by constructing contrast matrices (Hothorn,
Bretz & Westfall, 2008).

p = .08). Accuracy increased with greater frequency of the distractor words.

Word frequency
Word frequency is an important predictor in psycholinguistic
experiments. Since we did not collect frequency ratings from
the participants, we used the Worldlex database (Gimenes &
New, 2016) to calculate the average frequency of the words presented on the visual world display. We extracted the frequency
per million values based on blogs and newspapers for both L1
and L2 for TE cohorts and each of the distractors. The database
also has data from twitter but it was not available for L1: thus,
data from newspapers and blogs was used for both the languages
for the sake of consistency. The log transform of frequency of TE
cohorts (logTE) and the log transform of the average frequency of
the three distractors (logD) was then entered as a fixed effect in all
the analyses (eye movements and manual responses). They were
retained in the model only when it improved the model fit as
compared to a model without them. These model comparisons
were made using the ANOVA function in R which yielded p
values based on chi square tests.

Discussion

Results
Fixations
logTE and logD were excluded as fixed effects as they did not significantly improve the model fit. The analysis of the logit transformed proportion of fixations between 0-1000 ms after word
onset showed a main effect of time ( = 0.0001, t = -4.68,
p < .001). All the two-way and three-way interactions involving
time were also significant (|t| > 3) indicating that the participants'
looks towards TE cohorts and distractors varied as a function of
time. It is difficult to examine complex 3-way or 4-way interactions through pairwise comparisons. Thus, separate models
were created for each 200 ms time window in line with previous
studies (e.g., Prasad et al., 2019). Looks to TE cohorts were significantly greater than looks to distractors in the first three time windows (0 ms - 600 ms), as indicated by a main effect of object type
(TW1:  = -0.1, t = -3.88, p < .001; TW2:  = -0.14, t = -3.74,
p < .001; TW3:  = -0.16, t = -3.63, p < .001 respectively). The
language of the spoken word influenced cross-linguistic activation
from TW3 (400 ms) onwards as indicated by a significant interaction between spoken word and object type for TW 3, 4 and 5
(TW3:  = 0.18, t = 3.16, p = .001; TW4:  = -0.11, t = -2.97,
p = .003; TW5:  = -0.16, t = -4.13, p < .001 respectively).
Pairwise comparisons for TW3 showed higher cross-linguistic
activation when spoken word was in L2 ( = 0.16, z = 3.63,
p < .001) compared to when the spoken word was in L1 ( =
0.04, z = 0.99, p = .481). Pairwise comparisons for TW 4 revealed
significant cross-linguistic activation only when the spoken word
was in L1 ( = 0.12, z = 2.42, p = .027), but not when spoken word
was in L2 ( = 0.01, z = 0.21, p = .9). A similar pattern was
observed in TW5 with significant cross-linguistic activation
observed for L1 spoken words ( = 0.15, z = 3.41, p = .001) but
not for L2 ( = -0.004, z = -0.09, p = .99) spoken words.
Visual world task
Spoken word language did not modulate the accuracy during the
visual world task ( = -0.35, z = -0.91, p = .362, L1: M = 81.58%,
SD = 25.47; L2: M = 84.96%, SD = 23.56). There was a marginally
significant effect of distractor frequency ( = 0.49, z = 1.76,

The objective of this experiment was first to establish crosslinguistic activation in this set of bilinguals using the design of
Mishra and Singh (2014). Participants looked at the TE cohort
of the spoken word more compared to the distractors indicating
activation of the non-target language. Looks towards TE cohorts
were significantly fewer and diminished faster when participants
heard spoken words in L2. These results mostly replicate the findings of Mishra and Singh (2014) and other similar studies in this
set of Hindi-English bilinguals. Most of the visual world studies
on bilinguals conducted to examine translation activation have
used pictures in the display. While some of the interactive models
of bilingual language processing (such as BIA) predict both orthographic and phonological activation of the competitor language
while processing words in one language, experimental evidence
for such activation during cross-modal processing is sparse (but
see, Mishra & Singh, 2014; Veivo et al., 2016, 2018). Thus, the
results of Experiment 1 once again show that bilinguals activate
orthographic forms of the translations and their competitors
while listening to a spoken word.
Experiment 2
Methods
Participants
Twenty-two Hindi-English bilinguals (four females, mean age =
22.5 years, SD = 0.87) participated in the experiment. None of the
them had taken part in Experiment 1.
Material and stimuli
Stimuli used in the visual world component were the same as
Experiment 1.
Procedure
In addition to the visual world component, each trial had a working memory component. A string of 2 or 4 English alphabet letters (Times New Roman, Bold, 26 pt size) were displayed one after
the other at the center of the screen before the onset of the visual
world display. These letters were randomly selected (without
replacement) by the software on each trial from a set of nine
English alphabet letters: B, F, H, J, L, M, Q, R and X (Marty,
Chemla & Spector, 2013). Each letter stayed for 1000 ms with
an interval of 500 ms in between during which a blank screen
was displayed.
The visual world display was then presented followed by the
working memory task (test phase) in which the same set of letters
were shown together. On 50% of the trials, the exact backward
sequence of the letters was shown. On the other half of the trials,
some other combination of letters randomly chosen by the software was shown. For example, if the letters B, H, M, L were initially shown sequentially in that order, the test phase display
could be LMHB (correct backward sequence) or some other combination of the letters. The participants had to judge if the order
of letters in the test phase was the correct backward sequence of
the previously shown letters. Participants indicated their response
by pressing Yes (left shift key) or No (right shift key) on the

248

keyboard. The key-to-response mapping was counterbalanced
across participants. Our working memory task involved recognition and not recall (e.g., asking participants to recall the letters
shown earlier and enter them through keyboard). Recall tasks
are generally assumed to be more effortful, utilise greater working
memory resources and require self-initiated processes than recognition tasks (Arenberg, 1973; Burke & Light, 1981; Schonfield &
Robertson, 1966). It is possible that a recall task would have led
to ceiling effects such that no effect would be seen on the visual
world component of the task. As the participants were asked to
pay attention to both the working memory task (presented on
every trial) and the visual world task (presented on 10% of the
trials), it was important not to make the working memory task
too difficult. Many such dual-task studies that have examined
the role of working memory load - for example, on attentional
selection - have similarly used recognition tasks (e.g., Soto,
Heinke, Humphreys & Blanco, 2005; Woodman & Luck, 2004).
A total of 160 trials were divided equally between the language
directions and type of load into 4 blocks. Language direction was
the first level of blocking and type of load was the second level.
That is, all trials with L1 (or L2) spoken word were presented
together. Within that, trials with the two types of load were presented in blocks (where either of them could appear first). The
ordering of the blocks was randomised at each blocking level.
The trial order within each block was also randomised.
Data analysis
Eye movements
The analysis procedure was the same as that of Experiment
1. Load (4 letters, 2 letters) was additionally included as a fixed
effect in the analysis of logit transformed fixations with 4-letter
load as the reference
Manual responses
The accuracy data from the visual world task data was analysed
using mixed effects (as described for Experiment 1) with load
as an additional fixed effect. The accuracy on the working memory task was also analysed using the same procedure.
Results
Fixations
There was a main effect of Time ( = 0.0002, t = -4.84, p < .001).
A significant interaction between object type, load, time and spoken word language was also observed ( = 0.0006, t = 6.03,
p < .001 Table 3, Figure 3). Separate analysis was performed for
each 200 ms time window. No significant cross-linguistic activation was observed in the first time window ( = -0.03, t =
-0.55, p = .583). In the second time window (200 ms - 400 ms),
a significant interaction between spoken word language, load
and object type was observed ( = -0.17, t = -2.46, p = .014).
Separate models were created for each language direction to
probe this 3-way interaction. There was a significant interaction
between load and object-type for L1 ( = -0.11, t = -2.31,
p = .021) but not L2 spoken words ( = 0.05, t = 1.18, p = .238).
Pairwise comparisons showed significant cross-linguistic activation (greater amount of looks to TE cohorts compared to distractors) when the spoken words were in L1 only for the 2-letter load
condition ( = 0.1, z = 3.01, p = .005) but not for the 4-letter load
condition ( = -0.008, z = -0.25, p = .96). No difference in the
looks was observed for English spoken words in both the load

Seema Prasad and Ramesh Kumar Mishra

conditions (t < 1.2). Interestingly, looks to TE cohorts were significantly reduced (TW3:  = 0.09, t = 2.02, p = 0.048) or equivalent (TW4:  = 0.05, t = 0.88, p = .384; TW5:  = 0.1, t = 1.62,
p = .113) compared to distractors in the subsequent time windows, indicating that the cross-linguistic activation was completely inhibited 400 ms onwards. There were marginally or
fully significant interactions between language direction, object
type and load in these time windows (TW3:  = 0.1, t = -1.66,
p = .097; TW4:  = 0.12, t = 1.74, p = .081; TW4:  = 0.44, t =
6.35, p < .001) but they were driven by greater looks to distractors
compared to TE cohorts (see Appendix for full model outputs).
Visual world task
The accuracy on the visual world task was higher in the 2-letter
load condition compared to the 4-letter load condition ( =
1.19, z = 3.18, p = .001). There was also a significant interaction
between load and spoken word language ( = -1.19, z = -2.459,
p = .014). Pairwise comparisons showed that the difference in
accuracy between 2-letter and 4-letter load conditions was significant only when spoken words were in L2 ( = -1.19, z = -3.18,
p = .003, 2-letters: M = 71.59%, SD = 32.08; 4-letters, M = 50%,
SD = 31.81) compared to L1 ( = 0.00006, z = 0, p = 1, 2-letters:
M = 51.14%, SD = 23.75; 4-letters, M = 51.14%, SD = 24.97).
Thus, the working memory load interfered with the visual
world task more when the spoken words were in L2.
Working memory task
The accuracy was higher on trials with 2-letter load (M = 94.26%,
SD = 3.6) compared to the 4-letter load condition ( = 0.53, z =
3.62, p < .001, M = 90.96%, SD = 13.41).
Discussion
In Experiment 2, the visual world display was presented concurrently with a verbal working memory load of 2 or 4 letters.
Participants maintained the verbal load while attending to the
visual world task. Two important results emerged from the analysis of the proportion data. First, the proportion of looks
towards TE cohorts did not differ significantly from the looks
towards distractors when the input was in L2 for both the load
conditions. This means that the TE cohort activation that we
observed in Experiment 1 disappeared when participants listened to spoken words in the second language. When the
input was in L1, participants looked at the TE cohorts more
than the distractors only in the initial period (200-400 ms)
and only in the 2-letter load condition. Thus, the presence of
the load completely inhibited L2 - L1 activation, whereas L1 -
L2 activation was preserved under a low load (2 letters) in the
initial period and completely inhibited under a high load (4 letters). This indicates that the effect of the load was greater when
the spoken word was in L2. Second, accuracy was higher under
the 2-letter load condition compared to the 4-letter load condition, but only when the spoken words were in L2. The load did
not affect the accuracy of the responses when the spoken words
were in L1. It additionally showed that the load manipulation
worked and had differential effects on the processing involved
during the presentation of the visual world display.
In summary, we observed that looks towards competitors
were inhibited under a high working memory load (4 letters).
Since this claim is essentially based on a null result (no differences in looks to TE cohorts and distractors), we aimed to

Bilingualism: Language and Cognition

249

Table 3. Proportion of fixations under two and four -letter load conditions (Experiment 2)
Experiment 2
2 letters

Load
L1

Spoken word
Object type

4 letters
L2

L1

L2

TEC

D

TEC

D

TEC

D

TEC

D

0-200

0.23

0.24

0.23

0.24

0.23

0.23

0.24

0.24

200-400

0.25

0.23

0.24

0.24

0.23

0.23

0.24

0.24

400-600

0.24

0.23

0.22

0.24

0.23

0.23

0.23

0.24

600-800

0.22

0.24

0.23

0.24

0.24

0.23

0.23

0.23

800-1000

0.22

0.23

0.24

0.23

0.24

0.23

0.22

0.23

Time (in ms)

Note: Bold values indicate significant differences between looks to TE cohorts and distractors for that condition.

Fig. 3. Plots showing parallel activation in
Experiment 2 and 3. Early activation (200-400 ms)
was observed only for L1 spoken words ("L1 - L2")
under 2-letter load (Experiment 2 and 3) and 6-letter
load (Experiment 3) condition. In both Experiment 2
and 3, no activation was observed when spoken
words were in L2 ("L2 - L1").
Note: Proportion of fixations, denoted by P(fix), were
only used for plotting for ease of visualisation.
Statistical analysis was performed on logit transformed data. Error bands denote  1 SE

replicate this effect in the next experiment using three different
types of load: 2, 6 and 8 letters.

Material and stimuli
Stimuli used in the visual world component and the
working memory component were similar to that used in
Experiment 1.

Experiment 3
Methods
Participants
Twenty-four Hindi English bilinguals (Mean age = 24.42 years,
SD = 1.38, 18 female) participated in the main experiment. None
of the participants had taken part in either Experiment 1 or 2.

Procedure
A procedure similar to Experiment 2 was used here with a minor
change in the load manipulation. Three types of loads were used:
2, 6 and 8 letters.

250

Seema Prasad and Ramesh Kumar Mishra

Table 4. Proportion of fixations as a function of load (Experiment 3)
Experiment 3
2 letters
L1

Spoken word
Object type

6 letters
L2

8 letters

L1

L2

L1

L2

TEC

D

TEC

D

TEC

D

TEC

D

TEC

D

TEC

D

0-200

0.24

0.22

0.24

0.22

0.23

0.22

0.21

0.22

0.23

0.21

0.23

0.21

Time (in ms)

200-400

0.23

0.21

0.24

0.22

0.23

0.21

0.20

0.21

0.20

0.21

0.22

0.21

400-600

0.23

0.21

0.24

0.21

0.23

0.21

0.20

0.21

0.18

0.21

0.22

0.21

600-800

0.23

0.21

0.23

0.22

0.24

0.21

0.20

0.21

0.20

0.21

0.21

0.21

800-1000

0.23

0.22

0.22

0.22

0.23

0.22

0.20

0.22

0.20

0.20

0.21

0.22

Note: Bold values indicate significant differences between looks to TE cohorts (TEC) and distractors (D) for that condition.

Data analysis & Results
The analyses procedure was exactly the same as that of
Experiment 2.

SD = 14.17; 6-letters: M = 88.85%, SD = 11.31; 8-letters: M =
89.44%, SD = 12.86)
Discussion

Fixations
There was a significant effect of Time on the looks to the visual
world display ( = 0.0001, t = -2.59, p = .009. Separate models
for each time-window were created to examine the time-course
of these effects (Table 4, Figure 3). No significant cross-linguistic
activation was observed in the first time-window ( = -0.09, t =
-1.57, p = .121). In the second time window, a significant threeway interaction between spoken word language, load and object
type was observed ( = -0.33, t = -4.11, p < .001). Separate models were created for each language direction to examine this 3-way
interaction which showed a 2-way significant interaction between
load and object type ( = 0.2, t = 3.73, p < .001). Pairwise comparisons on this model showed that when spoken words were in L1,
the looks to TE cohorts were greater compared to distractors
under 2 and 6 letter load ( = 0.11, z = 2.12, p = .09 and  =
0.15, z = 2.78, p = .015 respectively). No such differences were
observed for eight-letter load ( = -0.09, z = -1.73, p = .204).
No cross-linguistic activation was observed for English spoken
words under any of the load conditions (t < 1). In all the subsequent time windows (400 ms onwards), there was no significant
cross-linguistic activation in any of the conditions (t < 1)
Visual world task
The accuracy analysis showed a marginally significant interaction
between spoken word language and load ( = -1.51, z = -1.65,
p = .099). Pairwise analysis indicated that the accuracy was higher
under 6-letter load when compared to 8-letter load ( = -1.86,
z = -2.82, p = .026, 2-letters: M = 62%, SD = 43.97; 6-letters:
M = 82.67%, SD = 21.77; 8-letters: M = 46%, SD = 37.97), only
when the spoken word was in L2. Load did not modulate the
level of accuracy for L1 spoken words ( p > .3, 2-letters: M =
65.33%, SD = 22.52; 6-letters: M = 58%, SD = 23.62; 8-letters:
M = 69.44%, SD = 27.66).

Once again, we found that the working memory load reduced the
proportion of looks to competitors. Specifically, when spoken
words were in L1, higher looks to TE cohorts were observed for 2
and 6 letter load conditions in 200-400 ms time window.
Subsequently, there was no difference between looks to competitors
and distractors. When spoken words were in L2, the looks to TE
cohorts did not differ significantly from looks to distractors in any
of the load conditions. Surprisingly, TE cohort activation was
observed for 6-letter load condition (Experiment 3) but not for
the 4-letter condition (Experiment 2) in the 200-400 ms time window. Based on the results of Experiment 2, we expected crosslinguistic activation to be completely inhibited for the 6-letter load
condition in both language directions even in the early timewindow. We currently do not have any explanation as to why significant early activation was observed with such a relatively high working memory load. It is necessary to do more experiments with
different levels of load to establish if there's a gradient effect of the
load on cross-linguistic activation or if there is a threshold point
beyond which no activations are seen. We are only suggesting that
cross-linguistic activations are indeed susceptible to external constraints such as working memory resources. Future research should
focus on the boundary conditions of this mechanism in more detail.
We also observed that the working memory load modulated the
accuracy on the visual world task, but only when spoken words
were in L2. Participants were more accurate in the 6 letters condition than in the 8 letters condition. Accuracy was not modulated by
load for L1 spoken words. To summarise, Experiment 3 replicated
the findings of Experiment 2 and showed that maintaining a very
high working memory load while viewing the visual world display
constrains cross-linguistic activation as seen through languagemediated eye movements. Overall, the constraining effect of the
load was higher in the L2 - L1 direction in both the experiments.
General discussion

Working memory task
The accuracy on the working memory task did not differ between
any of the three load conditions (t < 1, 2-letters: M = 86.32%,

In three experiments, we examined the effect of a verbal working
memory load on cross-linguistic translation activation in Hindi-

Bilingualism: Language and Cognition

English bilinguals. Participants saw four written words on the
screen along with the audio input of a spoken word while holding
2,4, 6 or 8 letters in memory (Experiment 2 and 3). The baseline
experiment without any load (Experiment 1) established the
robust activation of TE cohorts in both language directions
thereby replicating previous findings (Mishra & Singh, 2014).
With the load (2 and 6 letters), TE cohorts were activated only
in the initial time period (200-400 ms) and only when the spoken
word was in L1. There was no activation of the TE cohorts when
spoken was in L2 under any of the load conditions. The accuracy
on the visual world task was also modulated by the load, but only
when the spoken words were in L2. Thus, concurrent working
memory load (of 2,4 and 6 letters) suppressed later activation in
the L1 - L2 direction but completely inhibited L2 - L1 activation.
To our knowledge, this is the first study to show the constraining
effect of a concurrent working memory load on language nonselective activation during audio-visual spoken language processing. Whether such activations are automatic or subject to any
form of control has been a matter of debate (Mishra, Olivers &
Huettig, 2013). Our results clearly show that working memory
load can significantly affect such non-selective activations.
The critical question we wanted to answer is should a WM
load influence the forward or the backward direction more?
Given that our bilinguals were unbalanced with higher proficiency
in L2, we expected them to translate words with less ease from L2
to L1 than in the reverse direction. Assuming that the WM load
consumes cognitive resources, any process that requires such a
resource more - such as activating the translations and related
words - will also suffer more under the load. With concurrent
load, we found almost no evidence of TE cohort activation
when spoken words were in L2. This pattern of results can be
understood if we assume that for such bilinguals who have
acquired higher proficiency in the second language, translation
of lexical items from L2 to L1 does not proceed via the direct lexical route (as it does in early L2 learners) which probably makes it
more resource consuming than the activation in the opposite direction. Similar results were obtained on the same population by
Prasad et al. (2019) where the effect of a visual working memory
load on cross-linguistic activation was greater when the spoken
words were in L2. Although, Prasad et al. (2019) used line drawings in their display which also consisted of the spoken-word referent (unlike the current study which only had TE cohort in the
display). Nonetheless, both the studies lead us to tentatively conclude that L2 - L1 activation requires more verbal working memory resources than L1 - L2 activation in bilinguals with high L2
proficiency. More such studies directly comparing two groups
of bilinguals differing in their L2 proficiency are required to
firmly establish the role of L2 proficiency on the automatic nature
(or the lack of it) of cross-linguistic activation.
Proportions of looks towards visual referents in a visual world
design are time sensitive. Early or late activations indicate the
nature of attentional demands. Early activations have been
assumed to be more automatic and late activations to be more
strategic (Blumenfeld & Marian, 2013). We observed early
(200-400 ms after word onset) L1 - L2 activation in the 2 and
6 letter load conditions (Experiment 2 and 3). This activation
was suppressed in the later time windows. This pattern of data
suggests that working memory load constrains the strategic and
resource demanding (late) activations and not the more robust
initial automatic activations. Blumenfeld and Marian (2013)
observed that when English-Spanish bilinguals listened to spoken
words in Spanish, cross-linguistic activation in the early time

251

periods (300-500 ms after word onset) was not inhibited by
executive control processes. Executive control measures correlated
with cross-linguistic activations only for the later time windows
(633-767 ms). Participants with better performance on the
Stroop task also inhibited cross-linguistic activations more effectively. This led the authors to conclude that early parallel language
activation is automatic while later activation is under strategic
control. Our results are in line with these findings.
In Experiment 1, we mostly replicated the findings of Mishra
and Singh (2014). But, it is important to note a few key differences
in the results of the two studies. No cross-linguistic activation was
observed for L2 spoken words 600 ms onwards in the current
study whereas Mishra and Singh (2014) found robust activations
in both the language directions till 1000 ms after word-onset.
Interestingly, in the current study, TE cohort activation was also
enhanced in the 400-600 ms time window for L2 spoken words
compared to L1 spoken words. A possible reason for this discrepancy could be that the participants in this study were highly proficient in L2 compared to the Hindi-English bilinguals in
Allahabad who participated in Mishra and Singh (2014).
University of Allahabad is a state university in the northern
part of India where Hindi is the native language of most students,
as a result of which most students dominantly use Hindi for their
communication. In contrast, University of Hyderabad attracts students from all over the country and English is the predominant
language of communication on campus. As discussed previously,
high proficient bilinguals no longer have the need to translate L2
words into L1 which could have led to the reduced TE cohort activation in the current study for L2 spoken words at later stages. It
is not possible to quantify these differences as neither we nor
Mishra and Singh (2014) collected any data on the percentage
of use and exposure to L2. At the moment, we can only speculate
that contextual differences between the two locations could have
led to these observed differences in the results.
Our findings have consequences for various bilingual language
processing models. As discussed in the introduction, RHM
explains translation effects as a function of developing second language proficiency in unbalanced bilinguals. We cannot directly
compare our results with the predictions of RHM because the
model was proposed to account for language production data.
However, our task essentially required people to translate and
activate cohorts of translation simultaneously with audiovisual
input. Even if not directly related, Pickering and Garrod (2013)
have proposed an integrated model of language processing that
links the mechanisms of production and comprehensions
together. Therefore, RHM should predict active bidirectional
translation effects in our bilinguals who were unbalanced; in
our other studies we have shown translation effects in these
types of bilinguals in the Indian context. However, it remains to
be examined to what extent RHM predicts translation in bilinguals who use languages that have no cognates such as our bilinguals. It is to be noted that later criticisms of the model on the
grounds that relate to not finding evidence of asymmetry in naming tasks in bilinguals may not be directly relevant to our conclusions drawn from a visual world experiment (Brysbaert & Duyck,
2010). It has not always been straight forward to predict the directionality and magnitude of translation effects in balanced and
unbalanced bilinguals who use different languages. Our data
assumes a wide-ranging connectionist framework that has widespread activations and cross-linkages across phonological and
orthographic levels. This is precisely the working assumption of
BIA+ which predicts co-activations in multiple modalities across

252

languages. Although BIA+ does not include any cognitive constraints such as WM and attention, it certainly helps us in theorising about the nature of bottom-up co-activations both within
and between languages. BIA + predicts unconstrained language
non-selective activation. However, it is not always clear if BIA +
would predict activation of translations of words across languages
that involve semantics. It certainly predicts activation of cohorts
both within and across languages.
More recently Dijkstra, Wahl, Buytenhuijs, Van Halem,
Al-Jibouri, De Korte and Rekke (2019) proposed the Multilink
model, which integrates some of the assumptions of both RHM
and BIA + . The model is comprehensive and attempts to explain
data from various language tasks, but as Mishra (2019) pointed
out, the model has little relevance for languages such as Hindi
and English that do not share orthography or phonology.
Further, the model also does not account for cross-modal activations derived from the visual world paradigm. However, the
model does refer to the nonlinguistic control mechanisms that
could constrain language activation in the bilingual mind. Our
use of working memory load was cognitively constraining from
a limited resource point of view. This kind of data should further
help to modify the predictions of Multilink model which attempts
to synthesise both linguistic and cognitive variables in bilingual
language activation.
The Bilingual Language Interaction Network for Comprehension
of speech (BLINCS) model was developed specifically to explain
audio-visual integration and cross-linguistic activation as seen in
visual world eye-tracking paradigm (Shook & Marian, 2013). We
have shown non-cognate cross-linguistic activation, which
BLINCS explains at the level of semantics. In addition to supporting
BLINCS' core predictions regarding parallel activation in the audiovisual domain, we have further shown the dynamic influence of
WM load on such activations. Currently, there are no other models
that explain such multimodal interactions, and therefore, the
BLINCS model can be further extended by considering the various
other cognitive variables that influence parallel language activation.
However, it is to be noted that all the above-mentioned languages
have been built based on European languages. But there are
major differences in reading, writing, speaking and listening practices between different languages making it difficult to directly borrow their predictions and apply it to the Indian context. Further,
cognitive constraints on bilingual language processing depend on
individual differences in executive control (Mercier et al., 2014),
which in turn depend on broader social and cultural context.
Thus, there is an urgent need to incorporate diverse languages, especially belonging to different language families, in the existing bilingual language processing models.
Surprisingly, there was no effect of frequency in all the experiments. This was unexpected considering robust frequency effects
are often observed during word recognition (Brysbaert,
Buchmeier, Conrad, Jacobs, Bolte & Bohl, 2011, Brysbaert et al.,
2016; Monsell, Doyle & Haggard, 1989). Frequency effects are
generally observed in lexical decisions tasks or word recognition
tasks. The nature of frequency effects in cross-modal language
processing as seen in the visual world paradigm has not been
examined thoroughly. Although we expect that frequencies to
play some role in the language mediated eye movements seen in
this paradigm, we are not in a position to clearly explain the
lack of frequency effects in our study. One possible reason
could be that our participants were highly proficient users of
both the languages. The Hindi-English bilinguals lived in a
dynamic language context in the University where both L1 and

Seema Prasad and Ramesh Kumar Mishra

L2 are used actively. It is known that high language exposure
leads to reduced frequency effects (Monaghan, Chang, Welbourne
& Brysbaert, 2017), presumably because such individuals also get
exposed to low-frequency words more often. But this is just
speculation at this point. More studies directed at examining
the interaction between word frequency and spoken word recognition in visual world tasks are necessary to throw more light on this
issue.
There are a few limitations in the current design that might
have influenced our results and interpretations. Our sample size
was low in all the experiments (< 25). Visual-world studies typically have more number of participants to lend more power to the
findings. But it is to be noted that, we had a high number of
experimental trials (80 for each language direction) which is double the number of trials used in Mishra and Singh (2014). Further,
we replicated the main pattern of results across two experiments
with load making our findings moderately robust. However, we
acknowledge that more experiments with large number of participants are required to develop a comprehensive model of crosslinguistic activation as seen through eye movements and the
nature of their dependence on capacity-limited resources.
Additionally, our design created a dual-task scenario on some
trials. In such a context, participants have to strategise and prioritise their division of attention between the two tasks. Since the
working memory task preceded the visual world task and was present on every trial, we assume that the participants considered this
to be the primary task. Therefore, we cannot rule out the influence of such factors on the eye movements. We also acknowledge
that it is still unclear what exactly such eye movements refer to.
The linguistic activation of the TE cohort and programming an
eye movement to a visual referent are different things and involve
different mechanisms. While we measure the eye movements as a
dependent measure, we take them to indicate linguistic activation
itself. However, our results do not clarify if working memory load
influenced the linguistic activation itself or the oculomotor
aspects of the activation.
In sum, our study can contribute towards constraining the current models of cross-linguistic activation in bilinguals. In spite of
the limitations, we have provided some initial results to show that
cross-linguistic activation seen during bilingual language processing is not entirely spontaneous and uncontrolled when selective
attention is channelised elsewhere.
Acknowledgements. We thank Divya Bhatia and Kesaban Roychoudhuri
for help in data collection. This work was supported by a Department of
Science and Technology grant sanctioned to RKM from the Indian
Government.
Data availability statement. The data and materials for all experiments
are available at https://osf.io/psy3x/?view_only=b876874da97a4587a94f56424
dba4581

References
Altmann, GT and Kamide, Y (2007) The real-time mediation of visual attention by language and world knowledge: Linking anticipatory (and other) eye
movements to linguistic processing. Journal of Memory and Language 57,
502-518.
Arenberg, D (1973) Cognition and aging: Verbal learning, memory and problem solving. In Eisdorfer C and Lawton MP (eds), The psychology of adult
development and aging. Washington, DC: American Psychological
Association, pp. 74-97.
Barr, DJ (2008) Analyzing `visual world'eyetracking data using multilevel
logistic regression. Journal of Memory and Language 59, 457-474.

Bilingualism: Language and Cognition
Bates, D, Maechler, M, Bolker, B and Walker, S (2015) lme4: Linear
mixed-effects models using Eigen and S4. R package version 1.1-7. 2014.
Bhatia, D, Prasad, SG, Sake, K and Mishra, RK (2017) Task Irrelevant
External Cues Can Influence Language Selection in Voluntary Object
Naming: Evidence from Hindi-English Bilinguals. PloS One 12, e0169284.
Blumenfeld, HK and Marian, V (2013) Parallel language activation and cognitive control during spoken word recognition in bilinguals. Journal of
Cognitive Psychology 25, 547-567.
Brysbaert, M and Duyck, W (2010) Is it time to leave behind the Revised
Hierarchical Model of bilingual language processing after fifteen years of
service?. Bilingualism: Language and Cognition 13, 359-371.
Brysbaert, M, Buchmeier, M, Conrad, M, Jacobs, AM, Bolte, J and Bohl, A
(2011) The word frequency effect: A review of recent developments and
implications for the choice of frequency estimates in German.
Experimental Psychology 58, 412-424.
Brysbaert, M, Stevens, M, Mandera, P and Keuleers, E (2016) How many
words do we know? Practical estimates of vocabulary size dependent on
word definition, the degree of language input and the participant's age.
Frontiers in Psychology 7, 1116.
Burke, DM and Light, LL (1981) Memory and aging: The role of retrieval processes. Psychological Bulletin 90, 513.
Canseco-Gonzalez, E, Brehm, L, Brick, CA, Brown-Schmidt, S, Fischer, K
and Wagner, K (2010) Carpet or carcel: The effect of age of acquisition
and language mode on bilingual lexical access. Language and Cognitive
Processes 25, 669-705.
Christoffels, IK, De Groot, AM and Kroll, JF (2006) Memory and language
skills in simultaneous interpreters: The role of expertise and language proficiency. Journal of Memory and Language 54, 324-345.
Dijkstra, T and Van Heuven, WJ (2002) The architecture of the bilingual
word recognition system: From identification to decision. Bilingualism:
Language and Cognition 5, 175-197.
Dijkstra, T, Wahl, A, Buytenhuijs, F, Van Halem, N, Al-Jibouri, Z, De
Korte, M and Rekke, S (2019) Multilink: a computational model for bilingual word recognition and word translation. Bilingualism: Language and
Cognition 22, 657-679.
Duyck, W and Brysbaert, M (2004) Forward and backward number translation requires conceptual mediation in both balanced and unbalanced bilinguals. Journal of Experimental Psychology: Human Perception and
Performance 30, 889.
Gimenes, M and New, B (2016) Worldlex: Twitter and blog word frequencies
for 66 languages. Behavior Research Methods 48, 963-972.
Hadar, B, Skrzypek, JE, Wingfield, A and Ben-David, BM (2016) Working
memory load affects processing time in spoken word recognition: evidence
from eye-movements. Frontiers in Neuroscience 10, 221.
Hariprasad, VR, Koparde, V, Sivakumar, PT, Varambally, S, Thirthalli, J,
Varghese, M, Basavaraddi, IV and Gangadhar, BN (2013) Randomized
clinical trial of yoga-based intervention in residents from elderly homes:
Effects on cognitive function. Indian Journal of Psychiatry 55(Suppl 3),
S357.
Hothorn, T, Bretz, F and Westfall, P (2008) Simultaneous inference in general parametric models. Biometrical Journal 50, 346-363.
Hoshino, N and Kroll, JF (2008) Cognate effects in picture naming: Does crosslanguage activation survive a change of script? Cognition 106, 501-511.
Huettig, F and Altmann, GT (2005) Word meaning and the control of eye
fixation: Semantic competitor effects and the visual world paradigm.
Cognition 96, B23-B32.
Huettig, F and McQueen, JM (2007) The tug of war between phonological,
semantic and shape information in language-mediated visual search.
Journal of Memory and Language 57, 460-482.
Huettig, F, Olivers, CN and Hartsuiker, RJ (2011a) Looking, language, and
memory: Bridging research from the visual world and visual search paradigms. Acta Psychologica 137, 138-150.
Huettig, F, Rommers, J and Meyer, AS (2011b) Using the visual world paradigm to study language processing: A review and critical evaluation. Acta
Psychologica 137, 151-171.
Huettig, F and Janse, E (2016) Individual differences in working memory and
processing speed predict anticipatory spoken language processing in the visual world. Language, Cognition and Neuroscience 31, 80-93.

253
Ito, A, Corley, M and Pickering, MJ (2018) A cognitive load delays predictive
eye movements similarly during L1 and L2 comprehension. Bilingualism:
Language and Cognition 21(2) 251-264.
Ju, M and Luce, PA (2004) Falling on sensitive ears constraints on bilingual
lexical activation. Psychological Science 15, 314-318.
Kapiley, K and Mishra, RK (2018) Iconic culture-specific images influence
language non-selective translation activation in bilinguals. Translation,
Cognition & Behavior 1, 221-250.
Kapiley, K and Mishra, RK (2019) What do I choose? Influence of interlocutor awareness on bilingual language choice during voluntary object naming.
Bilingualism: Language and Cognition 22, 1029-1051.
Knoeferle, P and Crocker, MW (2006) The coordinated interplay of scene,
utterance, and world knowledge: Evidence from eye tracking. Cognitive
Science 30, 481-529.
Kroll, JF and Stewart, E (1994) Category interference in translation and picture naming: Evidence for asymmetric connections between bilingual memory representations. Journal of Memory and Language 33, 149-174.
Kroll, JF, Michael, E, Tokowicz, N and Dufour, R (2002) The development
of lexical fluency in a second language. Second Language Research 18, 137-
171.
Kroll, JF and Ma, F (2018) The bilingual lexicon. In Fernandez EM and
Cairns HS (eds), Blackwell handbooks in linguistics. The handbook of psycholinguistics. Wiley-Blackwell, pp. 294-319.
Lagrou, E, Hartsuiker, RJ and Duyck, W (2011) Knowledge of a second language influences auditory word recognition in the native language. Journal
of Experimental Psychology: Learning, Memory, and Cognition 37, 952.
Luke, SG (2017) Evaluating significance in linear mixed-effects models in R.
Behavior Research Methods 49, 1494-1502.
Marian, V, Spivey, M and Hirsch, J (2003) Shared and separate systems in
bilingual language processing: Converging evidence from eyetracking and
brain imaging. Brain and Language 86, 70-82.
Marty, P, Chemla, E and Spector, B (2013) Interpreting numerals and scalar
items under memory load. Lingua 133, 152-163.
Mercier, J, Pivneva, I and Titone, D (2014) Individual differences in inhibitory control relate to bilingual spoken word processing. Bilingualism:
Language and Cognition 17, 89-117.
Mishra, RK (2009) Interaction of language and visual attention: evidence from
production and comprehension. Progress in Brain Research 176, 277-292.
Mishra, RK, Olivers, CN and Huettig, F (2013) Spoken language and the
decision to move the eyes: To what extent are language-mediated eye movements automatic? In Progress in Brain Research 202, 135-149.
Mishra, RK and Singh, N (2014) Language non-selective activation of orthography during spoken word processing in Hindi-English sequential bilinguals: an eye tracking visual world study. Reading and Writing 27, 129-151.
Mishra, RK and Singh, N (2016) The influence of second language proficiency on bilingual parallel language activation in Hindi-English bilinguals.
Journal of Cognitive Psychology 28, 396-411.
Mishra, RK (2019) A few suggestions on broadening the cross-linguistic relevance of the Multilink model. Bilingualism: Language and Cognition 22,
691-692.
Monaghan, P, Chang, YN, Welbourne, S and Brysbaert, M (2017) Exploring
the relations between word frequency, language exposure, and bilingualism
in a computational model of reading. Journal of Memory and Language 93,
1-21.
Monsell, S, Doyle, MC and Haggard, PN (1989) Effects of frequency on visual word recognition tasks: Where are they? Journal of Experimental
Psychology: General 118, 43.
Poarch, GJ, Van Hell, JG and Kroll, JF (2015) Accessing word meaning in
beginning second language learners: Lexical or conceptual mediation?.
Bilingualism: Language and Cognition 18, 357-371.
Prasad, S, Viswambharan, S and Mishra, R (2019) Visual working memory
load constrains language non-selective activation under task-demands.
Linguistic Approaches to Bilingualism. https://doi.org/10.1075/lab.18045.pra
Pickering, MJ and Garrod, S (2013) An integrated theory of language production and comprehension. Behavioral and Brain Sciences 36, 329-347.
Rao, SL, Subbakrishna, DK and Gopukumar, K (2004) NIMHANS neuropsychology battery 2004, manual. National Institute of Mental Health
and Neurosciences.

254

Seema Prasad and Ramesh Kumar Mishra

Roychoudhuri, KS, Prasad, SG and Mishra, RK (2016) Iconic native culture cues inhibit second language production in a non-immigrant population: Evidence from Bengali-English bilinguals. Frontiers in Psychology
7, 1516.
Salverda, AP and Tanenhaus, MK (2010) Tracking the time course of orthographic information in spoken-word recognition. Journal of Experimental
Psychology: Learning, Memory, and Cognition 36, 1108.
Schonfield, D and Robertson, BA (1966) Memory storage and aging.
Canadian Journal of Psychology/Revue canadienne de psychologie 20, 228.
Shook, A and Marian, V (2013) The bilingual language interaction network for
comprehension of speech. Bilingualism: Language and Cognition 16, 304-324.
Soto, D, Heinke, D, Humphreys, GW and Blanco, MJ (2005) Early,
involuntary top-down guidance of attention from working memory.
Journal of Experimental Psychology: Human Perception and
Performance 31, 248.
Spivey, MJ and Marian, V (1999) Cross talk between native and second languages: Partial activation of an irrelevant lexicon. Psychological Science 10,
281-284.
Spreen, O and Strauss, E (1998) Controlled oral word association (FAS). In
Spreen O and Strauss E (eds), A compendium of neuropsychological tests.
New York: Oxford University, 447-64.
Sunderman, G and Kroll, JF (2006) First language activation during second
language lexical processing: An investigation of lexical form, meaning,
and grammatical class. Studies in Second Language Acquisition 28, 387-422.

Sunderman, GL and Priya, K (2012) Translation recognition in highly proficient Hindi-English bilinguals: The influence of different scripts but connectable phonologies. Language and Cognitive Processes 27, 1265-1285.
Talamas, A, Kroll, JF and Dufour, R (1999) From form to meaning: Stages in
the acquisition of second-language vocabulary. Bilingualism: Language and
Cognition 2, 45-58.
Tanenhaus, MK, Spivey-Knowlton, MJ, Eberhard, KM and Sedivy, JC
(1995) Integration of visual and linguistic information in spoken language
comprehension. Science 268, 1632-1634.
Thierry, G and Wu, YJ (2007) Brain potentials reveal unconscious translation
during foreign language comprehension. Proceedings of the National
Academy of Sciences 104, 12530-12535.
Van Hell, JG and Tanner, D (2012) Second language proficiency and crosslanguage lexical activation. Language Learning 62, 148-171.
Veivo, O, Jarvikivi, J, Porretta, V and Hyona, J (2016) Orthographic activation in L2 spoken word recognition depends on proficiency: evidence from
eye-tracking. Frontiers in Psychology 7, 1120.
Veivo, O, Porretta, V, Hyona, J and Jarvikivi, J (2018) Spoken second language words activate native language orthographic information in late
second language learners. Applied Psycholinguistics 39, 1011-1032.
Weber, A and Cutler, A (2004) Lexical competition in non-native spokenword recognition. Journal of Memory and Language 50, 1-25.
Woodman, GF and Luck, SJ (2004) Visual search is slowed when visuospatial
working memory is occupied. Psychonomic Bulletin & Review 11, 269-274.

Appendix 1.

Spoken word

Translationequivalent

TE cohort

Distracter 1

Distracter 2

Distracter 3

L1 spoken words
1

/kfu;k(dhaniya)

Coriander

Courier

Hook

Comb

Sheet

2

(soochi)

List

Literate

Battery

Song

Kennel

3

( jeebh)

Tongue

Torch

Book

Sandal

Garage

4

(kachra)

Trash

Track

Bag

Wood

Stable

5

(raakh)

Ash

Apple

Bus

Plug

Shed

6

flDdk(sikka)

Coin

Cot

Spoon

Den

Lens

7

(dahi)

Curd

Cup

Gear

Speaker

Mill

8

(saabun)

Soap

Sofa

Card

Magnet

Revolver

9

(manch)

Stage

Stain

Internet

Guitar

Dam

10

fiatjk(pinjara)

Cage

Cave

Jar

Ribbon

Globe

11

(gaon)

Village

Villain

Train

Cloud

Jail

12

(aloo)

Potato

Pot

Cylinder

Shoe

Violin

13

(gulab)

Rose

Roll

Vehicle

Car

Queen

14

gYnh(haldi)

Turmeric

Tunnel

Oven

Screen

Mess

15

(saancha)

Mold

Morgue

Key

Lace

Eye

16

(oonTh)

Camel

Camera

Ice

Window

Fog

17

District

Disk

Cycle

Door

Star

20

ftyk(Jila)
fyQkQk(lifafa)
rEckdw(tambakoo)
ifg;k(pahiya)

21

(gend)

18
19

Envelope

Enemy

Floor

Bike

House

Tobaco

Top

Bone

Cat

Switch

Wheel

Wheat

Bed

Tap

Cover

Ball

Bank

Clock

Tape

Fan
(Continued )

Bilingualism: Language and Cognition

255

(Continued.)
Spoken word

Translationequivalent

TE cohort

Distracter 1

Distracter 2

Distracter 3

Road

Rope

Fly

Box

Shadow

23

lM+d(saDak)
bZaV(eenT)

Brick

Bridge

Flight

Circle

Stone

24

(tel)

Oil

Oyster

Forest

Prophet

Note

25

(mukuT)

Crown

Crow

Footpath

Mountain

Pie

26

fgj.k(hiraN)
HksfM+;k(bheDiya)

Deer

Dean

Crystal

Grass

Custard

27

Wolf

Wool

Gold

Bull

Tar

28

(malayi)

Cream

Creek

Silver

Mud

Pepper

29

Paper

Paste

Shirt

Belt

Election

Flag

Flame

Plastic

Message

Heart

Lid

Litter

Cigarette

Skirt

Vote

Rubber

Rust

Earth

Scissor

Dust

Family

Farm

Tear

Drop

Hole

34

dkxt+(kaagaz)
>aMk( jhanDa)
<Ddu(Dhakkan)
jcM+(rabaD)
ifjokj (parivar)
pwYgk(choolha)

Stove

Store

Hall

Fire

Tail

35

(taar)

Wire

Wine

Gas

Tale

Pan

36

(mor)

Peacock

Peanut

Don

Button

Tree

37

dqfV;k(kuTiya)

Cottage

Cotton

Hotel

Dawn

Sleep

38

(Parda)

Curtain

Curfew

Dream

Rice

Glass

39

cUkh(batti)
rkSfy;k(touliya)

Lamp

Lamb

Screw

Meadow

Pool

40

Towel

Tomato

Pet

Sky

Pole

41

(sharaab)

Alcohol

Alumni

Index

Foam

Lighter

42

(sena)

Army

Arc

Cigar

Diamond

Ring

43

iUkk(patta)

Leaf

Leader

Frame

Heaven

Iron

44

(razai)

Quilt

Quill

Bulb

Hell

Radio

45

(hatheli)

Palm

Park

Coffin

Date

Temple

46

Chest

Chess

Police

Tomb

Socks

47

lUnwd(sandook)
NM+h(chaDi)

Stick

Stitch

Knee

Puzzle

Jewel

48

(deewar)

Wall

Watch

Boat

Port

Church

49

(palak)

Spinach

Spindle

Oar

Razor

Widow

50

(raja)

King

Kitchen

Wound

Yacht

Toy

51

(Daaku)

Bandit

Bangle

Van

Blood

Parachute

22

30
31
32
33

52

(machali)

Fish

Film

Robot

Bomb

Paradise

53

r'rjh(tashthari)

Plate

Play

Crackers

Riot

Blade

54

(minar)

Tower

Town

Booth

Society

Coffee

55

(chaTayi)

Mat

Map

Porcupine

Orphan

Boot

56

(nali)

Gutter

Gum

Photo

Beetle

Strawberry

57

(madhu)

Honey

Hunter

Throne

Desert

Snow

58

(sagar)

Sea

Seal

Finger

Tea

Eagle

59

(talwar)

Sword

Swamp

Caravan

Bear

Turtle

60

Dog

Doll

Ladder

Album

Vegetable

61

dqUkk(kutta)
fdyk(kila)

Fort

Fork

Circus

Piano

Angel

62

(dhaara)

Stram

Street

Glue

Beast

Video

63

(chaaku)

Knife

Knight

Soldier

Wizard

Vapour

64

(vaaNi)

Voice

Volume

Soup

Axe

Badge
(Continued )

256

Seema Prasad and Ramesh Kumar Mishra

(Continued.)
Spoken word

Translationequivalent

TE cohort

Distracter 1

Distracter 2

Distracter 3

65

(neev)

Foundation

Fountain

Paint

Nail

Shelter

66

(haathi)

Elephant

Element

Crayon

Table

Balloon

67

(choTi)

Peak

Peat

Canvas

Bar

Egg

68

(hawa)

Wind

Wing

Brush

Cane

File

69

(megh)

Coud

Clove

Poison

Beard

Cannon

70

o"kkZ(varsha)

Rain

Race

Fairy

Can

Beard

71

(kaanTa)

Thorn

Thousand

Bench

Feather

Carpet

72

Scarf

Scalp

Chain

Boot

Chain

Peel

Pink

Cherry

Flag

Bone

74

#eky(rumal)
fNydk(chilka)
ckM+(baaD)

Fence

Ferry

Bowl

Clown

Helmet

75

(thotha)

Parrot

Parlour

Grapes

Swing

City

76

(badan)

Body

Bottle

Hammer

Needle

Purse

77

(pair)

Leg

Letter

Mushroom

Jar

Hose

78

(doodh)

Milk

Mint

Clip

Pipe

Rainbow

79

(chooha)

Mouse

Mouth

Pear

Shell

Lizard

80

(suwar)

Pig

Pin

Roof

Basket

Stamp

73

L2 spoken words
1

Monkey

(bandar)

(bandook)

(okhli)

(almari)

( kuan)

2

Bangle

(kangan)

(kanghi)

(guD)

(palang)

(baadal)

3

Dance

(naach)

(naak)

(goBhi)

dqlhZ(kursi)

(chandan)

4

Pen

(kalam)

(kamal)

(tamaTar)

(shahar)

ifg;k(pahiya)

5

Rain

(barsaat)

(bartan)

(aalu)

(gulab)

6

Farmer

(daawat)

Fort

fdrkc(kitab)
fdjk;k(kiraya)

?kM+h(ghaDi)

7

fdlku(kisan)
fdyk(kila)

fNyuh(chilani)
mYyw(ullu)
f'k{kk(shiksha)

(phal)

(chooha)

8

Ear

(kaan)

(kaam)

(roTi)

(ghar)

(mousam)

9

Spinach

(paalak)

(paayal)

(chinTi)

jLlh(rassi)

(pul)

10

sun

(suraj)

(suwar)

(ghas)

( jheel)

(Tangana)

11

Wood

ydM+h(lakdi)

yM+dh(ladki)

uD'kk(naksha)

(chaara)

12

Brinjal

(baingan)

(bail)

fcYyh(billi)
dqYgkM+h(kulhaDi)

(menDhak)

tM+( jaD)

13

Sugar

(cheeni)

fpeVk(chimTa)

(okhli)

(ghosla)

14

Key

(chabhi)

(chaku)

(deewar)

15

Donkey

(gadha)

(bagula)

16

Hair

(baal)

?kM+k(ghaDa)
ckfj'k(baarish)

dqUkk(kutta)
yM+dk(laDka)
iUkk(patta)

dhM+k(keeDa)

(patang)

(chaawal)

17

Bamboo

(bans)

(bansuri)

(topi)

yM+dh(laDki)

(chaku)

18

Parrot

(thota)

(tohfa)

(aalu)

(veeNa)

isM+( peD)

19

Song

(gaana)

(gaay)

frryh(titli)

cYyk(balla)

(mooli)

(rassi)
(ajnabi)

20

Thumb

(angooTha)

(angoor)

(gulam)

(sangeet)

[kt+kuk(khajana)

21

Tiger

(baagh)

(baarat)

(pankha)

(kala)

( gehu)

22

Butter

(kaddu)

(saanp)

Chair

(thali)

flrkj (sitar)

( jhola)

24

Palm

(hatheli)

eD[kh(makkhi)
dqrkZ(kurta)
gFkkSM+k(hathouDa)

(patloon)

23

eD[ku(makkhan)
dqlhZ(kursi)

(chata)

(seb)

f[kM+dh(khiDaki)

25

Ring

(angooThi)

(ang)

(dhanush)

[kEck(khamba)

(raajma)

26

Boy

(baalak)

(baadam)

inkZ(curtain)

(gamla)

p'ek(chashma)
(Continued )

Bilingualism: Language and Cognition

257

(Continued.)

27

Spoken word

Translationequivalent

TE cohort

Distracter 1

Distracter 2

Distracter 3

Sand

(reth)

(resham)

( nimbu)

(pankh)

(naav)

28

Seed

(beej)

fctyh(bijali)

(Dholak)

(ghuTna)

(sena)

29

Bear

(bhalu)

(bhala)

(anDa)

(deepak)

(aakash)

30

Star

(taara)

(taala)

bZ'oj (eeshwar)
erZcku(martabaan)
fpfM+;k(chiDiya)
lh<+h(seeDhi)

(Tokri)

ekfpl(maachis)
fdrkc(kitab)
xqfM+;k(guDiya)
eafnj (mandir)

31

Bullet

(goli)

(gobji)

f[kykMh(khilaDi)
tehu(zameen)

32

Banana

(kela)

(kesh)

(lahar)

33

Bush

>kM+h( jhadi)

( jharna)

(sheesha)

(maanav)

rfd;k(takiya)
(loha)

34

Flag

( jhanDa)

( jharokha)

(batak)

(fasal)

35

Kite

(pathang)

iRFkj (patthar)

(haath)

 (sheesha)

36

Rabbit

(khargosh)

(kharbooja)

(rogi)

37

Nose

(naak)

(naam)

(pani)

38

Man

iq#"k(purush)

(purana)

(talab)

est+(mez)
dhM+k(keeDa)
jkLrk(raasta)

(loha)

39

Earthquake

(bhookamp)

(bhoomi)

(paisa)

enZ(mard)

40

Pitcher

?kM+k(ghaDa)

?kM+h(ghaDi)

>ksiM+h( jhopDi)
diM+k(kapaDa)

'k=q(shatru)

(mahal)

41

Wall

(Deewar)

(deep)

(maina)

(namak)

(shaitan)

42

Broom

( jhaaD)

>kM+h( jhadi)

(kaan)

(chai)

43

King

(raja)

(raashi)

(kamra)

(ghanTi)

iafMr(panDit)
Me:(Damru)

44

Fish

(machali)

ePNj (macchar)

tgj ( jahar)

(akaal)

(aurat)

45

Ocean

(sagar)

(saaman)

(aadmi)

(bulbul)

46

Sword

(talwar)

(talaak)

(baaja)

fdrkc(kitab)

fpfM+;k(chiDiya)
efLtn(masjid)

47

Dog

dqUkk(kutta)

(kundan)

( jhoola)

(haar)

(naana)

48

Night

(raat)

(raavan)

(gufa)

i{kh(pakshi)

49

Elephant

(haathi)

(haath)

bL=h(istri)
f[kM+dh(khiDaki)

(aam)

(paan)

50

Peak

(choTi)

(chori)

(moti)

(gajar)

igkM+( pahaD)

51

Fire

(aag)

(aaj)

dfork(kavita)

(mor)

52

Leaf

iUkk(patta)

(pata)

(heera)

frtksjh(tijori)
ekfpl(maachis)

(doodh)

53

Alcohol

(sharaab)

(shareer)

fMCck(Dibba)

(naayak)

(toofan)

54

Kite

(patang)

iUkk(patta)

(gali)

(kunchi)

(kanoon)

55

Lips

(honTh)

(hosh)

jLlh(rassi)

(sadan)

56

Lotus

(kamal)

(kalam)

(paisa)

'kfa(shakti)

57

Knife

(chaku)

(chaabi)

foKku(vijnaan)
fn;k(diya)
fu'kkuk(nishana)

58

Palm

(hatheli)

(hathouDa)

(sarovar)

xqCckjk(gubbara)
cfu;ku(baniyan)

(chadar)

59

Punishment

(saza)

(saral)

(ghar)

(keel)

(mehandi)

(moja)

60

Army

(sena)

(seva)

cLrh(basti)

(shadi)

edM+h(makaDi)

61

Pig

(suwar)

(suraj)

( jeera)

(mouka)

62

Potato

(aalu)

(aag)

(kharbooja)

okLrq(vasthu)
Hkqek(bhuTTa)
[kEck(khamba)
(uphaar)

(kahani)

?kksM+k(ghoDa)

(saahas)

(aadmi)

fdLer(kismat)

(lagaan)
(aadesh)

63

Nation

(desh)

(devatha)

(bansuri)

64

Rain

(barsaat)

crZu(bartan)

65

Rice

(chawal)

(chaku)

66

Peas

(maTar)

(maTka)

67

Rose

(gulab)

xqfM+;k(guDiya)

deht+(kameez)

(heera)

68

Camel

(oonTh)

(oon)

I;kt (pyaaz)
[kfut(khanij)
xkM+h(gaaDi)
isM(+ peD)
dkxt+(kaagaz)

(chand)

(nimbu)

69

Turmeric

gYnh(turmeric)

gYok(halwa)

(naav)

( jangal)

(khel)
(Continued )

258

Seema Prasad and Ramesh Kumar Mishra

(Continued.)
Spoken word

Translationequivalent

TE cohort

Distracter 1

Distracter 2

Distracter 3

70

Oil

(tel)

(tejas)

(khana)

( jeb)

gih(haDDi)

71

Sun

lw;Z(surya)

(sookha)

(rekha)

(namak)

(kathak)

72

Boat

(nouka)

(noukri)

(koyla)

rLohj (tasveer)

(log)

73

Wire

(taar)

(taakat)

pEep(chammach)

(topi)

(sangeet)

(gagan)

ukM+h(naDi)

ftanxh(zindagi)
fephZ(mirchi)

I;kt (pyaaz)

( joote)

(moonch)

(makan)

(laaThi)

(sapna)

(meeTha)

(naya)
(gahana)

d{kk(kaksha)
ijh{kk(pareeksha)

(paseena)

(thaili)

77

Crowd

la[;k(sankhya)
I;kyk(pyaala)
dkxt+(kaagaz)
HkhM+(bheeD)

78

Ghost

(bhoot)

HkwfoKku(bhovijnaan)

79

World

(sansar)

(sansad)

lh<+h(seeDhi)
fgEer(himmat)
gt+kj (hazaar)
eafty(manzil)
ejht+(mareez)

80

Tomorrow

(kal)

dtZ(karz)

(adalat)

74

Digit

75

Cup

76

Paper

(kanoon)
(bheekh)

Appendix B
Full model output from linear mixed effects analysis exported through "stargazer" package in R (Hlavac, 2014). Treatment coding was used for the variables
spoken-word language (Hindi, English) and object ("TE", "D"). The models assumed TE as baseline. Time windows are denoted as "TW1", "TW2", "TW3",
"TW4", "TW5". "logD" and "logTE" refer to log transformed average frequencies for distractors and TE cohorts respectively. In Experiment 2, load had two levels
("high", "low" with high as baseline). In Experiment 3, load had three levels: "load1", "load2", "load3" referring to 2-letter, 6-letter- and 8-letter condition respectively. "Load1" was the baseline. The full structure of the model is specified on top of the output table.

Experiment 1
1.1 Overall fixation
Looks  language * IA * Time + (1 | Subject) + (1 | Item)

Dependent variable:
Looks
languageHindi

-0.014
t = -0.151

IA_D

-0.162
t = -6.817***

Time

-0.0001
t = -4.685***

languageHindi:IA_D

0.124
t = 3.680***

languageHindi:Time

0.0002
t = 5.516***

IA_D:Time

0.0002
t = 3.899***

languageHindi:IA_D:Time

-0.0003
t = -4.741***

Constant

-1.923
t = -24.963***

Observations
Log Likelihood

285,000
-639,147.300

Bilingualism: Language and Cognition

259

Akaike Inf. Crit.

1,278,317.000

Bayesian Inf. Crit.

1,278,433.000

Note: *p < 0.05; **p < 0.01; ***p < 0.001

1.2 Time-window analysis
TW1: Looks  language * IA + (1 + language | Subject) + (1 | Item)
TW2,3,4,5: Looks  language * IA + (1 + language + IA | Subject) + (1 | Item)

Dependent variable:
Looks
TW1

TW2

TW3

TW4

TW5

0.046

0.048

-0.003

0.194

0.209

t = 0.338

t = 0.277

t = -0.023

t = 1.649

t = 1.878

-0.101

-0.141

-0.162

-0.011

0.004

t = -3.876***

t = -3.742***

t = -3.630***

t = -0.214

t = 0.092

0.055

0.030

0.118

-0.113

-0.158

t = 1.496

t = 0.817

t = 3.165**

t = -2.974**

t = -4.131***

-1.991

-1.916

-1.921

-2.059

-2.066

t = -18.888***

t = -14.323***

t = -15.685***

t = -20.622***

t = -22.881***

57,000

57,000

57,000

57,000

57,000

Log Likelihood

-126,092

-125,867.5

-126,830

-127,956.1

-128,138.7

Akaike Inf. Crit.

252,202.1

251,759

253,684

255,936.2

256,301.4

Bayesian Inf. Crit

252,282.6

251,866.4

253,791.4

256,043.6

256,408.8

languageHindi

IA_D

languageHindi:IA_D

Constant

Observations

Note: *p < 0.05; **p < 0.01; ***p < 0.001

Pairwise (TW3)
Fit: lmer(formula = Looks  language * IA + (1 + language +
IA | Subject) + (1 | Item), data =
control_noload_logit_long_filtered_TW3)
Linear Hypotheses:
Estimate Std. Error z value Pr(>|z|)
Eng = = 0 0.16236
0.04472 3.630 0.000544 ***
Hin = = 0 0.04433
0.04472 0.991 0.481642
---
Signif. codes: 0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
(Adjusted p values reported -- single-step method)

Pairwise (TW4)
Fit: lmer(formula = Looks  language * IA + (1 + language +
IA | Subject) + (1 | Item), data =
control_noload_logit_long_filtered_TW4)
Linear Hypotheses:
Estimate Std. Error z value Pr(>|z|)
Eng = = 0 0.01098
0.05128
0.214
0.9590
Hin = = 0 0.12418
0.05128
2.422
0.0267 *
---
Signif. codes: 0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
(Adjusted p values reported -- single-step method)

260

Seema Prasad and Ramesh Kumar Mishra

Pairwise (TW5)
Fit: lmer(formula = Looks  language * IA + (1 + language +
IA | Subject) + (1 | Item), data =
control_noload_logit_long_filtered_TW5)
Linear Hypotheses:
Estimate Std. Error z value Pr(>|z|)
Eng = = 0 -0.004171
0.045108 -0.092 0.99294
Hin = = 0 0.153654
0.045108 3.406 0.00126 **
---
Signif. codes: 0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
(Adjusted p values reported -- single-step method)

1.3 Accuracy on visual world task
TASK_ACCURACY  language + logD + (1 + logD | RECORDING_SESSION_LABEL) + (1 | serial)

Dependent variable:
TASK_ACCURACY
languageHindi

-0.353
t = -0.911

logD

0.489
t = 1.759

Constant

0.783
t = 0.719

Observations
Log Likelihood

285
-103.504

Akaike Inf. Crit.

217.007

Bayesian Inf. Crit.

235.270

Note: *p < 0.05; **p < 0.01; ***p < 0.001

Experiment 2
2.1 Overall fixations
Looks  language * IA * load * Time + (1 | Subject) + (1 | Item)

Dependent variable:
Looks
languageHindi

-0.092
t = -0.868

IA_D

-0.039
t = -1.299

Time

-0.0002
t = -4.841***

loadLow

-0.095
t = -0.901

languageHindi:IA_D

0.075
t = 1.769

Bilingualism: Language and Cognition

languageHindi:Time

261

0.0002
t = 4.508***

IA_D:Time

0.0002
t = 3.105**

languageHindi:loadLow

0.184
t = 1.228

IA_D:loadLow

0.181
t = 4.243***

Time:loadLow

0.0003
t = 4.738***

languageHindi:IA_D:Time

-0.0003
t = -3.677***

languageHindi:IA_D:loadLow

-0.273
t = -4.530***

languageHindi:Time:loadLow

-0.001
t = -6.680***

IA_D:Time:loadLow

-0.0003
t = -4.658***

languageHindi:IA_D:Time:loadLow

0.001
t = 6.027***

Constant

-1.900
t = -24.319***

Observations

352,000

Log Likelihood

-788,326.300

Akaike Inf. Crit.

1,576,691.000

Bayesian Inf. Crit.

1,576,895.000

Note: *p < 0.05; **p < 0.01; ***p < 0.001

2.2 Time-window analysis
TW1
Looks  language * IA * load + (1 + language + IA + load | Subject) + (1 | Item)

Dependent variable:
Looks
languageHindi

-0.056
t = -0.591

IA_D

-0.032
t = -0.553

loadLow

-0.043
t = -0.466

languageHindi:IA_D

0.052
t = 1.082

262

Seema Prasad and Ramesh Kumar Mishra

languageHindi:loadLow

0.030
t = 0.236

IA_D:loadLow

0.119
t = 2.490*
-0.078

languageHindi:IA_D:loadLow

t = -1.157
-1.922

Constant

t = -25.194***

Observations

70,400

Log Likelihood

-156,864.300

Akaike Inf. Crit.

313,768.700

Bayesian Inf. Crit.

313,951.900

Note: *p < 0.05; **p < 0.01; ***p < 0.001

TW2
Looks  language * IA * load + (1 + language +
IA | Subject) + (1 | Item)
L1: Looks  IA * load + (1 + load | Subject) + (1 | Item)
L2: Looks  IA * load + (1 + load + IA | Subject) + (1 | Item)

Dependent variable:
Looks
TW2
languageHindi

TW2:L1

TW2:L2

-0.014

0.008

-0.014

t = -0.282

t = 0.250

t = -0.226

-0.058
t = -0.366

IA_D

loadLow

-0.003

0.102

-0.003

t = -0.020

t = 0.644

t = -0.019

languageHindi:IA_D

0.023

languageHindi:loadLow

0.105

t = 0.475

t = 0.477
IA_D:loadLow

0.056

-0.110

0.056

t = 1.180

t = -2.310*

t = 1.180

languageHindi:IA_D:loadLow

-0.167

Constant

-1.927

-1.985

-1.927

t = -16.754***

t = -16.919***

t = -15.803***

Observations

70,400

35,200

35,200

Log Likelihood

-157,094.600

-78,524.960

-78,548.470

Akaike Inf. Crit.

314,221.100

157,067.900

157,120.900

Bayesian Inf. Crit.

314,367.700

157,144.100

157,222.600

t = -2.466*

Note: *p < 0.05; **p < 0.01; ***p < 0.001

Bilingualism: Language and Cognition

263

Pairwise (L1)
Fit: lmer(formula = Looks  IA * load + (1 + load | Subject) + (1 |
Item), data = expt2_logit_long_filtered_TW2_Hindi)
Linear Hypotheses:
Estimate Std. Error z value Pr(>|z|)
low(TE-D) = = 0
0.101869
0.033767
3.017
0.0051 **
high(TE - D) = = 0 -0.008433
0.033767 -0.250
0.9611
---
Signif. codes: 0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
(Adjusted p values reported -- single-step method)

TW3
Looks  language * IA * load + (1 + language + IA + load | Subject) + (1 | Item)
L1: Looks  IA * load + (1 + IA + load | Subject) + (1 | Item)
L2: Looks  IA * load + (1 + IA + load | Subject) + (1 | Item)

Dependent variable:
Looks
TW3
languageHindi

TE3:L1

TW3:L2

0.047
t = 0.327

IA_D

0.099
t = 2.024*

loadLow

languageHindi:IA_D

-0.005
t = -0.068

0.099
t = 2.037*

-0.033

0.023

-0.033

t = -0.231

t = 0.150

t = -0.247

0.065

-0.046

0.065

t = 1.373

t = -0.967

t = 1.378

-2.017

-1.970

-2.017

t = -18.928***

t = -16.503***

t = -19.943***

-0.104
t = -2.183*

languageHindi:loadLow

0.056
t = 0.283

IA_D:loadLow

languageHindi:IA_D:loadLow

-0.111
t = -1.656

Constant

Observations

70,400

35,200

35,200

-156,762.600

-78,461.610

-78,263.590

Akaike Inf. Crit.

313,565.300

156,947.200

156,551.200

Bayesian Inf. Crit.

313,748.500

157,048.800

156,652.800

Log Likelihood

Note: *p < 0.05; **p < 0.01; ***p < 0.001

264

Seema Prasad and Ramesh Kumar Mishra

TW4
Looks  language * IA * load + (1 + language + IA + load | Subject) + (1 | Item)
L1: Looks  IA * load + (1 + IA + load | Subject) + (1 | Item)
L2: Looks  IA * load + (1 + IA + load | Subject) + (1 | Item)

Dependent variable:
Looks
TW4
languageHindi

TE4:L1

TW4:L2

0.072
t = 0.653

IA_D

loadLow

languageHindi:IA_D

0.050

-0.056

0.050

t = 0.881

t = -0.831

t = 0.586

-0.004

-0.134

-0.004

t = -0.040

t = -1.106

t = -0.041

0.174

0.055

-0.106
t = -2.196*

languageHindi:loadLow

-0.129
t = -0.854

IA_D:loadLow

0.055
t = 1.137

languageHindi:IA_D:loadLow

t = 3.617***

t = 1.137

0.119
t = 1.745

Constant

Observations

-2.011

-1.938

-2.011

t = -22.591***

t = -19.334***

t = -21.208***

70,400

35,200

35,200

-157,691.300

-78,754.230

-78,852.650

Akaike Inf. Crit.

315,422.600

157,532.500

157,729.300

Bayesian Inf. Crit.

315,605.800

157,634.100

157,830.900

Log Likelihood

Note: *p < 0.05; **p < 0.01; ***p < 0.001

TW5
Looks  language * IA * load + (1 + language + IA + load | Subject) + (1 | Item)
L1: Looks  IA * load + (1 + IA + load | Subject) + (1 | Item)
L2: Looks  IA * load + (1 + IA + load | Subject) + (1 | Item)

Dependent variable:
Looks
TW5
languageHindi

TE5:L1

TW5:L2

0.121
t = 1.268

IA_D

loadLow

0.105

-0.058

0.105

t = 1.624

t = -0.655

t = 1.162

0.221

-0.149

0.221

t = 2.366*
languageHindi:IA_D

-0.163
t = -3.342***

t = -1.514

t = 2.314*

Bilingualism: Language and Cognition

265

-0.370

languageHindi:loadLow

t = -3.020**
-0.246

0.191

-0.246

t = -5.061***

t = 3.946***

t = -5.043***

IA_D:loadLow

languageHindi:IA_D:loadLow

0.437
t = 6.346***
-2.065

Constant

Observations

-1.944

-2.065

t = -23.843***

t = -20.419***

t = -20.201***

70,400

35,200

35,200

-158,346.200

-78,905.120

-79,313.370

Akaike Inf. Crit.

316,732.300

157,834.200

158,650.700

Bayesian Inf. Crit.

316,915.600

157,935.900

158,752.400

Log Likelihood

Note: *p < 0.05; **p < 0.01; ***p < 0.001

2.3 Accuracy on visual world task and memory task
TASK_ACCURACY  language * load + (1 + language |
RECORDING_SESSION_LABEL) + (1 | serial)
Memory_Accuracy  load + (1 | RECORDING_SESSION_LABEL) + (1 | serial)

Dependent variable:
TASK_ACCURACY
(1)
languageHindi

Memory_Accuracy
(2)

0.051
t = 0.121

loadLow

1.194
t = 3.184**

languageHindi:loadLow

0.537
t = 3.620***

-1.194
t = -2.448*

Constant

-0.004
t = -0.011

Observations

352

2.626
t = 13.098***
3,520

-227.038

-853.891

Akaike Inf. Crit.

470.075

1,715.783

Bayesian Inf. Crit.

500.984

1,740.448

Log Likelihood

Note: *p < 0.05; **p < 0.01; ***p < 0.001

Pairwise (visual world task)
Fit: glmer(formula = TASK_ACCURACY  language * load +
(1 + language | RECORDING_SESSION_LABEL) + (1 |
serial), data = expt2_vwRT, family = binomial(link = "logit"))
Linear Hypotheses:
Estimate Std. Error z value Pr(>|z|)
eng(high-low) = = 0 -1.194e+00 3.748e-01 -3.184 0.0029 **
hin(high-low) = = 0 1.751e-05 3.117e-01 0.000 1.0000
---
Signif. codes:
0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
(Adjusted p values reported -- single-step method)

266

Seema Prasad and Ramesh Kumar Mishra

Experiment 3
3.1 Overall fixations
Dependent variable:
Looks
languageHindi

-0.071
t = -0.608

IA_D

-0.165
t = -4.593***

Time

-0.0001
t = -2.598**

load_newLoad2

-0.246
t = -2.120*

load_newLoad3

-0.114
t = -0.981

languageHindi:IA_D

0.046
t = 0.908

languageHindi:Time

0.0001
t = 1.220

IA_D:Time

0.0001
t = 1.518

languageHindi:load_newLoad2

0.223
t = 1.363

languageHindi:load_newLoad3

-0.055
t = -0.336

IA_D:load_newLoad2

0.201
t = 3.958***

IA_D:load_newLoad3

0.034
t = 0.678

Time:load_newLoad2

0.00000
t = 0.077

Time:load_newLoad3

-0.00004
t = -0.684

languageHindi:IA_D:Time

-0.0001
t = -1.151

languageHindi:IA_D:load_newLoad2

-0.175
t = -2.444*

languageHindi:IA_D:load_newLoad3

0.115
t = 1.591

languageHindi:Time:load_newLoad2

0.0001
t = 1.009

languageHindi:Time:load_newLoad3

-0.00001
t = -0.061

IA_D:Time:load_newLoad2

-0.00002
t = -0.204

Bilingualism: Language and Cognition

IA_D:Time:load_newLoad3

267

0.0001
t = 1.227

languageHindi:IA_D:Time:load_newLoad2

-0.0001
t = -0.851

languageHindi:IA_D:Time:load_newLoad3

-0.0001
t = -0.916

Constant

-1.910
t = -18.521***

Observations

357,400

Log Likelihood

-794,386.800

Akaike Inf. Crit.

1,588,828.000

Bayesian Inf. Crit.

1,589,119.000

Note: *p < 0.05; **p < 0.01; ***p < 0.001

3.2 Time-window analysis
TW1
Looks  language * IA * load_new + (1 + language +
load_new + IA | Subject) + (1 | Item)

Dependent variable:
Looks
languageHindi

0.003
t = 0.027

IA_D

-0.092
t = -1.573

load_newLoad2

-0.160
t = -1.464

load_newLoad3

-0.061
t = -0.540

languageHindi:IA_D

-0.043
t = -0.752

languageHindi:load_newLoad2

0.115
t = 0.997

languageHindi:load_newLoad3

-0.032
t = -0.278

IA_D:load_newLoad2

0.099
t = 1.737

IA_D:load_newLoad3

-0.009
t = -0.149

languageHindi:IA_D:load_newLoad2

-0.043
t = -0.528

languageHindi:IA_D:load_newLoad3

0.039
t = 0.475

Constant

-1.950
t = -19.248***

268

Seema Prasad and Ramesh Kumar Mishra

Observations

71,480
-158,579.700

Log Likelihood
Akaike Inf. Crit.

317,217.400

Bayesian Inf. Crit.

317,483.500

Note: *p < 0.05; **p < 0.01; ***p < 0.001

TW2
Looks  language * IA * load_new + (1 + language +
load_new + IA | Subject) + (1 | Item)
L1: Looks  IA * load_new + (1 + IA + load_new | Subject) + (1 | Item)
L2: Looks  IA * load_new + (1 + IA + load_new | Subject) + (1 | Item)

Dependent variable:
Looks
TW2
languageHindi

TW2:L1

TW3:L2

-0.103
t = -0.605

IA_D

-0.173
t = -3.751***

load_newLoad2

load_newLoad3

languageHindi:IA_D

-0.116
t = -2.112*

-0.173
t = -0.401

-0.332

0.018

-0.332

t = -1.931

t = 0.098

t = -0.714

-0.177

-0.260

-0.177

t = -1.010

t = -1.342

t = -0.382

0.057
t = 1.014

languageHindi:load_newLoad2

0.351
t = 1.573

languageHindi:load_newLoad3

-0.097
t = -0.436

IA_D:load_newLoad2

0.290
t = 5.152***

IA_D:load_newLoad3

0.104
t = 1.837

languageHindi:IA_D:load_newLoad2

-0.037

0.290

t = -0.658

t = 0.479

0.212
t = 3.729***

0.104
t = 0.171

-0.328
t = -4.113***

languageHindi:IA_D:load_newLoad3

0.092
t = 1.153

Constant

Observations

-1.915

-2.018

-1.915

t = -13.672***

t = -12.666***

t = -5.777***

71,480

35,480

36,000

-157,597.000

-78,065.280

-74,583.560

Akaike Inf. Crit.

315,252.000

156,166.600

149,207.100

Bayesian Inf. Crit.

315,518.100

156,319.100

149,376.900

Log Likelihood

Note: *p < 0.05; **p < 0.01; ***p < 0.001

Bilingualism: Language and Cognition

269

Pairwise (L1)
Fit: lmer(formula = Looks  IA * load_new + (1 + IA + load_new | Subject) +
(1 | Item), data = expt3_logit_long_filtered_TW2_Hindi)
Linear Hypotheses:
Estimate Std. Error z value Pr(>|z|)
load1(TE-D = = 0
0.11557 0.05472 2.112 0.0903.
load2(TE-D) = = 0 0.15242 0.05475 2.784 0.0150 *
load3(TE-D) = = 0 -0.09609 0.05550 -1.731 0.2037
---
Signif. codes: 0 `***' 0.001 `**' 0.01 `*' 0.05 `.' 0.1 ` ' 1
(Adjusted p values reported -- single-step method)

TW3, TW4 and TW5
Looks  language * IA * load_new + (1 + language +
load_new + IA | Subject) + (1 | Item)

Dependent variable:
Looks
TW3

TW4

TW5

languageHindi

-0.059

-0.061

0.056

t = -0.127

t = -0.167

t = 0.194

IA_D

-0.178

-0.125

-0.022

t = -0.404

t = -0.360

t = -0.080

-0.253

-0.290

-0.182

t = -0.546

t = -0.788

t = -0.632

-0.121

-0.205

-0.110

t = -0.261

t = -0.558

t = -0.382

0.061

0.026

-0.123

t = 0.098

t = 0.054

t = -0.319

0.279

0.381

0.205

t = 0.426

t = 0.734

t = 0.502

-0.226

0.007

-0.002

t = -0.344

t = 0.013

t = -0.005

0.213

0.243

0.114

t = 0.342

t = 0.494

t = 0.296

0.078

0.180

0.087

t = 0.126

t = 0.366

t = 0.225

-0.245

-0.363

-0.153

t = -0.278

t = -0.522

t = -0.280

0.260

-0.039

-0.056

t = 0.296

t = -0.056

t = -0.102

-1.960

-1.970

-2.039

t = -5.860***

t = -7.349***

t = -9.545***

Observations

71,480

71,480

71,480

Log Likelihood

-148,498.700

-153,482.600

-156,673.300

Akaike Inf. Crit.

297,031.400

306,999.300

313,380.500

Bayesian Inf. Crit.

297,187.500

307,155.300

313,536.500

load_newLoad2

load_newLoad3

languageHindi:IA_D

languageHindi:load_newLoad2

languageHindi:load_newLoad3

IA_D:load_newLoad2

IA_D:load_newLoad3

languageHindi:IA_D:load_newLoad2

languageHindi:IA_D:load_newLoad3

Constant

Note: *p < 0.05; **p < 0.01; ***p < 0.001

270

Seema Prasad and Ramesh Kumar Mishra

3.3 Accuracy on visual world and memory task
Visual world task: TASK_ACCURACY  condition_spoken_word * load_new + (1 | RECORDING_SESSION_LABEL) + (1 | serial)
WM task:

Dependent variable:

languageHindi

TASK_ACCURACY

Memory_Accuracy

(1)

(2)

0.188
t = 0.298

load_newLoad2

load_newLoad3

languageHindi:load_newLoad2

1.172

0.097

t = 1.779

t = 0.366

-0.685

0.351

t = -1.004

t = 1.146

-1.507
t = -1.647

languageHindi:load_newLoad3

0.879
t = 0.984

Constant

Observations

0.515

2.389

t = 1.054

t = 8.317***

372

3,724

-224.872

-1,170.425

Akaike Inf. Crit.

465.744

2,370.850

Bayesian Inf. Crit.

497.095

2,464.189

Log Likelihood

Note: *p < 0.05; **p < 0.01; ***p < 0.001

Pairwise (Visual world task)
Fit: glmer(formula = TASK_ACCURACY  language * load_new +
(1 | RECORDING_SESSION_LABEL) + (1 | serial), data = expt3_vwRT,
family = binomial(link = "logit"))
Linear Hypotheses:
Estimate Std. Error z value Pr(>|z|)
eng(6-2) = = 0
1.1716
0.6587
1.779
0.322
eng(8-2) = = 0
-0.6853
0.6825
-1.004
0.818
eng(8-6) = = 0
-1.8568
0.6572
-2.825
0.026 *
hin(6-2) = = 0
-0.3351
0.6341
-0.529
0.980
hin(8-2) = = 0
0.1937
0.5757
0.337
0.996
hin(8-6) = = 0
0.5289
0.6387
0.828
0.901
---

