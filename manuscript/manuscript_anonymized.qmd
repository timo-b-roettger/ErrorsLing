---
title: "Statistical Reporting Inconsistencies in Experimental Linguistics"
shorttitle: "Statistical Inconsistencies"
author:
    - name: anonymous
      affiliations:
      - name: anonymous
  # - name: Dara Leonard Jenssen Etemady
  #   corresponding: true
  #   email: daraetemady@gmail.com
  #   affiliations:
  #     - name: University of Oslo
  #       department: Department of Linguistics & Scandinavian Studies
  #       city: Oslo
  #       country: Norway
  # - name: Timo B. Roettger
  #   corresponding: false
  #   orcid: 0000-0000-0000-0001
  #   email: timo.roettger@gmail.com
  #   affiliations:
  #     - name: University of Oslo
  #       department: Department of Linguistics & Scandinavian Studies
  #       city: Oslo
  #       country: Norway
author-note:
  status-changes: 
    affiliation-change: null
    deceased: null
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: The authors have no conflict of interest to declare.
    financial-support: null
    gratitude: null
    # authorship-agreements: Conceptionalization, Methodology, Validation, Formal Analysis, Review & Editing of Manuscript, Data Curation - T.B.R. & D.L.J.E.; Software, Investigation - D.L.J.E.; Writing of Original Draft, Visualization, Supervision - T.B.R.
abstract: "The present paper investigates the prevalence of statistical reporting consistencies across articles in eight experimental linguistics journals published between 2000 and 2023. Using the R package “Statcheck”, we retrieved 85,442 statistical test from 13,065 articles and assessed whether p-values were consistent with their test statistic and degrees of freedom. Half of all articles (50%) that used null hypothesis significance testing contained at least one inconsistent p-value. More than one in eight articles (13%) contained an inconsistency that may have affected the statistical conclusion. The inconsistency rates were comparable across journals and seem stable over publication years. We discuss possible reasons for this high rate and offer actionable steps for authors, reviewers, and editors to remedy this state-of-affairs."
keywords: [statistics, Statcheck, reproducibility, null hypothesis significance testing]
bibliography: bibliography.bib
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: doc
    keep-tex: true
header-includes: \usepackage{annotate-equations}
execute:
  echo: false
editor: 
  markdown: 
    wrap: sentence
---

# 1. Introduction

What we know about human language and its cognitive underpinnings is often informed by experimental data.
Researchers test theoretical predictions with their data using statistical tests.
Depending on the results of their tests, researchers make claims for or against theoretical assumptions.\
Since these tests play such an integral part in the argumentation process of experimentalists, both the data these tests are based on and the computational procedure of the tests itself should be transparent in order for other researchers to critically evaluate them.
But more than that, things can go wrong.
Humans are fallible; they make mistakes.
Transparent sharing allows others to detect and correct human error.
In recent years, quantitative linguistics have seen repeated calls to become more transparent and reproducible by sharing data and statistical protocols, often under the banner of "open science" [@roettger2019researcher; @arvan2022reproducibility; @laurinavichyute2022share].
Despite these calls, sharing of statistical protocols is still rather rare across the language sciences [@bochynska2023reproducible].
If statistical procedures cannot be critically evaluated, human errors might be left undetected and thus remain uncorrected in the publication record.
And if undetected errors affect the decision procedure of the analysis, i.e. whether a hypothesis is rejected or accepted, these errors might lead to -at best- overconfident, -at worst- false theoretical conclusions.
The present paper will present evidence that the published literature in experimental linguistics contains a concerning amount of such statistical errors, a state of affairs which warrants more rigorous data sharing practices.

# 2. Statistal reporting inconsistencies

The null-hypothesis significance testing (NHST) framework is, to date, the most dominant statistical framework that researchers use to test hypotheses in the language sciences [@sondereggera2024advancements].
NHST tests are commonly reported in specific formats which usually contain the name of the test (e.g. F, t, χ2), a test statistic, the degrees of freedom of that test (if applicable), and the p-value, representing the probability of observing the data (or more extreme data) given the null hypothesis (i.e. given that the test statistic is zero) (see example 1):  

 

```{=tex}
\begin{equation*}
  \tikzmarknode{node1}{(1) }
  \eqnmarkbox[gray]{node2}{F}
  \eqnmarkbox[orange]{node3}{(1,66)}
  \tikzmarknode{node4}{=}
  \eqnmarkbox[purple]{node5}{3.88}
  \tikzmarknode{node6}{, }
  \eqnmarkbox[blue]{node7}{p < .05}
\end{equation*}

\annotate[yshift=1em]{left}{node2}{name of test} 
\annotate[yshift=-1em]{below,left}{node3}{degrees of freedom} 
\annotate[yshift=1em]{right}{node5}{test statistic} 
\annotate[yshift=-1em]{below,right}{node7}{p-value}
```
 

Without access to the data and scripts, interested readers are left with trusting the authors that the statistical analysis is run and reported correctly.
However, the three sets of indices in (1) have clearly defined mathematical relationships and can thus be easily assessed for consistency.
For example, an F test with the specified degrees of freedom and a test-statistic of 3.88 should result in a p-value of 0.053 which is larger, not smaller, than 0.05.
Possible reasons for this inconsistency are manifold: It could be a typo of the comparison sign, i.e. the authors meant to use `=` or `<` rather than `>`.
Additionally, any of the numbers could contain a typo and sometimes an error might indicate erroneous rounding (e.g. 0.057 being rounded down to 0.05).
Without access to data and scripts, it remains unclear to the reader what has caused this inconsistency.
Such inconsistencies can be particularly concerning if the calculated p-value (here 0.057) and the reported p-values (here \<0.05) are not on the same side of the alpha threshold.
In NHST, p-values below a conventionalized alpha threshold, most commonly 0.05, are interpreted as evidence that the data are sufficiently inconsistent with the null hypothesis ("significant").
P-values above that threshold are considered consistent with the null hypothesis and practically lead to rejecting the alternative hypothesis ("non-significant").
In (1) above, the reported p-value suggest a significant result, the p-value derived from the degrees of freedom and the test statistic suggest a non-significant result.
In the following we refer to these inconsistencies as "decision inconsistencies".

The consistency of these values can be automatically assessed if statistical tests are reported in an unambiguous format.
Recently, a series of studies used such automatic assessments to evaluate the prevalence of inconsistent statistical reporting in psychology [@bakker2011mis; @bakker2014outlier; @caperos2013consistency; @veldkamp2014statistical; @wicherts2011willingness; @nuijten2016prevalence; @nuijten2020statcheck; @green2018statcheck; @claesen2023data], medical sciences [@garcia2004incongruence; @van2023comparing], psychiatry [@berle2007inconsistencies], cyber security studies [@gross2021fidelity], technological education research [@buckley2023estimating], and experimental philosophy [@colombo2018statistical].
For example, looking at over 250,000 p-values published in major psychology journals, @nuijten2016prevalence found that around 50% of the articles with statistical results contained at least one inconsistency and around 13% contained at least one decision inconsistency.
Other studies report on inconsistency rates between 4% and 14%, with between 10% and 63% of articles containing at least one inconsistency and between 3% and 21% contain at least one decision consistency.
These high inconsistency rates in other disciplines are concerning and have led to a constructive dialog, resulting in recommendations for best practice to either avoid inconsistencies or make them more detectable.

To assess the prevalence of statistical-reporting inconsistencies in experimental linguistics, the present paper conceptually replicates @nuijten2016prevalence and assesses p-values reported in thirteen experimental linguistic journals published between 2000 and 2023.
We explore whether the rate of inconsistency differs across journals, whether their rate has changed over the course of the last 23 years and whether there is evidence for bias in these statistical-reporting inconsistencies.
We discuss the results and offer concrete recommendations for authors, reviewers, and editors to tackle this problem.

```{r source-script}
#| echo: false
#| output: false
#| warning: false
#| message: false
#| include: false

suppressWarnings(suppressMessages(suppressPackageStartupMessages({
source("../scripts/01_Analysis.R")
})))

knitr::opts_chunk$set(dpi = 300)

## to do:
# add options to share original articles

```

# 3. Method

## 3.1. Data availability statement

All quantitative analyses were conducted using @rmanual. All derived data and corresponding R scripts are available [here](https://osf.io/gx3ub/?view_only=25957e06b5f449d6820b529eb0ae1937).

## 3.2. Sample

Focusing on experimental linguistic research, we used @kobrock2023assessing as a point of departure who list 100 linguistic journals that had at least a hundred articles published at the time of assessment (2021) and a high ratio of articles containing the search string "experiment\*" in title, abstract and/or keywords.
Out of these 100 journals, we selected all journals with at least 10% of articles containing the search string "experiment\*".
Out of the remaining 37 journals, we selected those journals that urged APA formatting either in the main body of the text or specifically regarding statistics in the author guidelines, resulting in nine remaining journals.
Moreover, to access the articles in .pdf format, the articles had to be either accessible to us through our library license, or open access, resulting in a final list of eight journals: Applied Psycholinguistics (APS), Bilingualism: Language and Cognition (BLC), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA). 
An anonymous reviewer raised the justified concern that the resulting sample might not represent the vast majority of work in experimental linguistics. 
To address this concern, we additionally included those five journals that have published the highest absolute number of experimental articles, regardless of whether they explicitly urge APA formatting or not, resulting in the inclusion of the following psycholinguistic outlets: Journal of Memory and Language (JML), Language Cognition and Neuroscience (LCN, former Language and Cognitive Processes), Journal of Psycholinguistic Research (JPR), Journal Of Speech Language And Hearing Research (SLH), and Brain and Language (BAL).

We included only original research articles within the publication years of 2000-2023, excluding any book reviews, response articles, commentaries, editorials, corrigenda, errata, advertisements, etc.

## 3.3. Statcheck

We used the R package Statcheck [Version 1.5.0, @nuijten2023statcheck] to automatically detect statistical-reporting inconsistencies.
Statcheck works as follows: After converting articles in pdf or html format to plain text, Statcheck searches for specific strings that correspond to a NHST result using regular expressions.
That way, Statcheck can detect results of t-tests, F-tests, Z-tests, χ2-tests, correlation tests, and Q tests as long as the test result fulfills three conditions: (a) the test result is reported completely including the test statistic, the degrees of freedom (if applicable), and the p-value; (b) the test result is in the body of the text, i.e.
Statcheck usually misses information in tables; and (c) the test result is reported in American Psychological Association style [@APA2020].
Given these constraints, Statcheck is estimated to detect roughly 60% of all reported NHST results [@nuijten2016prevalence].
Statcheck uses the reported test statistic and degrees of freedom to recalculate the p-value, compares the reported and recalculated p-value and, if there is a mismatch, flags the test as containing an “inconsistency.” The algorithm takes into account that tests might have been performed as one-tailed by identifying the search strings “one-tailed,” “one-sided,” or “directional” in the body of the text.
Moreover, Statcheck considers p = .000 and p \< .000 as inconsistent because p-values of exactly zero are mathematically impossible and the APA manual [@APA2020] advises to report very small p-values as p \< .001.
Validity checks of Statcheck suggest that inter-rater reliability between manual coding and Statcheck is high, i.e. 0.76 for inconsistencies and 0.89 for decision inconsistencies [@nuijten2016prevalence].
The overall accuracy of Statcheck is estimated to be between 96.2% to 99.9% [@nuijten2017validity].
We thus consider Statcheck a valid proxy of the prevalence of statistical reporting inconsistencies.

Articles from LAB spanned 2011-2023; LCN spanned 2015-2023.
Statcheck could not parse 291 articles, likely related to issues with rendering the Chi-Squared symbol being erroneously converted from .pdf to .txt, resulting in some gaps in the coverage.
This procedure resulted in `{r} length(files)` research articles which were submitted to analysis.

## 3.4. Analysis and research questions
The aims of this study were explicitly exploratory and hypothesis generating, thus analyses remain merely descriptive by reporting on the proportion of articles/tests that are statistically (in)consistent.
Our investigation was set out to explore the following research questions:
(1) How prevalent are statistical inconsistencies (and decision inconsistencies) in our sample?
(2) Do inconsistency rates vary across journals and/or their publication year? 
And (3) is there evidence for bias, i.e. are processes that result in inconsistencies more likely to produce lower p-values?

# 4. Results

## 4.1 Inconsistencies are highly prevalent

The results are summarized in @tbl-summary.
Out of `{r} journal_summary_counts[14,2][[1]]` articles, `{r} journal_summary_counts[14,3][[1]]` articles contained statistical tests that Statcheck could assess (`{r} 100 * round(journal_summary_counts[14,3][[1]] / journal_summary_counts[14,2][[1]],2)`%), amounting to `{r} journal_summary_counts[14,4][[1]]` assessable p-values. 
Interestingly, the five journals that did not explicitly encourage APA formatting had virtually identical rates of assessable articles compared to the original sample.
Overall, `{r} journal_summary_counts[14,5][[1]]` p-values were flagged as inconsistent (`{r} 100 * round(journal_summary_counts[14,5][[1]] / journal_summary_counts[14,4][[1]],3)`%) out of which `{r} journal_summary_counts[14,6][[1]]` were considered decision inconsistencies (`{r} 100* round(journal_summary_counts[14,6][[1]] / journal_summary_counts[14,4][[1]],3)`%) (see @tbl-summary).


```{r}
#| label: tbl-summary
#| tbl-cap: "Number of eligible articles, assessable articles and results, inconsistencies and decision inconsistencies across all journals. (Applied Psycholinguistics (APS), Language and Brain (BAL), Bilingualism: Language and Cognition (BLC), Journal of Memory and Language (JML), Journal of Psycholinguistic Research (JPR), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Cognition and Neuroscience (LCN, former Language and Cognitive Processes), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA), Journal Of Speech Language And Hearing Research (SLH))"

knitr::kable(journal_summary_counts)

```

## 4.2 Inconsistencies are stable across journals and publication year

The proportion of inconsistencies ranged from `{r} round(min(journal_summary_prop[,2]),0)` to `{r} round(max(journal_summary_prop[,2]),0)`% across journals (`{r} round(min(journal_summary_prop[,3]),0)` to `{r} round(max(journal_summary_prop[,3]),0)`% for decision inconsistencies) (see @fig-stacked-bar).
These rates appear to be stable across year of publication (see @fig-year-prop).
On average, `{r} 100*round(gross_per_article[[1]],2)`% of assessable articles contained one or more inconsistencies (journals range from `{r} 100*round(min(per_article[,2]),2)` to `{r} 100*round(max(per_article[,2]),2)`%) and `{r} 100*round(gross_per_article[[2]],2)`% contained one or more decision inconsistencies (journals range from `{r} 100*round(min(per_article[,3]),2)` to `{r} 100*round(max(per_article[,3]),2)`%).

```{r}
#| label: fig-stacked-bar
#| out-width: 100%
#| fig-pos: 'H'
#| fig-cap: "Proportion of articles containing at least one inconsistency / decision inconsistency. (Applied Psycholinguistics (APS), Language and Brain (BAL), Bilingualism: Language and Cognition (BLC), Journal of Memory and Language (JML), Journal of Psycholinguistic Research (JPR), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Cognition and Neuroscience (LCN, former Language and Cognitive Processes), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA), Journal Of Speech Language And Hearing Research (SLH))"

knitr::include_graphics("../plots/figure1.png")

```

```{r}
#| label: fig-year-prop
#| out-width: 100%
#| fig-cap: "Proportion of inconsistencies / decision inconsistencies across time overall (left panel) and split into journals (right panel). (Applied Psycholinguistics (APS), Language and Brain (BAL), Bilingualism: Language and Cognition (BLC), Journal of Memory and Language (JML), Journal of Psycholinguistic Research (JPR), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Cognition and Neuroscience (LCN, former Language and Cognitive Processes), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA), Journal Of Speech Language And Hearing Research (SLH))"

knitr::include_graphics("../plots/figure2.png")

```

## 4.3 Inconsistencies are biased

Inconsistencies that report the p-value as being larger or smaller than a reference value (e.g. p \> 0.05 or p \< 0.05, respectively) are not equally prevalent in the sample:
There were `{r} 100*round(sign_distribution[6]/sign_distribution[5], 3)`% of inconsistencies with p being reported as larger than a reference but `{r} 100*round(sign_distribution[2]/sign_distribution[1], 3)`% inconsistencies with p being reported as smaller than a reference.
So even if we assumed these inconsistencies were merely typos of the comparison sign (e.g. \< instead of \>), inconsistencies that erroneously report the p-value to be smaller than a reference value are more frequent than inconsistencies that erroneously report the p-value to be larger than a reference value.

These biases are also reflected in decision inconsistencies.
Of all decision inconsistencies (n = `{r} nrow(xdata[xdata$decision_error == TRUE,])`), `{r} 100 * round(nrow(xdata[xdata$decision_error == TRUE & xdata$computed_p >= .05,]) / nrow(xdata[xdata$decision_error == TRUE,]),2)`% represent cases in which a reported significant result (p \< 0.05) is recalculated as non-significant (p \> 0.05), i.e. non-significant results are more than twice as likely to be erroneously reported as significant than the other way around. 
The latter pattern, however, seems to be a bias of the past. 
Reproducing @nuijten2016prevalence, @fig-scatter plots the development of the bias observed for decision inconsistencies over time.
The prevalence of decision inconsistencies in significant p-values seems to have slightly decreased over the years, while the prevalence of decision inconsistencies in non-significant p-values seems to have slightly increased over the years. 

```{r}
#| label: fig-scatter
#| out-width: 100%
#| fig-cap: "Percentage of decision inconsistencies falsely reporting significance (black) or non-significance (orange), plotted across publication years."

knitr::include_graphics("../plots/figure3.png")

```

# 5. Discussion and Recommendations

## 5.1. Statistical reporting inconsistencies are prevalent

The present study found a large amount of statistical reporting inconsistencies across a sample of `{r} journal_summary_counts[14,2][[1]]` experimental linguistic articles, containing `{r} journal_summary_counts[14,4][[1]]` assessable p-values.
`{r} 100 * round(journal_summary_counts[14,5][[1]] / journal_summary_counts[14,4][[1]],3)`% of all p-values were flagged as inconsistent and `{r} 100 * round(journal_summary_counts[14,6][[1]] / journal_summary_counts[14,4][[1]],3)`% were flagged as decision inconsistencies, i.e. the reported p-value is on the opposite side of the alpha threshold than the recalculated p-value.
On average, `{r} 100*round(gross_per_article[[1]],2)`% of assessable articles contained at least one inconsistency and `{r} 100*round(gross_per_article[[2]],2)`% contained at least one decision inconsistency.

The present study can be considered a conceptual replication of previous studies investigating statistical-reporting inconsistency across different disciplines [@bakker2011mis; @bakker2014outlier; @caperos2013consistency; @veldkamp2014statistical; @wicherts2011willingness] and most recent assessments using Statcheck [@nuijten2016prevalence; @buckley2023estimating; @colombo2018statistical; @gross2021fidelity]. 
The discovered inconsistency rates fall in line with these studies that report on rates between 4% and 14%, with between 10% and 63% of articles containing at least one inconsistency.

Even if the prevalence of these inconsistencies could be largely attributed to inconsequential typos or rounding errors (an assumption we cannot test without access to the data), the sheer amount of these inconsistencies that have made it through peer-review should concern us.
They are human errors.
If such a substantial amount of errors is found in plain site, the question naturally arises as to how many errors during the data analysis itself remain undetected.
We should ask ourselves, if the tip of the iceberg is already so large, what is the volume of the submerged iceberg?

The present examination did not indicate any noticeable trend over a period of 23 years of publications.
Observed inconsistencies were characterized by reported p-values being on average lower than their recalculated counterparts and the prevalence of decision inconsistencies was higher for p-values reported as significant than for those reported as non-significant.
These biases have been reported on for other disciplines as well [@nuijten2016prevalence; @bakker2011mis].
These skewed patterns could indicate a systematic bias in favor of lower p-values in general and a bias towards significant results in particular.
Our data do not speak to the causes of these biases, but possible reasons include the following: 

First, researchers might intentionally round down p-values because they think lower p-values are more convincing to reviewers and readers.
This practice has been admitted to by 1 in 5 surveyed psychological researchers [@john2012measuring].
Given that a non-trivial amount of quantitative linguists have admitted to commit questionable research practices (and even fraud) [@isbell2022misconduct], we cannot exclude the possibility that some of the inconsistencies in our sample were indeed intentional.
It is our strong belief, however, that the majority of inconsistencies are unintentional.

Second, researchers might scrutinize non-significant results more than significant results or are less likely to double check significant results than non-significant results because results that confirm their hypothesis feed into their confirmation bias [@nickerson1998confirmation].
For example, @fugelsang2004theory let researchers evaluate data that are either consistent or inconsistent with their prior expectations.
They showed that when researchers encounter results that are not in line with their expectations, they are likely to blame the methodology while results that confirmed their expectations were rarely critically scrutinized.

Third, the observed bias might merely be a reflection of publication bias [@franco2014publication; @sterling1959publication] with (erroneously) reported significant p-values being more likely to be published than non-significant ones. 
Publication bias is a well established pattern in experimental linguistic research with many recent meta analyses discussing possible evidence for it [@isbilen2022statistical; @lehtonen2018bilingualism; @lu2024meta]
For example, @de2015cognitive showed that studies with results supporting the bilingual-advantage theory were most likely to be published, while studies with results challenging the theory were significantly less likely to be published. 

Regardless of possible causes for bias in the processes that generate inconsistencies, the data also suggest that some of these biases has decreased over time, pointing to a positive development.

## 5.2. Limitations of our study

While we believe our work offers an important contribution to improving statistical reporting practices in experimental linguistics, the present assessment and the conclusions we can draw from them are limited.
First, our sample is limited to only a subset of experimental linguistic journals. 
However, given the selection of journals and their standing in the field, and given that the inconsistency rates of our study are not only comparable to similar studies from other disciplines but also relatively stable across journals and time, our findings should be considered relevant for experimental linguistics at large.

Second, given the constraints on automatically detecting test statistics, Statcheck misses reported values that either diverge from APA reporting standards or are reported in tables.
However, inconsistency rates in our own sample have been shown to be similar for results in APA format vs. results that diverge from APA formatting [@bakker2011mis; @nuijten2016prevalence]. 

Third, Statcheck slightly overestimates inconsistency rates, because it might not accurately detect corrections for multiple comparisons [@schmidt2017statcheck].
@nuijten2017validity, however, show that not only were there only a small proportion of flagged inconsistencies related to multiple comparisons, but also that these multiple comparisons themselves were often erroneously reported.
They conclude that "\[a\]ny reporting inconsistencies associated with these tests and corrections could not explain the high prevalence of reporting inconsistencies" [@nuijten2017validity, p. 27].

More elaborate automatic tools for the extraction of statistical information might allow for a more detailed and more accurate assessment of statistical reporting in the future [e.g. @kalmbach2023rule].
Despite its limitations, Statcheck provides a rough proxy of true inconsistency rates in the published literature and we hope the reader agrees that the prevalence of inconsistencies is a state of affairs that should be reflected upon.

## 5.3. Recommendations for the field

There are concrete actionable steps the field of experimental linguistics can take to reduce statistical reporting inconsistencies.
In order to avoid simple copy-and-paste errors related to working in two separate programs for writing the manuscript and conducting the statistical analysis, authors should consider 'literate programming', i.e. an integration of analysis code and prose into a single, dynamic document [@knuth1984literate; @casillas2023opening].
Several implementations of literate programming are freely available to researchers including common R markdown files (Rmd) and Quarto markdown files (qmd).
Literate programming can ensure that values derived from the statistical analysis are automatically integrated into the manuscript document, avoiding errors that might happen during a manual transfer from one program to the other.

Authors should generally engage in transparent and reproducible practices that can reduce human error or at least make them detectable by sharing their derived data (i.e. the anonymized data table that was analyzed) as well as a detailed description of their statistical protocol, ideally in form of reproducible scripts.
Sharing reproducible analyses with reviewers allows the reviewers the reproduce the authors' analyses, possibly detect errors or even inappropriate statistical choices before publication, thus improving the quality and robustness of the final product.
Moreover, publicly sharing their analyses has numerous benefits to the authors themselves beyond error detection: Open data and materials can facilitate collaboration [@boland2017ten], increase efficiency and sustainability [@lowndes2017our], and are cited more often [@colavizza2020citation].

Reviewers can additionally check the statistical reporting consistency in the manuscript by using tools such as Statcheck [@nuijten2023statcheck, http://statcheck.io] or p-checker [@schonbrodt2015p, http://shinyapps.org/apps/p-checker/].
Reviewers could consider requesting data and scripts during peer review.
Such requests might be particularly justified when inconsistencies are apparent.
Explicitly requesting to share data might already instill additional care and quality checks when authors prepare their materials, but also allows the reviewers to carefully reproduce the results, and critically evaluate all choices made in the statistical analysis [@sakaluk2014analytic].
Recent evidence suggest experimental linguistics are still characterized by a pluralism of statistical approaches, even when trying to answer the same research question [@coretta_multidimensional_2023].
Some of these approaches might be more appropriate than others [@sondereggera2024advancements; @vasishth2023some], so more thorough evaluations of how researchers arrive at their statistical conclusions might elevate their analytical robustness.
Moreover, a turn towards inferential frameworks that do not focus on binary decision procedures such as the null hypothesis significance testing framework, might alleviate some of the biases we observed in the direction of inconsistencies [@vasishth2018statistical; @cumming2014new].

Journal editors could explicitly recommend consistency checks with algorithms such as Statcheck during peer review, a practice that has been taken up on by several journals from neighboring disciplines (Psychological Science[^1], Advances in Methods and Practices in Psychological Science[^2], Stress & Health @barber2017meticulous). 
Editors could also demand, recommend or at least encourage data sharing for publication in their journal. 
Data sharing policies have been shown to substantially increase the reproducibility of analyses [e.g., @hardwicke2018data; @laurinavichyute2022share] and a number of linguistic journals have already implemented such policies, including journals within our sample. 
Having said that, open data policies which for example the Journal of Memory and Language introduced in 2018 did not seem to have affected the proportion of statistical inconsistencies. The inconsistency rates for JML (and other journals) were rather stable across time, so open data alone might not resolve the issue without further changes to the research eco-system.

[^1]: http://www.psychologicalscience.org/publications/psychological_science/ps-submissions; accessed on July 15, 2024.

[^2]: https://www.psychologicalscience.org/publications/ampps/ampps-submission-guidelines; accessed on on July 15, 2024.

Researchers make errors.
Researchers have biases.
This is who we are as humans and there is not much we can do about our nature.
Being aware of this fact and how it might affect research might help us to make possibly negative consequences detectable and preventable.

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::
