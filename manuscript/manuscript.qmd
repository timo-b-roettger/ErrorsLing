---
title: "Statistical Reporting Inconsistencies in Experimental Linguistics"
shorttitle: "Statistical Inconsistencies"
author:
  - name: Dara Leonard Jenssen Etemady
    corresponding: false
    email: daraetemady@gmail.com
    affiliations:
      - name: University of Oslo
        department: Department of Linguistics & Scandinavian Studies
        city: Oslo
        country: Norway
  - name: Timo B. Roettger
    corresponding: true
    orcid: 0000-0000-0000-0001
    email: timo.roettger@gmail.com
    affiliations:
      - name: University of Oslo
        department: Department of Linguistics & Scandinavian Studies
        city: Oslo
        country: Norway
author-note:
  status-changes: 
    affiliation-change: null
    deceased: null
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: The authors have no conflict of interest to declare.
    financial-support: null
    gratitude: null
    authorship-agreements: Conceptionalization, Methodology, Validation, Formal Analysis, Review & Editing of Manuscript, Data Curation - T.B.R. & D.L.J.E.; Software, Investigation - D.L.J.E.; Writing of Original Draft, Visualization, Supervision - T.B.R.
abstract: "The present paper investigates the prevalence of statistical reporting consistencies across articles in eight experimental linguistics journals, published between 2000 and 2023. Using the R package “Statcheck”, we retrieved 39532 statistical test and assessed whether p-values were inconsistent with their test statistic and degrees of freedom. Half of all articles that used null hypothesis signficance testing contained at least one inconsistent p-value. One in eight articles contained an inconsistency that may have affected the statistical conclusion. The inconsistency rates were stable across journals and publication years. We offer actionable steps for authors, reviewers, and editors to remmedy this state-of-affairs."
keywords: [statistics, statcheck, reproducibility]
bibliography: bibliography.bib
format:
  apaquarto-docx: default
  apaquarto-html: default
  apaquarto-pdf:
    # Can be jou (journal), man (manuscript), stu (student), or doc (document)
    documentmode: doc
    keep-tex: true
header-includes: \usepackage{annotate-equations}
execute:
  echo: false
editor: 
  markdown: 
    wrap: sentence
---

# 1. Introduction
What we know about human language and its cognitive underpinnings is often informed by experimental data.
Researchers test theoretical predictions with these data using statistical tests.
Depending on the results of such tests, researchers make claims for or against theoretical assumptions.\
Since they play such an integral part in the reasoning process of experimentalists, both the data these tests are based on and the computational procedure of the tests itself should be transparent in order for other researchers to critically evaluate them.
Moreover things can go wrong.
To err is human.
Transparent sharing allows others to detect and correct human error.
In the recent years, the quantitative sciences have seen repeated calls to become more transparent and reproducible by sharing data and statistical protocols [@roettger2019researcher; @arvan2022reproducibility; @laurinavichyute2022share].
Despite these calls, sharing of statistical protocols is still rather rare across the language sciences [@bochynska2023reproducible].
If statistical procedures cannot be critically evaluated, human errors might be left undetected and thus remain uncorrected in the publication record.
And if undetected errors affect the decision procedure of the analysis, i.e. whether a hypothesis is rejected or accepted, these errors might lead to -at best- overconfident, -at worst- false theoretical conclusions.
The present paper will present evidence that the published literature in experimental linguistics contains a concerning amount of such statistical errors, a state of affairs which warrants more rigorous data sharing practices.

# 2. Statistal reporting inconsistencies
The null-hypothesis significance testing (NHST) framework is, to date, the most dominant statistical framework that researchers use to test hypotheses in the language sciences [@sondereggera2024advancements].
NHST tests are commonly reported in specific formats which usually contain the name of the test (e.g. F, t, χ2), a test statistic, the degrees of freedom of that test (if applicable), and the p-value, representing the probability of observing the data (or more extreme data) given the null hypothesis (i.e. given that the test statistic is zero) (see example 1):
&nbsp;

&nbsp;

```{=tex}
\begin{equation*}
  \tikzmarknode{node1}{(1) }
  \eqnmarkbox[gray]{node2}{F}
  \eqnmarkbox[orange]{node3}{(1,66)}
  \tikzmarknode{node4}{=}
  \eqnmarkbox[purple]{node5}{3.88}
  \tikzmarknode{node6}{, }
  \eqnmarkbox[blue]{node7}{p < .05}
\end{equation*}

\annotate[yshift=1em]{left}{node2}{name of test} 
\annotate[yshift=-1em]{below,left}{node3}{degrees of freedom} 
\annotate[yshift=1em]{right}{node5}{test statistic} 
\annotate[yshift=-1em]{below,right}{node7}{p-value}

```
&nbsp;

Since data and statistical protocol sharing remains rare across the language sciences [@bochynska2023reproducible], interested readers are left with trusting the authors that the statistical analysis is run and reported correctly.
However, the three sets of indices in (1) have clearly defined mathematical relationships and can thus be easily checked for consistency.
An F test with the specified degrees of freedom and a test-statistic of 3.88 should result in a p-value of 0.053 which is larger, not smaller, than 0.05.
Possible reasons for this inconsistency are manifold: It could be a typo of the comparison sign, i.e. the authors meant to use `=` or `>` rather than `<`.
Any of the numbers could contain a typo.
Sometimes an error might indicate erroneous rounding (e.g. 0.057 being rounded down to 0.05).
Without access to data and scripts, it remains unclear to the reader what has caused this inconsistency.
Such inconsistencies can be particularly concerning if the calculated p-value (here 0.057) and the reported p-values (here <0.05) are not on the same side of the alpha threshold.
In NHST, p-values below a conventionalized alpha threshold, most commonly 0.05, are interpreted as evidence that the data are sufficiently inconsistent with the null hypothesis ("significant").
P-values above that threshold are considered consistent with the null hypothesis and practically lead to rejecting the alternative hypothesis ("non-significant").
In (1) above, the reported p-value suggest a significant result, the p-value derived from the degrees of freedom and the test statistic suggest a non-significant result.
In the following we refer to these inconsistencies as "decision inconsistencies".

The consistency of these values can be automatically assessed if statistical tests are reported in an unambiguous format.
Recently, a series of studies used such automatic assessments to evaluate the prevalence of inconsistent statistical reporting in psychology [@bakker2011mis; @bakker2014outlier; @caperos2013consistency; @veldkamp2014statistical; @wicherts2011willingness; @nuijten2016prevalence; @nuijten2020statcheck; @green2018statcheck; @claesen2023data], medical sciences [@garcia2004incongruence; @van2023comparing], psychiatry [@berle2007inconsistencies], cyber security studies [@gross2021fidelity], technological education research [@buckley2023estimating], and experimental philosophy [@colombo2018statistical].
For example, looking at over 250 thousand p-values published in major psychology journals, @nuijten2016prevalence found that around 50% of the articles with statistical results contained at least one inconsistency and around 13% contained at least one “decision inconsistency”. 
Other studies report on inconsistency rates between 4% and 14%, with between 10% and 63% of articles containing at least one inconsistency and between 3% and 21% decision consistencies.

To assess the prevalence of statistical-reporting inconsistencies in experimental linguistics, the present paper conceptually replicates @nuijten2016prevalence and assesses p-values reported in eight experimental linguistic journals published between 2000 and 2023.
We explore whether the rate of inconsistency differs across journals, whether that rate has changed over the course of the last 20 years and whether there is evidence for bias in these statistical-reporting inconsistencies.
We discuss the results and offer concrete recommendations for authors, reviewers, and editors to tackle this problem.

```{r source-script}
#| echo: false
#| output: false
#| warning: false
#| message: false
#| include: false

suppressWarnings(suppressMessages(suppressPackageStartupMessages({
source("../scripts/01_Analysis.R")
})))

knitr::opts_chunk$set(dpi = 300)

## to do:
# add options to share original articles

```

# 3. Method

All quantitative analyses were conducted using @rmanual and the r packages \[LIST ALL PACKAGES from 01_Analysis.R and add to references\]

## 3.1. Statcheck
We used the R package Statcheck [Version 1.4.1-beta.2, @nuijten2023statcheck] to automatically detect statistical-reporting inconsistencies.
Statcheck works as follows: After converting articles in pdf or html format to plain text, Statcheck searches for specific strings that correspond to a NHST result using “regular expressions”.
That way, statcheck can detect results of t-tests, F-tests, Z-tests, χ2-tests, correlation tests, and Q tests as long as the test result fulfills three conditions: (a) the test result is reported completely including the test statistic, the degrees of freedom (if applicable), and the p-value; (b) the test result is in the body of the text, i.e. Statcheck usually misses reported in tables; and (c) the test result is reported in American Psychological Association style [@APA2020].
Given these constraints, Statcheck is estimated to detect roughly 60% of all reported NHST results [@nuijten2016prevalence]. 
Statcheck uses the reported test statistic and degrees of freedom to recalculate the p-value, compares the reported and recalculated p-values and, if there is a mismatch, flags the test as containing an “inconsistency.” 
The algorithm takes into account that tests might have been performed as one-tailed by identifying the search strings “one-tailed,” “one-sided,” or “directional” in the body of the text.
Moreover, Statcheck considers p = .000 and p \< .000 as inconsistent because p-values of exactly zero are mathematically impossible and the APA manual [@APA2020] advises to report very small p-values as p \< .001.
Validity checks of Statcheck suggest that inter-rater reliability between manual coding and Statcheck is high, i.e. 0.76 for inconsistencies and 0.89 for decision inconsistencies [@nuijten2016prevalence].
The overall accuracy of Statcheck is estimated to be between 96.2% to 99.9% [@nuijten2017validity].
We thus consider Statcheck a valid proxy of the prevalence of statistical reporting inconsistencies.

## 3.2. Sample
Focusing on experimental linguistic research, we used @kobrock2023assessing as a point of departure who list 100 linguistic journals that had at least a hundred articles published at the time of assessment (2021) and a high ratio of articles containing the search string "experiment\*" in title, abstract and/or keywords.
Out of these 100 journals, we selected all journals with at least 10% of articles containing the search string "experiment\*".
Out of the remaining 37 journals, we selected those journals that urged APA formatting either in the main body of the text or specifically regarding statistics in the author guidelines, resulting in nine remaining journals.
Moreover, to access the articles in .pdf format, the articles had to be either accessible to us through our library license, or open access, resulting in a final list of eight journals: Applied Psycholinguistics (APS), Bilingualism: Language and Cognition (BLC), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA).

We included only original research articles within the publication years of 2000-2023, excluding any book reviews, response articles, commentaries, editorials, corrigenda, errata, advertisements, etc.
Articles from LAB spanned 2011-2023, while the other journals spanned 2000-2023.
Statcheck could not parse 157 articles, likely related to issues with rendering the Chi-Squared symbol being erroneously converted from .pdf to .txt. 
No assessable statistics were found by Statcheck in JCL articles from 2000 to 2002 and in SLA articles from 2004 to 2008.
This procedure resulted in `{r} length(files)` research articles which were submitted to analysis.


## 3.3. Data availability statement
All derived data and corresponding R scripts are publicly available here: LINK.

# 4. Results

## 4.1 Prevalence of inconsistencies

The results are summarized in @tbl-summary.
Out of `{r} journal_summary_counts[9,2][[1]]` articles, `{r} journal_summary_counts[9,3][[1]]` articles contained statistical tests that statcheck could assess (`{r} 100 * round(journal_summary_counts[9,3][[1]] / journal_summary_counts[9,2][[1]],2)`%), amounting to `{r} journal_summary_counts[9,4][[1]]` assessable p-values.
`{r} journal_summary_counts[9,5][[1]]` p-values were flagged as inconsistent (`{r} 100 * round(journal_summary_counts[9,5][[1]] / journal_summary_counts[9,4][[1]],3)`%) out of which `{r} journal_summary_counts[9,6][[1]]` were considered decision inconsistencies (`{r} 100* round(journal_summary_counts[9,6][[1]] / journal_summary_counts[9,4][[1]],3)`%) (see @tbl-summary).

```{r}
#| label: tbl-summary
#| tbl-cap: "Number of eligible articles, assessable articles and results, inconsistencies and decision inconsistencies across all journals. (Applied Psycholinguistics (APS), Bilingualism: Language and Cognition (BLC), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA))"

knitr::kable(journal_summary_counts)

```

The proportion of inconsistencies ranged from `{r} round(min(journal_summary_prop[,2]),0)` to `{r} round(max(journal_summary_prop[,2]),0)`% across journals (`{r} round(min(journal_summary_prop[,3]),0)` to `{r} round(max(journal_summary_prop[,3]),0)`% for decision inconsistencies) (see @fig-stacked-bar).
These rates appear to be stable across year of publication (see @fig-year-prop).
On average, `{r} 100*round(gross_per_article[[1]],2)`% of assessable articles contained one or more inconsistencies (journals range from `{r} 100*round(min(per_article[,2]),2)` to `{r} 100*round(max(per_article[,2]),2)`%) and `{r} 100*round(gross_per_article[[2]],2)`% contained one or more decision inconsistencies (journals range from `{r} 100*round(min(per_article[,3]),2)` to `{r} 100*round(max(per_article[,3]),2)`%).

```{r}
#| label: fig-stacked-bar
#| out-width: 100%
#| fig-pos: 'H'
#| fig-cap: "Proportion of articles containing at least one inconsistency / decision inconsistency. (Applied Psycholinguistics (APS), Bilingualism: Language and Cognition (BLC), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA))"

knitr::include_graphics("../plots/figure1.png")

```

```{r}
#| label: fig-year-prop
#| out-width: 100%
#| fig-cap: "Proportion of inconsistencies / decision inconsistencies plotted from 2000 to 2023. Left: rates pooled across journals; Right: rates per journal. (Applied Psycholinguistics (APS), Bilingualism: Language and Cognition (BLC), Linguistic Approaches to Bilingualism (LAB), Language and Speech (LaS), Language Learning and Techology (LLT), Journal of Language and Social Psychology (LSP), Journal of Child Language (JCL), and Studies in Second Language Acquisition (SLA))"

knitr::include_graphics("../plots/figure2.png")

```

When examining reported against recalculated p-values for all inconsistencies (see @fig-scatter), we can identify certain spatial patterns (only inconsistencies that are based on '=' comparisons were included to make them easier to interpret, n = `{r} nrow(xdata[xdata$error == TRUE & xdata$p_comp == "=",])` out of `{r} nrow(xdata[xdata$error == TRUE,])`).
First, the center of density of points is located in the bottom left corner with more p-values reported closer to the standard alpha level of 0.05.
This is not surprising, since the quantitative sciences exhibit a documented publication bias, with hypotheses being more often confirmed (and thus p \< .05) than not [@franco2014publication; @sterling1959publication].
Second, there is a linear density of points along the diagonal axis which corresponds to numerically small inconsistencies, some of which might be related to simple rounding errors.
However, comparing the diagonal to the black line, which represents a linear model predicting recalculated by reported p-values, we can see a clear divergence of what is expected if inconsistencies were equally likely in both directions.
The slope of the regression line is flatter than the diagonal axis which means that, on average, inconsistencies have a tendency to exhibit smaller reported p-values than their recalculated counterparts.
Similarly, inconsistencies that report the p-value as being larger or smaller than a reference value (e.g. p > 0.05 or p < 0.05 ) are not equally prevalent. 
There were `{r} 100*round(sign_distribution[6]/sign_distribution[5], 3)`% of inconsistencies with p being reported as larger than a reference but `{r} 100*round(sign_distribution[2]/sign_distribution[1], 3)`% inconsistencies with p being reported as smaller than a reference (e.g. p < 0.05). 
So even if we assumed these inconsistencies were merely typos of the comparison sign, inconsistencies that erroneously report the p-value to be smaller than a reference value are more frequent than inconsistencies that erroneously report the p-value to be larger than a reference value.
These biases are also reflected in decision inconsistencies.
Of all decision inconsistencies (n = `{r} nrow(xdata[xdata$decision_error == TRUE,])`), `{r} 100 * round(nrow(xdata[xdata$decision_error == TRUE & xdata$computed_p >= .05,]) / nrow(xdata[xdata$decision_error == TRUE,]),2)`% represent cases in which a reported significant result (p \< 0.05) is recalculated as non-significant (p \> 0.05).


```{r}
#| label: fig-scatter
#| out-width: 100%
#| fig-cap: "Reported vs. recalculated p-values for inconsistencies that are based on '=' comparions. Black line descriptively indicates the  linear relationship, indicating a bias towards lower reported p-values."

knitr::include_graphics("../plots/figure3.png")

```

# 5. Discussion and Recommendations

## 5.1. Statistical reporting inconsistencies are prevalent
The present study found a large amount of statistical reporting inconsistencies across a sample of `{r} journal_summary_counts[9,2][[1]]` experimental linguistic articles, containing `{r} journal_summary_counts[9,3][[1]]` assessable p-values.
`{r} 100 * round(journal_summary_counts[9,5][[1]] / journal_summary_counts[9,4][[1]],3)`% of all p-values were flagged as inconsistent and `{r} 100 * round(journal_summary_counts[9,6][[1]] / journal_summary_counts[9,4][[1]],3)`% were flagged as decision inconsistencies, i.e. the reported p-value is on the opposite side of the alpha threshold than the recalculated p-value.
On average, `{r} 100*round(gross_per_article[[1]],2)`% of assessable articles contained at least one inconsistency and `{r} 100*round(gross_per_article[[2]],2)`% contained at least one decision inconsistency.

The present study can be considered a conceptual replication of previous studies investigating statistical-reporting inconsistency across different disciplines [@bakker2011mis; @bakker2014outlier; @caperos2013consistency; @veldkamp2014statistical; @wicherts2011willingness] and most recent such assessment using Statcheck [@nuijten2016prevalence; @buckley2023estimating; @colombo2018statistical; @gross2021fidelity]. 
Previous studies report on inconsistency rates between 4% and 14%, with between 10% and 63% of articles containing at least one inconsistency and between 3% and 21% contain at least one decision consistency.

Even if the prevalence of these inconsistencies could be largely attributed to inconsequential typos or rounding errors (an assumption we cannot test without access to the data), the sheer amount of these inconsistencies that have made it through peer-review should concern us.
They are human errors.
If such a substantial amount of errors is found in plain site, the question naturally arises as to how many errors during the data analysis itself remain undetected.
If the tip of the iceberg is already so large, what is the volume of the submerged iceberg?

The present examination assessment did not indicate any noticeable trend over the last 23 years of publications nor did the data suggest large differences between journals.

Observed inconsistencies were characterized by reported p-values being on average lower than their recalculated counterparts and the prevalence of decision inconsistencies was higher for p-values reported as significant than for those reported as non-significant.
These patterns could indicate a systematic bias in favor of lower p-values in general and a bias towards significant results in particular.
Our data do not speak to the causes of these biases, but possible reasons include the following: Researchers might intentionally round down p-values because they think a lower p-value is more convincing to reviewers and/or readers.
This practice has been admitted to by 1 in 5 surveyed psychological researchers [@john2012measuring].
Given that a non-trivial amount of quantitative linguists have admitted to commit questionable research practices (and even fraud) [@isbell2022misconduct], we cannot exclude the possibility that some of the inconsistencies were intentional.
It is our strong belief, however, that the majority of inconsistencies are unintentional.

Researchers might scrutinize non-significant results more than significant results or are less likely to double check significant results than non-significant results because results that confirm their hypothesis feed into their confirmation bias [@nickerson1998confirmation].
For example, @fugelsang2004theory let researchers evaluate data that are either consistent or inconsistent with their prior expectations.
They showed that when researchers encounter results that disconfirm their expectations, they are likely to blame the methodology while results that confirmed their expectations were rarely critically scrutinized.
Alternatively, the observed bias might be a reflection of publication bias [@franco2014publication; @sterling1959publication] with (erroneously) reported significant p-values being more likely to be published than non-significant ones.

## 5.2. Limitations of our study

While we believe our work offers an important contribution to improving statistical reporting practices in experimental linguistics, there are, of course, a number of limitations.
The present assessment and the conclusions we can draw from them are limited.
First, our sample is limited to only a subset of experimental linguistic journals.
Our sample is based on a crude criterion of what constitutes an experimental linguistic journal [see @kobrock2023assessing] and we restricted ourselves further to (for us) accessible journals which explicitly require APA statistical formatting in the author guidelines.
Thus it is possible that a different selection of journals would have resulted in different results.
However, given that the inconsistency rates of our study are comparable to similar studies from other disciplines and that those rates are relatively stable across journals and time, our findings should be considered relevant for experimental linguistics at large.

Second, given the constraints on automatically detecting test statistics, Statcheck misses reported values that either diverge from APA reporting standards or are reported in tables.
However, inconsistency rates have been shown to be similar for results in APA format vs. results that diverge from APA formatting [@bakker2011mis; @nuijten2016prevalence].

Third, statcheck slightly overestimates inconsistency rates, because it might not accurately detect corrections for multiple comparisons [@schmidt2017statcheck].
@nuijten2017validity, however, show that not only were there only a small proportion of flagged inconsistencies related to multiple comparisons, but also that these multiple comparisons themselves were often erroneously reported.
They conclude that "\[a\]ny reporting inconsistencies associated with these tests and corrections could not explain the high prevalence of reporting inconsistencies" [@nuijten2017validity, p. 27].

More elaborate automatic tools for the extraction of statistical information might allow for a more detailed and more accurate assessment of statistical reporting in the future [e.g. @kalmbach2023rule].
Despite its limitations, Statcheck provides a rough proxy of true inconsistency rate in the published literature and we hope the reader agrees that the prevalence of inconsistencies is a state of affairs that should be reflected upon.

## 5.3. Recommendations for the field

There are concrete actionable steps the field of experimental linguistics can make to reduce statistical reporting inconsistencies.
In order to avoid simple copy-and-paste errors related to working in two separate programs for writing the manuscript and conducting the statistical analysis, authors should consider 'literate programming', i.e. an integration of analysis code and prose into a single, dynamic document [@knuth1984literate; @casillas2023opening].
Several implementations of literate programming are freely available to researchers including common R markdown files (Rmd) and Quarto markdown files (qmd).
Literate programming can ensure that values derived from the statistical analysis are automatically integrated into the manuscript document, avoiding errors that might happen during a manual transfer from one program to the other.

Authors should also consider sharing their derived data (i.e. the anonymized data table that was analyzed) as well as a detailed description of their statistical protocol, ideally in form of reproducible scripts.
Sharing reproducible analyses with reviewers allows the reviewers the reproduce the authors' analyses, possibly detect errors or even inappropriate statistical choices before publication, thus improving the quality and robustness of the final product.
Moreover, publicly sharing their analyses has numerous benefits to the authors themselves beyond error detection: Open data and materials can facilitate collaboration [@boland2017ten], increase efficiency and sustainability [@lowndes2017our], and are cited more often [@colavizza2020citation].

If authors do not share data and scripts, reviewers can check at least the statistical reporting consistency in the manuscript by using tools such as statcheck [@nuijten2023statcheck, http://statcheck.io] or p-checker [@schonbrodt2015p, http://shinyapps.org/apps/p-checker/].
Reviewers could consider requesting data and scripts during peer review.
Such requests might be particularly justified when inconsistencies are apparent.
Explicitly requesting to share data might already instill additional care and quality checks when authors prepare their materials, but also allows the reviewers to carefully reproduce the results, and critically evaluate all choices made in the statistical analysis [@sakaluk2014analytic].
Recent evidence suggest experimental linguistics are still characterized by a pluralism of statistical approaches, even when trying to answer the same research question [@coretta_multidimensional_2023].
Some of these approaches might be more appropriate than others [@sondereggera2024advancements; @vasishth2023some], so more thorough evaluations of how researchers arrive at their statistical conclusions might elevate their analytical robustness.

Journal editors could explicitly recommend consistency checks with algorhythms such as statcheck during peer review, a practice that has been taken up on by several journals from neighboring disciplines \[Psychological Science[^1], Advances in Methods and Practices in Psychological Science[^2], Stress & Health @barber2017meticulous\]. 
Editors could also demand, recommend or at least encourage data sharing for publication in their journal. 
Data sharing policies have been shown to substantially increase the reproducibility of analyses [e.g. @laurinavichyute2022share; @hardwicke2018data].

[^1]: http://www.psychologicalscience.org/publications/psychological_science/ps-submissions; accessed on July 15, 2024.

[^2]: https://www.psychologicalscience.org/publications/ampps/ampps-submission-guidelines; accessed on on July 15, 2024.

Researchers make errors. 
Researchers have biases.
This is who we are as humans and there is not much we can do about our nature. 
Being aware of this fact and how it might affect research might help us to make possibly negative consequences detectable and preventable. 


# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::
